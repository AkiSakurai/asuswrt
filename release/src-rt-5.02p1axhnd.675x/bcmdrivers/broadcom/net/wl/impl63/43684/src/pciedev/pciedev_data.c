/*
 * PCIEDEV timing critical datapath functions
 * compiled for performance rather than size
 * Copyright 2020 Broadcom
 *
 * This program is the proprietary software of Broadcom and/or
 * its licensors, and may only be used, duplicated, modified or distributed
 * pursuant to the terms and conditions of a separate, written license
 * agreement executed between you and Broadcom (an "Authorized License").
 * Except as set forth in an Authorized License, Broadcom grants no license
 * (express or implied), right to use, or waiver of any kind with respect to
 * the Software, and Broadcom expressly reserves all rights in and to the
 * Software and all intellectual property rights therein.  IF YOU HAVE NO
 * AUTHORIZED LICENSE, THEN YOU HAVE NO RIGHT TO USE THIS SOFTWARE IN ANY
 * WAY, AND SHOULD IMMEDIATELY NOTIFY BROADCOM AND DISCONTINUE ALL USE OF
 * THE SOFTWARE.
 *
 * Except as expressly set forth in the Authorized License,
 *
 * 1. This program, including its structure, sequence and organization,
 * constitutes the valuable trade secrets of Broadcom, and you shall use
 * all reasonable efforts to protect the confidentiality thereof, and to
 * use this information only in connection with your use of Broadcom
 * integrated circuit products.
 *
 * 2. TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED
 * "AS IS" AND WITH ALL FAULTS AND BROADCOM MAKES NO PROMISES,
 * REPRESENTATIONS OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR
 * OTHERWISE, WITH RESPECT TO THE SOFTWARE.  BROADCOM SPECIFICALLY
 * DISCLAIMS ANY AND ALL IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY,
 * NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF VIRUSES,
 * ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
 * CORRESPONDENCE TO DESCRIPTION. YOU ASSUME THE ENTIRE RISK ARISING
 * OUT OF USE OR PERFORMANCE OF THE SOFTWARE.
 *
 * 3. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL
 * BROADCOM OR ITS LICENSORS BE LIABLE FOR (i) CONSEQUENTIAL, INCIDENTAL,
 * SPECIAL, INDIRECT, OR EXEMPLARY DAMAGES WHATSOEVER ARISING OUT OF OR
 * IN ANY WAY RELATING TO YOUR USE OF OR INABILITY TO USE THE SOFTWARE EVEN
 * IF BROADCOM HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES; OR (ii)
 * ANY AMOUNT IN EXCESS OF THE AMOUNT ACTUALLY PAID FOR THE SOFTWARE ITSELF
 * OR U.S. $1, WHICHEVER IS GREATER. THESE LIMITATIONS SHALL APPLY
 * NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.
 *
 * $Id: pciedev_data.c  $
 */

/**
 * @file
 * Note that the firmware (of which this file is a part of) uses PROP_TXSTATUS macro's, but the host
 * driver is not built with PROP_TXSTATUS support, reason being that the prop_tx 'channel' runs
 * between wl layer and bus layer instead of between wl layer and host driver. As a consequence,
 * several PROP_TXSTATUS macro's have a different meaning than their names suggest. For instance,
 * WL_SEQ_SET_FROMDRV() does not mean that a packet was generated by the host driver.
 */

#include <typedefs.h>
#include <osl.h>
#include <hnd_cplt.h>
#include <d11_cfg.h>
#include <pciedev_priv.h>
#include <pciedev.h>
#include <pciedev_dbg.h>
#include <event_log.h>
#include <wlfc_proto.h>
#include <bcm_buzzz.h>
#include <bcmutils.h>
#include <msgtrace.h>
#include <dngl_msgtrace.h>
#include <dngl_logtrace.h>
#include <sbchipc.h>
#include <hnd_resvpool.h>

#ifdef WLCFP
#include <802.3.h>
#endif // endif
#ifdef BCMHWA
#include <hwa_export.h>
#endif // endif

/* Static definitions */
typedef int (*d2h_msg_handler)(struct dngl_bus *pciedev, void *p);
typedef void (*h2d_dma_fn_t)(struct dngl_bus *pciedev, dma64addr_t src, uint16 src_len,
        uint8 *dst, msgbuf_ring_t *ring, uint8 msgtype, uint8 dma_ch);
typedef void (*d2h_dma_fn_t)(struct dngl_bus *pciedev, dma64addr_t dst, uint16 len,
        uint8 *src, uint8 msgtype, uint8 dma_ch);
/* Static functions */
static bool pciedev_start_reading_host_buffer(struct dngl_bus *pciedev, msgbuf_ring_t *ring);
static int8 pciedev_allocate_flowring_fetch_rqst_entry(struct dngl_bus *pciedev);
static void pciedev_free_flowring_fetch_rqst_entry(struct dngl_bus *pciedev, uint8 index);
static void pciedev_rewind_flow_fetch_ptr(msgbuf_ring_t *flow_ring, uint16 index);
static void pciedev_update_rdp_ptr_unacked(msgbuf_ring_t *flow_ring);
static void pciedev_alloc_reuse_seq_list(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring);
#ifdef WL_REUSE_KEY_SEQ
static void pciedev_alloc_reuse_key_seq_list(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring);
#endif // endif
#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
static uint32 pciedev_flow_ring_age_out_check(struct dngl_bus *pciedev, void *p, uint32 status);
#endif // endif
static int pciedev_update_txstatus(struct dngl_bus *pciedev, uint32 status,
	uint16 ring_idx, uint16 flowid, uint16 seq, bool hold, uint8 *key_seq);
static void pciedev_lclpool_free_lclbuf(msgbuf_ring_t * ring, void *p);
static int pciedev_process_rxpost_msg(struct dngl_bus * pciedev, uint32 *bufid, uint16 *len,
	dma64addr_t *haddr);
static void pciedev_add_to_inuselist(struct dngl_bus *pciedev, void *p, uint8 max_items);
static void pciedev_unpack_rxpost_msg(struct dngl_bus *pciedev, void *p, uint32 *bufid,
	uint16 * len, dma64addr_t *haddr);
static int pciedev_dma_queue_avail(dma_queue_t *dmaq);
static void pciedev_enque_fetch_cmpltq(struct dngl_bus *pciedev, struct fetch_rqst *fr,
	uint8 dmach);
static struct fetch_rqst * pciedev_deque_fetch_cmpltq(struct dngl_bus *pciedev, uint8 dmach);
static void*
pciedev_get_msgbuf_host_addr(msgbuf_ring_t *ring, uint16 *available_len, uint16 max_len);
static void pciedev_ring_update_readptr(struct dngl_bus *pciedev, msgbuf_ring_t *ring,
	uint16 bytes_read);
static void pciedev_ring_update_writeptr(msgbuf_ring_t *ring, uint16 bytes_written);
static uint32 pciedev_get_ring_space(struct dngl_bus *pciedev,
	msgbuf_ring_t *ring, uint16 msglen);
static bool pciedev_resource_avail_check(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf);
static bool pciedev_check_process_d2h_message(struct dngl_bus *pciedev, void *p,
	d2h_msg_handler *rtn);
static int pciedev_process_d2h_rxpyld(struct dngl_bus *pciedev, void *p);
static int pciedev_process_d2h_txmetadata(struct dngl_bus *pciedev, void *p);
static void pciedev_queue_rxcomplete_msgring(struct dngl_bus *pciedev,  msgbuf_ring_t *ring);
static dma64addr_t pciedev_get_haddr_from_lfrag(struct dngl_bus *pciedev, void *p, uint32 *bufid,
	uint16 *dataoffset);
#ifndef HWA_TXCPLE_BUILD
static int pciedev_queue_txstatus(struct dngl_bus *pciedev, uint32 pktid,
	uint8 ifindx, uint16 ringid, uint16 txstatus, ipc_timestamp_t *ts);
#endif /* !HWA_TXCPLE_BUILD */
static int pciedev_tx_msgbuf(struct dngl_bus *bus, void *p, ret_buf_t *ret_buf,
	uint8 msg_type, uint16 msglen, msgbuf_ring_t *ring);
static uint16 pciedev_handle_h2d_pyld(struct dngl_bus *pciedev, uint8 msgtype, void *p,
	uint16 pktlen, msgbuf_ring_t *ring, uint8 dmach);
static void pciedev_process_tx_post(struct dngl_bus *bus, void *p, uint16 msglen,
	uint16 ring, uint16 fetch_ptr, uint32 cfp_enabled, bool last_packet);
static void pciedev_sendup_or_chain_lfrag(struct dngl_bus *pciedev, void *lfrag,
	msgbuf_ring_t *flow_ring, bool break_chain, bool cfp_not_capable);
static void pciedev_h2dmsgbuf_dma(struct dngl_bus *pciedev, dma64addr_t src, uint16 src_len,
	uint8 *dest, msgbuf_ring_t *ring, uint8 msgtype, uint8 dma_ch);
static void pciedev_push_pkttag_tlv_info(struct dngl_bus *pciedev, void *p,
	msgbuf_ring_t *flow_ring, uint16 index);
static void pciedev_chained_rxpkt(struct dngl_bus *pciedev, void *p);
static uint8* pciedev_get_cirbuf_pool(msgbuf_ring_t * ring, uint16 len);
static void pciedev_free_cirbuf_pool(msgbuf_ring_t * ring, void *p, uint16 len);
static void pciedev_insert_fetch_rqst(struct dngl_bus * pciedev, flow_fetch_rqst_t *node);
static void pciedev_remove_fetch_rqst_head(struct dngl_bus * pciedev);
static int pciedev_dispatch_core_fetch_rqst(struct fetch_rqst *fr, struct dngl_bus *pciedev,
	uint8 dma_ch, h2d_dma_fn_t h2d_dma, dma_mode_type_t dma_mode);

static uint16 pciedev_reuse_seq_amsdu_cnt(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring);

#if defined(BCMPCIE_IDMA)
static void pciedev_idma_reset(struct dngl_bus *pciedev, hnddma_t *di);
#endif /* BCMPCIE_IDMA */

const uint8 tid_prio_map[] = {2, 0, 1, 3, 4, 5, 6, 7};
const uint8 AC_prio_map[] = {1, 0, 2, 3};
const uint8 tid2AC_map[] = {0, 1, 1, 0, 2, 2, 3, 3};
/** AC priority map for lossless roaming */
const uint8 AC_prio_llr_map[] = {1, 0, 2, 3, 4, 4, 4, 4};
const uint8 tid2AC_llr_map[] = {0, 1, 2, 3, 3, 3, 3, 3};

static void pciedev_chained_rxpkt(struct dngl_bus *pciedev, void *p);

#ifdef BCMPOOLRECLAIM /* implemented for [TetheringOffload] / NATOE feature */
static int pciedev_put_host_addr(struct dngl_bus *pciedev, uint32 *bufid, uint16 *len,
	dma64addr_t *haddr);

static void pciedev_putback_rxpost_msg(struct dngl_bus *pciedev, void *p, uint32 *bufid,
	uint16 *len, dma64addr_t *haddr);

static int pciedev_freeup_rxcplid(pktpool_t *pool, void *arg, void *p, int rxcpl_needed);

static int pciedev_freeup_haddr(pktpool_t *pool, void *arg, void *frag, int rxcpl_needed);
#endif /* BCMPOOLRECLAIM */

static void pciedev_update_bitmaps_tracking(struct dngl_bus *pciedev,
	msgbuf_ring_t *flow_ring, uint16 rindex);
static void pciedev_tx_processed(struct dngl_bus *pciedev, void *p,
	uint32 *status, uint16 *seq, uint8 *key_seq);

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
static void pciedev_sync_d2h_write_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring);
static INLINE void pciedev_upd_last_queued_flowring(struct dngl_bus * pciedev, uint16 ringid);
static INLINE void pciedev_sync_flowring_read_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring);
#endif // endif
static void pciedev_bulk_update_txstatus(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring);

/** counter for debug purpose */
uint releasing_not_needed_rxcpl_buffer = 0;

/**
 * Called when eg a mailbox 0 interrupt was generated by the host, indicating that the host has
 * added a new message in host memory. Firmware now has to decode host read/write pointers and
 * schedule DMA to pull messages into device memory (local circular buffers).
 */
bool
pciedev_msgbuf_intr_process(struct dngl_bus *pciedev)
{
	pciedev_ctrl_resp_q_t *resp_q = pciedev->ctrl_resp_q;

	/* Host->Dongle --> DoorBell Rang
	* Doorbell intr can be triggered either due to
	* data written to data ring or control ring on host side
	* Need to check both data ring and control ring on the host,
	* but give first preference to the control ring.

	 * MsgBuf is a generic implementation and does
	 * not get into how to read and where to read into. We need to provide the core_read
	 * routine (for eg. htod_msgbuf_read_handler) and ctx which is the pciedev in our case
	 */
	/* Local queue storing responses is full
	 * so process the items in the queue
	 * then read host buffer
	 */
	if (WRITE_SPACE_AVAIL(resp_q->r_indx, resp_q->w_indx, resp_q->depth) <
		(PCIEDEV_CNTRL_CMPLT_Q_IOCTL_ENTRY +
		PCIEDEV_CNTRL_CMPLT_Q_STATUS_ENTRY +
		CTRL_SUB_BUFCNT)) {
		pciedev->ctrl_resp_q->status |= CTRL_RESP_Q_FULL;
		pciedev_process_ctrl_cmplt(pciedev);
		DBG_BUS_INC(pciedev, pciedev_msgbuf_intr_process);
		return FALSE;
	}

	/* CHECK the read/write offsets and call the right routines */
	if (pciedev->ctrl_lock == FALSE) { /* block ioctls when a prior one is pending */
		pciedev_start_reading_host_buffer(pciedev, pciedev->htod_ctrl);
	}

	/* the above call could change the ioctl lock status */
	if (pciedev->ctrl_lock == FALSE) {
#if !defined(BCMHWA) || !defined(HWA_RXPOST_BUILD)
		pciedev_start_reading_host_buffer(pciedev, pciedev->htod_rx);
#endif // endif
		pciedev_schedule_flow_ring_read_buffer(pciedev);
	}

	return FALSE;
} /* pciedev_msgbuf_intr_process */

/**
 * After the host notified the device that new message(s) are available in host memory, the device
 * needs to pull these message(s) into local memory. This function kicks off the m2m DMA transfer
 * for that. When this function returns, DMA can still be in progress.
 *
 * ring: a ring of local buffers. The 'pciedev' device contains multiple message rings.
 */
static bool
pciedev_start_reading_host_buffer(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
#ifdef PCIE_DEBUG_CYCLE_COUNT
	static uint32 cbuf_time = 0, tot_time = 0;
	static int iter = 0;
	uint32 start_time = osl_getcycles();
#endif /* PCIE_DEBUG_CYCLE_COUNT */

	uint8 *dest_addr;	/**< address on the device */
	uint8 *host_addr;	/**< lower 32 bits of an address on the host */
	uint16 src_len;
	uint32 txdesc, rxdesc;
	int dma_qavail;

	PCI_TRACE(("pciedev_start_reading_host_buffer\n"));

	if (pciedev_ds_in_host_sleep(pciedev)) {
		return FALSE;
	}
	if (!ring->inited || ring->paused) {
		DBG_BUS_INC(pciedev, pciedev_start_reading_host_buffer);
		return FALSE;
	}

	while (1) {
		/* First check if there is any message space available in the circular buffer */
		if (MSGBUF_READ_AVAIL_SPACE(ring) == 0) {
			PCI_TRACE(("pciedev_start_reading_host_buffer: no space in cir buf\n"));
			DBG_BUS_INC(pciedev, pciedev_start_reading_host_buffer);
			return FALSE;
		}
		/* Before proceeding with circular buffer, check if enough descrs are available */
		/* minimum of 2 descriptors are required per messages */
		/* another 2 are required for ioctl request */
		txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, HTOD, TXDESC);
		rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, HTOD, RXDESC);
		dma_qavail = pciedev_dma_queue_avail(pciedev->htod_dma_q[pciedev->default_dma_ch]);
		if (txdesc < MIN_TXDESC_AVAIL || rxdesc < MIN_RXDESC_AVAIL || dma_qavail <= 1) {
			DBG_BUS_INC(pciedev, pciedev_start_reading_host_buffer);
			return TRUE;
		}

		if (!LCL_BUFPOOL_AVAILABLE(ring)) {
			PCI_TRACE(("Ring: %c%c%c%c Local ring bufs not available,"
				"Dont read out from host ring \n",
				ring->name[0], ring->name[1], ring->name[2], ring->name[3]));
			DBG_BUS_INC(pciedev, pciedev_start_reading_host_buffer);
			return TRUE;
		}

		/* Allocs lcl buffers / lcl msg ring items that are going to contain host message,
		 * advances ring->fetch_ptr.
		 */
		host_addr = pciedev_get_msgbuf_host_addr(ring, &src_len, 0);
		if (host_addr == NULL || src_len == 0) {
			DBG_BUS_INC(pciedev, pciedev_start_reading_host_buffer);
			return FALSE;
		}

		/* Gets one buffer from a pool of local buffers */
		dest_addr = pciedev_lclpool_alloc_lclbuf(ring);
		if (dest_addr != NULL) {
			/* Now we have the src details as well the dest details to copy/dma the data
			 * from htod_msgbuf into the local_htod ring. Schedule the DMA now.
			 */
			dma64addr_t haddr;

			pciedev->msg_pending++;

			PHYSADDR64HISET(haddr, HADDR64_HI(MSGBUF_HADDR64(ring)));
			PHYSADDR64LOSET(haddr, (uint32) host_addr);

			/* kicks off dma. next stop: pciedev_handle_h2d_dma */
			pciedev_h2dmsgbuf_dma(pciedev, haddr, src_len, dest_addr,
				ring, MSG_TYPE_API_MAX_RSVD, pciedev->default_dma_ch);
		} else {
			PCI_TRACE(("pciedev_start_reading_host_buffer\n"));
			DBG_BUS_INC(pciedev, pciedev_start_reading_host_buffer);
			return TRUE;
		}
	}

	return FALSE;
} /* pciedev_start_reading_host_buffer */

/**
 * Host can request device to 'flush' a flow ring, by issuing either a 'flush' or a 'delete'
 * message. When the ring is flushed, firmware has to send a 'flush complete' message to the host.
 *
 * If we have pending flush and need to drain more, fetch those; otherwise if there are pending
 * flush/delete response then send them now.
 */
void
pciedev_process_pending_flring_resp(struct dngl_bus * pciedev, msgbuf_ring_t *flow_ring)
{
	ASSERT(flow_ring != NULL);

	PCI_TRACE(("pciedev_process_pending_flring_resp: ringid=%d; flow_ring->status:0x%x\n",
		flow_ring->ringid, flow_ring->status));

	if (((MSGBUF_WR(flow_ring) == MSGBUF_RD(flow_ring)) ||
		(((MSGBUF_WR(flow_ring) == flow_ring->fetch_ptr) ||
		(flow_ring->status & FLOW_RING_FAST_DELETE_ACTIVE)) &&
		flow_ring->flow_info.pktinflight == 0 && flow_ring->fetch_pending == 0)) &&
		flow_ring->d2h_q_txs_pending == 0) {
		flow_ring->status &= ~FLOW_RING_FLUSH_PENDING;
		if (flow_ring->status & FLOW_RING_FLUSH_RESP_PENDING) {
			pciedev_send_flow_ring_flush_resp(pciedev, flow_ring->ringid,
			BCMPCIE_SUCCESS, NULL);
		} else if (flow_ring->status & FLOW_RING_DELETE_RESP_PENDING) {
			PCIEDEV_XMIT_TXSTATUS(pciedev);
			pciedev_send_flow_ring_delete_resp(pciedev, flow_ring->ringid,
			BCMPCIE_SUCCESS, NULL);
		}
	} else
		pciedev_h2d_start_fetching_host_buffer(pciedev, flow_ring);
}

/** to delete elements from flowrings on timeout */
void
pciedev_flow_ageing_timerfn(dngl_timer_t *t)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through active queues */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (!flow_ring->reuse_seq_list) {
				cur = dll_next_p(cur);
				continue;
			} else {
				if (flow_ring->ring_ageing_info.sup_cnt == 0) {
					pciedev_free_reuse_seq_list(pciedev, flow_ring);
				} else
					flow_ring->ring_ageing_info.sup_cnt = 0;
			}
			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
}

#ifdef WLATF_DONGLE
/** Normalize raw weight (airtime) from WL layer to pciedev weight.
 * Since smaller airtime is corresponds to "heavier" weight
 * therefore need to do invert calculation to derive the weight
 * from the airtime.
 */
static INLINE void
pciedev_normalize_flow_ring_weight(struct dngl_bus *pciedev,
		msgbuf_ring_t *sch_flow_rings[], uint32 num)
{
	uint32 i;
	uint32 W = 0, w;
	msgbuf_ring_t *flow_ring;

	for (i = 0; i < num; i++) {
		flow_ring = sch_flow_rings[i];
		w = SCHEDCXT_SUM_WEIGHT(pciedev) / SCHEDCXT_FL_WEIGHT(flow_ring);
		W += w;
		SCHEDCXT_FL_WEIGHT(flow_ring) = w;
	}
	if (num > 0) {
		SCHEDCXT_SUM_WEIGHT(pciedev) = W;
		ASSERT(SCHEDCXT_SUM_WEIGHT(pciedev));
	}
}
#endif /* WLATF_DONGLE */

#ifdef WLSQS
static inline void sqs_v2r_revert(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf,
	uint16 v2r_request);
static inline void sqs_vpkts_rewind(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf,
	uint16 count);
static inline void sqs_vpkts_forward(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf,
	uint16 count);

static inline void
sqs_v2r_revert(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf, uint16 v2r_request)
{
	uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, msgbuf);

	if (CFP_FLOWID_VALID(msgbuf->cfp_flowid)) {
		wlc_sqs_v2r_revert(msgbuf->cfp_flowid, tid_ac, v2r_request);
	}
}

static inline void
sqs_vpkts_rewind(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf, uint16 count)
{
	uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, msgbuf);

	if (CFP_FLOWID_VALID(msgbuf->cfp_flowid)) {
		wlc_sqs_vpkts_rewind(msgbuf->cfp_flowid, tid_ac, count);
	}
}

static inline void
sqs_vpkts_forward(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf, uint16 count)
{
	uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, msgbuf);

	if (CFP_FLOWID_VALID(msgbuf->cfp_flowid)) {
		wlc_sqs_vpkts_forward(msgbuf->cfp_flowid, tid_ac, count);
	}
}
/* Check if the Flow ring is open for data transfer */
bool
sqs_flow_ring_active(void* arg, uint16 ringid)
{
	struct dngl_bus *pciedev = (struct dngl_bus *)arg;
	msgbuf_ring_t *msgbuf;

	msgbuf = pciedev->h2d_submitring_ptr[ringid];
	ASSERT(msgbuf);

	/* Return 0 is suppression pending */
	if (msgbuf->status & FLOW_RING_SUP_PENDING) {
		return FALSE;
	} else {
		return TRUE;
	}

}
/* Schedule a HW 3.a request to fetch packets from flow ring.
 * @param ringid  Flow irngid
 * @param v2r_request  No of packets to be fetched.
 *
 * This may fail to schedule the fetch request for all the requested packets.
 * Return back the remaining no of packets to be fetched.
 */
uint16 /* Handle a V2R request */
sqs_v2r_request(void* arg, uint16 ringid, uint16 v2r_request)
{
	struct dngl_bus *pciedev = (struct dngl_bus *)arg;
	msgbuf_ring_t *msgbuf;
	uint16 wi_index;
	int trans_id;
	uint8 ifidx = 0;
	uint16 ret_len = v2r_request;
	uint16 transfer_count;
	uint16 process_len;
	uint16 remain_len;
	uint16 maxpktcnt;
#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
	uint32 inuse;
	uint32 available;
#endif // endif
	uint8 tid_ac;

	msgbuf = pciedev->h2d_submitring_ptr[ringid];
	ASSERT(msgbuf);

	/* TBD: Should we check the available lfrags too? */

	tid_ac = PCIEDEV_TID_REMAP(pciedev, msgbuf);

	/* Block V2R request if suppression pending */
	if (msgbuf->status & FLOW_RING_SUP_PENDING) {
		return 0;
	}

#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
	/* For sliding windows/short bitmaps, let's make sure max_fetch
	 * does not include RDP - RD.
	 */
	if (msgbuf->fetch_ptr >= MSGBUF_RD(msgbuf))  {
		inuse = msgbuf->fetch_ptr - MSGBUF_RD(msgbuf);
	} else {
		inuse = MSGBUF_MAX(msgbuf) - MSGBUF_RD(msgbuf) + msgbuf->fetch_ptr;
	}
	available = msgbuf->bitmap_size - inuse - 1;
	ret_len = MIN((int)ret_len, (int)available);
#endif /* FLOWRING_SLIDING_WINDOW || FLOWRING_USE_SHORT_BITSETS */
	if (ret_len) {
		ret_len = bcm_count_zeros_sequence((uint32*)msgbuf->inprocess,
			MSGBUF_MODULO_IDX(msgbuf, msgbuf->fetch_ptr),
			ret_len, msgbuf->bitmap_size);
	}

	if (ret_len != v2r_request) {
		PCI_TRACE(("%s: v2r_request<%u> schedcmd<%u> fetch_ptr<%u>\n",
			__FUNCTION__, v2r_request, ret_len, msgbuf->fetch_ptr));
	}

	/* Nothing to fetch */
	if (!ret_len) {
		pciedev_update_rdp_ptr_unacked(msgbuf);
		return 0;
	}

	process_len = 0;
	remain_len = ret_len;
	maxpktcnt = msgbuf->flow_info.maxpktcnt;
	wi_index = msgbuf->fetch_ptr;

	while (remain_len) {
		transfer_count = MIN(remain_len, maxpktcnt);
		trans_id = hwa_txpost_schedcmd_request(hwa_dev,
			msgbuf->ringid, wi_index, transfer_count);
		if (trans_id == HWA_FAILURE) {
			/* schedcmd could fail due to queue full */
			PCI_TRACE(("%s: hwa_txpost_schedcmd_request failed\n", __FUNCTION__));
			break;
		}

		ASSERT(remain_len >= transfer_count);

		remain_len -= transfer_count;
		process_len += transfer_count;
		wi_index = (wi_index + transfer_count) % MSGBUF_MAX(msgbuf);
	}

	ASSERT((process_len + remain_len) == ret_len);

	if (process_len) {
		set_bitrange(msgbuf->inprocess,
			MSGBUF_MODULO_IDX(msgbuf, msgbuf->fetch_ptr),
			MSGBUF_MODULO_IDX(msgbuf, msgbuf->fetch_ptr +
			process_len - 1), msgbuf->bitmap_size - 1);
		msgbuf->flow_info.pktinflight += process_len;
		msgbuf->fetch_ptr = wi_index;

		BUZZZ_KPI_QUE1(KPI_QUE_BUS_RP_UPD, 2, msgbuf->fetch_ptr, msgbuf->ringid);

		if (MSGBUF_RD(msgbuf) > msgbuf->fetch_ptr) {
			msgbuf->current_phase = msgbuf->current_phase ?
				0 : MSGBUF_RING_INIT_PHASE;
		}
		/* Update per ifidx stats */
		ifidx = msgbuf->flow_info.ifindex;
		pciedev->ifidx_account[ifidx].cur_cnt += process_len;
		pciedev->pend_user_tx_pkts += process_len;
		/* Check inprocess & fetch_ptr */
		pciedev_update_rdp_ptr_unacked(msgbuf);

		/* Now since HWA 3a scheduling is done, update v2r counters now */
		if (CFP_FLOWID_VALID(msgbuf->cfp_flowid)) {
			wlc_sqs_v2r_enqueue(msgbuf->cfp_flowid, tid_ac, process_len);
		}
	}

	return process_len;
} /* sqs_v2r_request */

/* ---------------- End of SQS V2R Convertor Subsystem ---------------------- */

static inline int sqs_sendup(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf, bool* taf_schedule);
static inline int sqs_sendup_force(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf);
static inline void pciedev_schedule_sqs_timer(struct dngl_bus *pciedev);

/**
 * return value: 0 = no vpkts to convert, positive value: v2r convert and release,
 * negative value: some error condition TBD
 */
static inline int
sqs_sendup(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf, bool* taf_schedule)
{
	int v2r_request = 0;
	int v_pkts;	/* new offered load for a flowring */
	/* Remap the flowring priority */
	uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, msgbuf);

	ASSERT(CFP_FLOWID_VALID(msgbuf->cfp_flowid));

	/* Retrieve the number of new coming packets */
	v_pkts = MSGBUF_VPKTS(msgbuf);
	if (v_pkts)
		BUZZZ_KPI_QUE1(KPI_QUE_BUS_WR_UPD, 2, MSGBUF_WR(msgbuf), msgbuf->ringid);

	if (wlc_sqs_capable(msgbuf->cfp_flowid, tid_ac)) {

		/* Enqueue the new virtual packets to SCB precedence Queue */
		if (pciedev->taf_scheduler) {
			if (v_pkts) {
				*taf_schedule = TRUE;
				wlc_sqs_vpkts_enqueue(msgbuf->cfp_flowid, tid_ac, v_pkts);
			}
		} else {
			if (v_pkts || wlc_sqs_vpkts(msgbuf->cfp_flowid, tid_ac)) {
				v2r_request = wlc_sqs_sendup(msgbuf->cfp_flowid, tid_ac, v_pkts);
			}
		}

		PCI_TRACE(("SQS sendup : flow id %d prio %d   New V pkts %d Total V pkts %d "
			"New V2R %d Curr v2r rqst %d Curr N pkts %d "
			"Curr in transit pkts %d  Curr tbr pkts %d \n",
			msgbuf->cfp_flowid, tid_ac, v_pkts,
			wlc_sqs_vpkts(msgbuf->cfp_flowid, tid_ac),
			v2r_request, wlc_sqs_v2r_pkts(msgbuf->cfp_flowid, tid_ac),
			wlc_sqs_n_pkts(msgbuf->cfp_flowid, tid_ac),
			wlc_sqs_in_transit_pkts(msgbuf->cfp_flowid, tid_ac),
			wlc_sqs_tbr_pkts(msgbuf->cfp_flowid, tid_ac)));
	} else {
		/* BC/MC or other non-AMPDU packets are sent through the legacy path.
		 * The workitems would be retrieved through the HWA3a block and
		 * are expected to go through the legacy path in hwa_txpost_sendup()
		 * as there is no valid CFP flow for BC/MC and non-AMPDU packets.
		 */
		PCI_TRACE(("sqs_sendup: no AMPDU, transmit in legacy path\n"));
		v2r_request = MSGBUF_READ_AVAIL_SPACE(msgbuf);

		/* Update virtual pakcets for non BC/MC flowing */
		if (!(ETHER_ISMULTI(msgbuf->flow_info.da) && msgbuf->flow_info.iftype_ap)) {
			if (v_pkts) {
				wlc_sqs_vpkts_enqueue(msgbuf->cfp_flowid, tid_ac, v_pkts);
			}

			v2r_request = MIN(v2r_request, wlc_sqs_vpkts(msgbuf->cfp_flowid, tid_ac));

			ASSERT(v2r_request >= 0);
		}
	}

	return v2r_request;
}

/**
 * This function will be called from below path:
 * 1. Packet processing related to power save mode for AP mode.
 * 2. Flow ring flush processing.
 */
static inline int
sqs_sendup_force(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf)
{
	uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, msgbuf);
	int v_pkts;	/* new offered load for a flowring */
	int v2r_request;
	int ret = 0;

	v2r_request = MIN(MSGBUF_READ_AVAIL_SPACE(msgbuf), msgbuf->flow_info.maxpktcnt);

	if (CFP_FLOWID_VALID(msgbuf->cfp_flowid)) {

		/* Update virtual pakcets for non BC/MC flowing */
		if (!(ETHER_ISMULTI(msgbuf->flow_info.da) && msgbuf->flow_info.iftype_ap)) {
			/* Retrieve the number of new coming packets */
			v_pkts = MSGBUF_VPKTS(msgbuf);
			if (v_pkts) {
				wlc_sqs_vpkts_enqueue(msgbuf->cfp_flowid, tid_ac, v_pkts);
			}

			/* Estimate the v2r fetch count */
			v2r_request = MIN(v2r_request, wlc_sqs_vpkts(msgbuf->cfp_flowid, tid_ac));

			ASSERT(v2r_request >= 0);
		}
	}

	if (v2r_request > 0) {
		/* Forward v2r_request to sqs_v2r subsystem, to allocate
		 * lfrags, convert virtual to real, and pass up real on
		 * completion of DMA.
		 */
		ret = sqs_v2r_request(pciedev, msgbuf->ringid, v2r_request);

		if (ret) {
			/* Setup HWA flag to indicate its a SQS force request */
			hwa_txpost_schedcmd_flags_update(hwa_dev,
				TXPOST_SCHED_FLAGS_SQS_FORCE);
		}
	}

	return ret;

}

static inline void pciedev_schedule_sqs_timer(struct dngl_bus *pciedev)
{
	if (!pciedev_ds_in_host_sleep(pciedev) && !pciedev->flow_sch_timer_on) {
		dngl_add_timer(pciedev->flow_schedule_timer, 0, FALSE);
		pciedev->flow_sch_timer_on = TRUE;
	}
}
#endif /* WLSQS */

/** Temp PULL model/partial push : Check for write pointer updates from all data flow rings */
void
pciedev_schedule_flow_ring_read_buffer(struct dngl_bus *pciedev)
{
#if defined(WLSQS)
	msgbuf_ring_t *flow_ring;
	struct dll *cur, *prio, *last_fetch_node_bkp;
	int v2r_request;
	bool sqs_fetch_allowed = TRUE;
	uint8 tid_ac;
	bool taf_schedule = FALSE;

	if (pciedev_ds_in_host_sleep(pciedev)) {
		DBG_BUS_INC(pciedev, pciedev_schedule_flow_ring_read_buffer);
		return;
	}

	/* To avoid recursive strides if any pktfree happens */
	if (pciedev->sqs_stride_in_progress) {
		PCI_ERROR(("%s: SQS stride in progress\n", __FUNCTION__));
		return;
	}
	pciedev->sqs_stride_in_progress = TRUE;

	/* Reusing the priority rings as a stride circle, assuming
	 * there is only one prioring and always stride from
	 * the first prioring.
	 */
	/* get first priority ring out of pool */
	prio = dll_head_p(&pciedev->active_prioring_list);

	/* loop through all active priority rings */
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;

		/* start from the last fetch node to be fair to all rings */
		cur = dll_next_p(prioring->last_fetch_node);
		last_fetch_node_bkp = prioring->last_fetch_node;

		/* loop through all active flow rings */
		while (1) {
			/* skip the head node which does not hold any info */
			if (dll_end(cur, &prioring->active_flowring_list))
				goto nextnode;

			flow_ring = ((flowring_pool_t *)cur)->ring;

			/* Remap the flowring priority */
			tid_ac = PCIEDEV_TID_REMAP(pciedev, flow_ring);

			/* Force to fetch flow ring work items. */
			if (flow_ring->status & FLOW_RING_FLUSH_PENDING) {
				sqs_sendup_force(pciedev, flow_ring);
				goto nextnode;
			}

#ifdef WLCFP
			/* Give a chance to do relink if it's just roaming. */
			if ((flow_ring->cfp_flowid == CFP_FLOWID_INVALID) &&
				(flow_ring->status & FLOW_RING_ROAM_CHECK)) {
				int response_cfp;
				response_cfp = dngl_bus_cfp_link(pciedev->dngl,
					flow_ring->flow_info.ifindex,
					flow_ring->ringid, tid_ac,
					flow_ring->flow_info.da, CFP_FLOWID_LINK,
					&flow_ring->tcb_state, &flow_ring->cfp_flowid);
				if (response_cfp == BCME_OK) {
					PCI_PRINT(("CFP relink succeeded for ringid<%d>"
						" RD<%u> WR<%u> fetch_ptr<%u> wr_peeked<%u>\n",
						flow_ring->ringid,
						MSGBUF_RD(flow_ring), MSGBUF_WR(flow_ring),
						flow_ring->fetch_ptr, flow_ring->wr_peeked));
					/* Sync the wr_peeked pointer */
					flow_ring->wr_peeked = flow_ring->fetch_ptr;
				}
				flow_ring->status &= ~FLOW_RING_ROAM_CHECK;
			}
#endif /* WLCFP */

			if (!CFP_FLOWID_VALID(flow_ring->cfp_flowid)) {
				PCI_TRACE(("%s: SQS flow is not ready\n", __FUNCTION__));
				goto nextnode;
			}

			/* Check write pointer for the open interface ports of the flow
			 * ring and mark it for pending pkt pull
			 */
			if (((flow_ring->status & FLOW_RING_CLOSED) &&
				(!(flow_ring->flow_info.flags & FLOW_RING_FLAG_INFORM_PKTPEND) ||
				(flow_ring->flow_info.flags & FLOW_RING_FLAG_LAST_TIM))) ||
				(flow_ring->status & FLOW_RING_FAST_DELETE_ACTIVE) ||
				!MSGBUF_READ_AVAIL_SPACE(flow_ring)) {
				/* No packets for this flowring */
				goto nextnode;
			}

			if (!flow_ring->flow_info.pktinflight && !flow_ring->fetch_pending)
				flow_ring->status &= ~FLOW_RING_SUP_PENDING;

			if (flow_ring->status & FLOW_RING_SUP_PENDING)
				goto nextnode;

			if ((flow_ring->status & FLOW_RING_CLOSED)) {
				if ((flow_ring->flow_info.flags & FLOW_RING_FLAG_INFORM_PKTPEND) &&
					!(flow_ring->flow_info.flags & FLOW_RING_FLAG_LAST_TIM)) {
					/* informs WL subsystem about the TIM_SET operation */
					bool ret;
					ret = dngl_flowring_update(pciedev->dngl,
						flow_ring->flow_info.ifindex,
						(uint8) flow_ring->handle,
						FLOW_RING_TIM_SET,
						(uint8 *)&flow_ring->flow_info.sa,
						(uint8 *)&flow_ring->flow_info.da, tid_ac, NULL);

					if (ret) {
						flow_ring->flow_info.flags |=
							FLOW_RING_FLAG_LAST_TIM;
					} else {
						uint16 count;
						count = pciedev_reuse_seq_amsdu_cnt(pciedev,
								flow_ring);
						flow_ring->flow_info.maxpktcnt = count;
						flow_ring->flow_info.reqpktcnt += count;
						flow_ring->flow_info.flags |=
							FLOW_RING_FLAG_PKT_REQ;
						sqs_sendup_force(pciedev, flow_ring);
						flow_ring->flow_info.maxpktcnt =
						prioring->maxpktcnt[flow_ring->flow_info.ifindex];
					}
				}
				goto nextnode;
			}

			v2r_request = sqs_sendup(pciedev, flow_ring, &taf_schedule);
			if (sqs_fetch_allowed && (v2r_request > 0)) {
				/* Forward v2r_request to sqs_v2r subsystem, to allocate
				 * lfrags, convert virtual to real, and pass up real on
				 * completion of DMA.
				 */
				int ret;
				uint16 request = v2r_request;

				ret = sqs_v2r_request(pciedev, flow_ring->ringid, request);
				if (ret) {
					prioring->last_fetch_node = cur;
					pciedev->last_fetch_prio = prio;
				} else {
					/* Block the v2r fetch on the very first failure */
					sqs_fetch_allowed =  FALSE;
				}
			}

nextnode:
			if (cur == last_fetch_node_bkp)
				break;

			/* get next flow ring node */
			cur = dll_next_p(cur);
		} /* while(): loop through all active flow rings */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while(): loop through all active priority rings */

	pciedev->sqs_stride_in_progress = FALSE;

	/* Trigger TAF release algorithm */
	if (taf_schedule)
		wlc_sqs_taf_txeval_trigger();

#else /* !WLSQS */
	bool flow_schedule = FALSE;
	msgbuf_ring_t *flow_ring;
	struct dll *cur, *prio;
	uint8 loop[pciedev->max_slave_devs], schedule[pciedev->max_slave_devs];

#ifdef WLATF_DONGLE
	msgbuf_ring_t *sch_flows_idx[BCMPCIE_MAX_TX_FLOWS];
	uint16 sch_flows_num = 0;
	bool normalize_flow = FALSE;

	/* The scheduler would changes the state for FF scheduler based
	 * on whether the FF Scheduler feature is disabled or enabled.
	 */
	if (!FFSHCED_ENAB(pciedev) &&
		(FFSHCED_ST_ENABLED(pciedev) || FFSHCED_ST_UNKNOWN(pciedev)))
		FFSHCED_ST(pciedev) = FFSCH_STATE_DISABLED;
	else if (FFSHCED_ENAB(pciedev) &&
		(FFSHCED_ST_DISABLED(pciedev) || FFSHCED_ST_UNKNOWN(pciedev))) {
		FFSHCED_ST(pciedev) = FFSCH_STATE_ENABLED;
	}
	if (FFSHCED_ENAB(pciedev)) {
		bzero(sch_flows_idx, sizeof(sch_flows_idx));
	}
#endif /* WLATF_DONGLE */

	if (pciedev_ds_in_host_sleep(pciedev) || pciedev->flow_sch_timer_on) {
		DBG_BUS_INC(pciedev, pciedev_schedule_flow_ring_read_buffer);
		return;
	}

	bzero(loop, sizeof(loop));
	bzero(schedule, sizeof(schedule));

	/* get first priority ring out of pool */
	prio = dll_head_p(&pciedev->active_prioring_list);

#ifdef WLATF_DONGLE
	if (FFSHCED_ST_ENABLED(pciedev)) {
		SCHEDCXT_SUM_WEIGHT(pciedev) = 0;
	}
#endif // endif

	/* loop through all active priority rings */
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;

		prioring->schedule = FALSE;

		/* get first flow ring out of pool */
		cur = dll_head_p(&prioring->active_flowring_list);

		/* loop through all active flow rings */
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
#ifdef WLATF_DONGLE
			if (FFSHCED_ENAB(pciedev)) {
				SCHEDCXT_FL_WEIGHT(flow_ring) = 0;
			}
#endif /* WLATF_DONGLE */

#ifdef WLCFP
			/* Give a chance to do relink if it's just roaming. */
			if ((flow_ring->cfp_flowid == CFP_FLOWID_INVALID) &&
				(flow_ring->status & FLOW_RING_ROAM_CHECK)) {
				int response_cfp;
				uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, flow_ring);
				response_cfp = dngl_bus_cfp_link(pciedev->dngl,
					flow_ring->flow_info.ifindex,
					flow_ring->ringid, tid_ac,
					flow_ring->flow_info.da, CFP_FLOWID_LINK,
					&flow_ring->tcb_state, &flow_ring->cfp_flowid);
				if (response_cfp == BCME_OK) {
					PCI_PRINT(("CFP relink succeeded for ringid %d\n",
						flow_ring->ringid));
				}
				flow_ring->status &= ~FLOW_RING_ROAM_CHECK;
			}
#endif /* WLCFP */

			/* Check write pointer for the open interface ports of the flow
			 * ring and mark it for pending pkt pull
			 */
			if (((flow_ring->status & FLOW_RING_CLOSED) &&
				(!(flow_ring->flow_info.flags & FLOW_RING_FLAG_INFORM_PKTPEND) ||
				(flow_ring->flow_info.flags & FLOW_RING_FLAG_LAST_TIM))) ||
				(flow_ring->status & FLOW_RING_FAST_DELETE_ACTIVE) ||
				!MSGBUF_READ_AVAIL_SPACE(flow_ring)) {
#ifdef WLATF_DONGLE
				/* No packets for this flowring.
				 * Setting PKT_IDLE status bit.
				 * Resetting the weight to initial default.
				 */
				if (FFSHCED_ST_ENABLED(pciedev)) {
					if (!(flow_ring->status & FLOW_RING_PKT_IDLE)) {
						flow_ring->status |= FLOW_RING_PKT_IDLE;
						pciedev_reset_flowring_weight(pciedev, flow_ring);
					}
				}
#endif /* WLATF_DONGLE */

				cur = dll_next_p(cur);
				continue;
			}

			flow_ring->status |= FLOW_RING_PKT_PENDING;
			prioring->schedule = TRUE;
			schedule[flow_ring->flow_info.ifindex] = 1;

#ifdef WLATF_DONGLE
			if (FFSHCED_ST_ENABLED(pciedev)) {
				flow_ring->status &= ~FLOW_RING_PKT_IDLE;
				/* sum raw weight */
				SCHEDCXT_SUM_WEIGHT(pciedev) += FL_W_NEW(flow_ring);
				/* scheduling cycle context weight */
				SCHEDCXT_FL_WEIGHT(flow_ring) = FL_W_NEW(flow_ring);
				sch_flows_idx[sch_flows_num++] = flow_ring;
				ASSERT(SCHEDCXT_SUM_WEIGHT(pciedev));
			}
#endif /* WLATF_DONGLE */

			/* get next flow ring node */
			cur = dll_next_p(cur);
		} /* while(): loop through all active flow rings */

		/* adjust maxpktcnt to make the hightest priority ring use max fetch count,
		 * and others use half of previous priority per ifidx.
		 */
		if (prioring->schedule) {
			int i;
			for (i = 0; i < pciedev->max_slave_devs; i++) {
				if (schedule[i]) {
					prioring->maxpktcnt[i] = MAX(PCIEDEV_MIN_PACKETFETCH_COUNT,
						pciedev->tunables[MAXPKTFETCH] / (1 << loop[i]));
					loop[i] += 1;
					schedule[i] = 0;
				}
			}
			flow_schedule |= prioring->schedule;
#ifdef WLATF_DONGLE
			/* The fair weight scheduler has to track the flag separately depending on
			 * the enable state.
			 */
			if (FFSHCED_ST_ENABLED(pciedev)) {
				normalize_flow |= prioring->schedule;
				ASSERT(SCHEDCXT_SUM_WEIGHT(pciedev));
			}
#endif /* WLATF_DONGLE */
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while(): loop through all active priority rings */

	if (flow_schedule && !pciedev->flow_sch_timer_on) {
#ifdef WLATF_DONGLE
		if (FFSHCED_ST_ENABLED(pciedev) && normalize_flow) {
			if (sch_flows_num > 0) {
				ASSERT(SCHEDCXT_SUM_WEIGHT(pciedev));
				pciedev_normalize_flow_ring_weight(pciedev, sch_flows_idx,
					sch_flows_num);
			}
		}
#endif /* WLATF_DONGLE */
		dngl_add_timer(pciedev->flow_schedule_timer, 0, FALSE);
		pciedev->flow_sch_timer_on = TRUE;
	}
#endif /* !WLSQS */
} /* pciedev_schedule_flow_ring_read_buffer */

#ifdef WLATF_DONGLE
static inline void
pciedev_flow_calc_maxlfrags(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
	ASSERT(SCHEDCXT_FL_WEIGHT(flow_ring));
	ASSERT(SCHEDCXT_SUM_WEIGHT(pciedev));

#ifdef BCMFRAGPOOL
	SCHEDCXT_FL_MAXLFRAGS(flow_ring) = pktpool_max_pkts(pciedev->pktpool_lfrag);
#endif // endif
#if defined(BCMHWA) && defined(HWA_TXPOST_BUILD)
	SCHEDCXT_FL_MAXLFRAGS(flow_ring) = HWA_TXPATH_PKTS_MAX;
#endif // endif

	/* The smaller weight number is assumed "heavier".
	 * Therefore need to do invert calculation.
	 * w = 1 - (w/W), where W is sum of all weights.
	 */
	if (SCHEDCXT_SUM_WEIGHT(pciedev) > SCHEDCXT_FL_WEIGHT(flow_ring)) {

		/* Shaping max lfrag count */
		SCHEDCXT_FL_MAXLFRAGS(flow_ring) =
			(SCHEDCXT_FL_MAXLFRAGS(flow_ring) * SCHEDCXT_FL_WEIGHT(flow_ring)) /
			SCHEDCXT_SUM_WEIGHT(pciedev);

		if (SCHEDCXT_FL_MAXLFRAGS(flow_ring) == 0) {
			SCHEDCXT_FL_MAXLFRAGS(flow_ring) = PCIEDEV_MIN_SHCEDLFRAG_COUNT;
		}

	}
}

#ifdef BCMFRAGPOOL
static inline void
pciedev_flow_calc_maxpktcnt(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
	ASSERT(SCHEDCXT_FL_WEIGHT(flow_ring));
	ASSERT(SCHEDCXT_SUM_WEIGHT(pciedev));

	if (SCHEDCXT_SUM_WEIGHT(pciedev) > SCHEDCXT_FL_WEIGHT(flow_ring)) {

		/* Shaping pkt max fetch count */
		FL_MAXPKTCNT(flow_ring) =
			(FL_MAXPKTCNT(flow_ring) * SCHEDCXT_FL_WEIGHT(flow_ring)) /
			SCHEDCXT_SUM_WEIGHT(pciedev);
		if (FL_MAXPKTCNT(flow_ring) == 0)
			FL_MAXPKTCNT(flow_ring) = PCIEDEV_MIN_PACKETFETCH_COUNT;
	}
}
#endif /* BCMFRAGPOOL */
#endif  /* WLATF_DONGLE */

/**
 * One shot timer function, this timer callback was scheduled for Data Flow rings.
 * Pull the packets from the host and send up as required
 */
void
pciedev_flow_schedule_timerfn(dngl_timer_t *t)
{
#if defined(WLSQS)
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);

	pciedev->flow_sch_timer_on = FALSE;
	pciedev_schedule_flow_ring_read_buffer(pciedev);
#else
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	int	fetch = 1;
	bool ret;
	bool skip_update = FALSE;
#ifdef WLATF_DONGLE
	bool any_fetch = FALSE;
#endif // endif

	pciedev->flow_sch_timer_on = FALSE;

	if (pciedev_ds_in_host_sleep(pciedev)) {
		DBG_BUS_INC(pciedev, pciedev_flow_schedule_timerfn);
		return;
	}

	/* start fom the last fetch prio to be fair to low prios */
	prio = dll_next_p(pciedev->last_fetch_prio);

	/* loop through all active priority rings */
	while (fetch) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;

		/* skip the head node which does not hold any info */
		if (dll_end(prio, &pciedev->active_prioring_list))
			goto nextprio;

		if (!prioring->schedule) {
			/* all flowrings in the prio are empty, skip and move on */
			goto nextprio;
		}

		/* start from the last fetch node to be fair to all rings */
		cur = dll_next_p(prioring->last_fetch_node);

		/* loop through all nodes */
		while (1) {
			/* skip the head node which does not hold any info */
			if (dll_end(cur, &prioring->active_flowring_list))
				goto nextnode;

			/* get flow ring from nodes */
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if (!flow_ring->flow_info.pktinflight &&
				!flow_ring->fetch_pending)
				flow_ring->status &= ~FLOW_RING_SUP_PENDING;

			if (flow_ring->status & FLOW_RING_SUP_PENDING)
				goto nextnode;

			flow_ring->flow_info.maxpktcnt =
				prioring->maxpktcnt[flow_ring->flow_info.ifindex];

			if ((flow_ring->status & FLOW_RING_CLOSED)) {
				if ((flow_ring->status & FLOW_RING_PKT_PENDING) &&
					(flow_ring->flow_info.flags
					& FLOW_RING_FLAG_INFORM_PKTPEND) &&
					!(flow_ring->flow_info.flags
					& FLOW_RING_FLAG_LAST_TIM)) {
					/* informs WL subsystem about the TIM_SET operation */
					uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, flow_ring);

					ret = dngl_flowring_update(pciedev->dngl,
						flow_ring->flow_info.ifindex,
						(uint8) flow_ring->handle,
						FLOW_RING_TIM_SET,
						(uint8 *)&flow_ring->flow_info.sa,
						(uint8 *)&flow_ring->flow_info.da, tid_ac, NULL);

					if (ret) {
						flow_ring->flow_info.flags |=
							FLOW_RING_FLAG_LAST_TIM;
						goto nextnode;
					} else {
						uint16 count;
						count = pciedev_reuse_seq_amsdu_cnt(pciedev,
								flow_ring);
						flow_ring->flow_info.maxpktcnt = count;
						flow_ring->flow_info.reqpktcnt += count;
						flow_ring->flow_info.flags |=
							FLOW_RING_FLAG_PKT_REQ;
					}
				} else if (!(flow_ring->status & FLOW_RING_FLUSH_PENDING)) {
					/* Keep flushing packets even if its closed */
					goto nextnode;
				}
			}

			PCI_TRACE(("h2d FLOW RING %d write %p  read %p  WI %d RI %d\n",
				flow_ring->ringid,
				flow_ring->wr_ptr,
				flow_ring->rd_ptr,
				MSGBUF_WR(flow_ring),
				MSGBUF_RD(flow_ring)));

			if (!(flow_ring->status & FLOW_RING_PKT_PENDING))
				goto nextnode;
#ifdef WLATF_DONGLE
			/* assigning max budget of lfrags based on the weight. */
			if (FFSHCED_ST_ENABLED(pciedev)) {
#ifdef BCMFRAGPOOL
				pciedev_flow_calc_maxlfrags(pciedev, flow_ring);
#endif /* BCMFRAGPOOL */
				/* assigning max budget of pkt fetch count based on the weight. */
				pciedev_flow_calc_maxpktcnt(pciedev, flow_ring);
			}
#endif /* WLATF_DONGLE */
			/* Read out messages from the flow ring */
			fetch = pciedev_h2d_start_fetching_host_buffer(pciedev, flow_ring);
			if (fetch) {
				if (skip_update == FALSE) {
#ifdef WLATF_DONGLE
					if (FFSHCED_ENAB(pciedev)) {
						any_fetch = TRUE;
					}
#endif /* WLATF_DONGLE */
					prioring->last_fetch_node = cur;
					pciedev->last_fetch_prio = prio;
				}
			} else {
				/* Stop updating last fetch node at the first failure */
				skip_update = TRUE;
			}

			flow_ring->status &= ~FLOW_RING_PKT_PENDING;

	nextnode:
			if (cur == prioring->last_fetch_node)
				break;

			cur = dll_next_p(cur);
		} /* while(1) */

	nextprio:
		if (prio == pciedev->last_fetch_prio)
			break;
		/* get next priority ring node */
		prio = dll_next_p(prio);
	}

#ifdef WLATF_DONGLE
	if (FFSHCED_ENAB(pciedev)&& any_fetch)
#endif // endif
	{
		pciedev_schedule_flow_ring_read_buffer(pciedev);
	}
#endif /* !WLSQS */
} /* pciedev_flow_schedule_timerfn */

/**
 * Get max number of packets that can be fetched from the caller supplied flow ring. Suppress
 * related.
 */
static uint pciedev_get_maxpkt_fetch_count(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring,
	uint16 availcnt)
{
	uint32 lbuf_avail = 0, lbuf_reserved = 0, ret_len;
#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
	uint32 inuse;
	uint32 available;
#endif // endif
	uint8 ifidx = 0;
	uint16 ifmin = 0;
	uint16 max_fetch = 0;

	PCI_TRACE(("pciedev_get_maxpkt_fetch_count\n"));

	BCM_REFERENCE(lbuf_reserved);

#if !(defined(BCMHWA) && defined(HWA_TXPOST_BUILD))
	/* if we are flushing packets, fetch all & send tx status bypassing wl */
	if (flow_ring->status & FLOW_RING_FLUSH_PENDING) {
#ifdef PCIEDEV_FAST_DELETE_RING
		/**
		 * If the flush is due to flow_ring delete and fast delete ring is enabled
		 * then only fetch suppressed frames. As long as fetch_ptr is below
		 * the delete_idx then it is fine to fetch as much as possible. Otherwise
		 * stop fetching here.
		 */
		if ((pciedev->fastdeletering) &&
		    (flow_ring->status & FLOW_RING_DELETE_RESP_PENDING)) {
			if (flow_ring->status & FLOW_RING_FAST_DELETE_ACTIVE)
				return 0;
			if (flow_ring->fetch_ptr < MSGBUF_WR(flow_ring)) {
				if ((flow_ring->fetch_ptr > flow_ring->delete_idx) ||
				    (flow_ring->delete_idx > MSGBUF_WR(flow_ring)))
					return 0;
			} else {
				if ((flow_ring->fetch_ptr > flow_ring->delete_idx) &&
				    (flow_ring->delete_idx > MSGBUF_WR(flow_ring)))
					return 0;
			}
		}
#endif /* PCIEDEV_FAST_DELETE_RING */

		ret_len = MIN(MSGBUF_READ_AVAIL_SPACE(flow_ring), availcnt);
#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
		/* For sliding windows/short bitmaps, let's make sure max_fetch
		 * does not include RDP - RD.
		 */
		if (flow_ring->fetch_ptr >= MSGBUF_RD(flow_ring))  {
			inuse = flow_ring->fetch_ptr - MSGBUF_RD(flow_ring);
		} else {
			inuse = MSGBUF_MAX(flow_ring) - MSGBUF_RD(flow_ring) +
				flow_ring->fetch_ptr;
		}
		available = flow_ring->bitmap_size - inuse - 1;
		ret_len = MIN((int)ret_len, (int)available);
#endif /* FLOWRING_SLIDING_WINDOW || FLOWRING_USE_SHORT_BITSETS */
		return ret_len;
	}

#ifdef BCMRESVFRAGPOOL
	if (BCMRESVFRAGPOOL_ENAB()) {
		lbuf_avail = rsvpool_avail(pciedev->pktpool_resv_lfrag);
	}
#endif // endif
#endif /* !(BCMHWA && HWA_TXPOST_BUILD) */

#if !(defined(BCMHWA) && defined(HWA_TXPOST_BUILD))
#ifdef BCMFRAGPOOL
	lbuf_avail = lbuf_avail + LFRAG_POOL_AVAIL(pciedev);

	lbuf_reserved = pciedev->frwd_resrv_bufcnt;

	lbuf_avail = ((lbuf_avail > lbuf_reserved) ?
			(lbuf_avail - lbuf_reserved) : 0);

	/* If there is no lbuf/lfrags wait for freeups */
	if (!lbuf_avail) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"No lbus/lfrags Available\n");
		DBG_BUS_INC(pciedev, pciedev_get_maxpkt_fetch_count);
		return 0;
	}
#endif /* BCMFRAGPOOL */

	/* Check for extra tx lfrag requirement from upper layers */
	if (pciedev->extra_lfrags) {
		if (lbuf_avail > pciedev->extra_lfrags)
			lbuf_avail -= pciedev->extra_lfrags;
		else {
			DBG_BUS_INC(pciedev, pciedev_get_maxpkt_fetch_count);
			return 0;
		}
	}
#endif /* !(BCMHWA && HWA_TXPOST_BUILD) */

#if defined(BCMHWA) && defined(HWA_TXPOST_BUILD)
	BCM_REFERENCE(lbuf_avail);

	/* We equial to Tx buffer count in TxBM */
	ret_len = availcnt;
#else
	/* Fetch packets only if we have enough lbuf/pktpool lbuf_avail */
	ret_len = lbuf_avail;
#endif /* BCMHWA && HWA_TXPOST_BUILD */

#if defined(WLATF_DONGLE) && !defined(WLATF_DONGLE_DISABLED)
	/* Leaky Bucket Algorithm:
	 * Adjusting the lfrags credit for given flowring.
	 * If credit is not enough then skipping pkt fetch.
	 */
#ifdef BCMFRAGPOOL
	if (FFSHCED_ST_ENABLED(pciedev)) {
		if (FL_PKTINFLIGHT(flow_ring) < SCHEDCXT_FL_MAXLFRAGS(flow_ring)) {
			/* If lfrag credit limit is not crossed */
			uint16 lfrags_credit =
				SCHEDCXT_FL_MAXLFRAGS(flow_ring) - FL_PKTINFLIGHT(flow_ring);
			if (SCHEDCXT_SUM_WEIGHT(pciedev) > SCHEDCXT_FL_WEIGHT(flow_ring)) {
				/* Calculate lfrags based on flowring weight.
				 * l = L x w/W;
				 */
				ret_len = (lbuf_avail * SCHEDCXT_FL_WEIGHT(flow_ring)) /
					SCHEDCXT_SUM_WEIGHT(pciedev);
				/* In case if weight or lbuf_avail value is very small the ret_len
				 * is going to be 0.
				 * Assigning minimum weight if there is available packets in the
				 * pool with taking into account currently fetched pending packets.
				 */
				if (ret_len <= 1) {
					lbuf_avail = LFRAG_POOL_AVAIL(pciedev);

					if (pciedev->fetch_pending < lbuf_avail)
						ret_len = 1;
					else
						return 0;
				}
			}
			ret_len = MIN(ret_len, lfrags_credit);
		} else {
			/* The credit limit is crossed for this flowring. Skipping fetch. */
			FL_STATUS(flow_ring) |= FLOW_RING_FETCH_SKIPPED;
			DBG_BUS_INC(pciedev, pciedev_get_maxpkt_fetch_count);
			return 0;
		}
	}
#endif /* BCMFRAGPOOL */
	if (FFSHCED_ENAB(pciedev)) {
		ret_len = MIN(ret_len, FL_MAXPKTCNT(flow_ring));
	}
#else /* WLATF_DONGLE */
	/* Consider fetch packets pending in fetch queue */
	if (ret_len >= pciedev->fetch_pending)
		ret_len -= pciedev->fetch_pending;
	else {
		EVENT_LOG(EVENT_LOG_TAG_PCI_ERROR, "Extra Tx Lbuf allocated\n");
		DBG_BUS_INC(pciedev, pciedev_get_maxpkt_fetch_count);
		return 0;
	}
#endif /* WLATF_DONGLE */

	/* Do not fetch more than maxpktcnt packets for this ring */
	/* If there are multiple flows active with same maxpktcnt and pktinflight_max, */
	/* first flow itself will use up all available txlfrags */
	/* No of pkts to be fetched should be proportional to lfrags available */
	max_fetch = MIN(flow_ring->flow_info.maxpktcnt, flow_ring->flow_info.pktinflight_max / 4);
	ret_len = MIN(ret_len, max_fetch);

	/* Apply per flow max thresholds */
	if (flow_ring->flow_info.pktinflight_max >
		(flow_ring->flow_info.pktinflight + flow_ring->fetch_pending)) {
		ret_len = MIN(ret_len, (flow_ring->flow_info.pktinflight_max -
			(flow_ring->flow_info.pktinflight + flow_ring->fetch_pending)));
	} else {
		return 0;
	}

	ifidx = flow_ring->flow_info.ifindex;

	/* Apply per interface  max thresholds */
	if (pciedev->ifidx_account[ifidx].max_cnt > pciedev->ifidx_account[ifidx].cur_cnt) {
		ifmin = pciedev->ifidx_account[ifidx].max_cnt -
			pciedev->ifidx_account[ifidx].cur_cnt;
		ret_len = MIN(ret_len, ifmin);
	} else {
		return 0;
	}

	/* Cap till contiguous available buffer */
	ret_len = MIN(ret_len, MSGBUF_READ_AVAIL_SPACE(flow_ring));

#if !(defined(BCMHWA) && defined(HWA_TXPOST_BUILD))
	/* Do not fetch more than availcnt in circular buffer */
	ret_len = MIN(ret_len, availcnt);
#endif /* !(BCMHWA && HWA_TXPOST_BUILD) */

#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
	/* For sliding windows/short bitmaps, let's make sure max_fetch
	 * does not include RDP - RD.
	 */
	if (flow_ring->fetch_ptr >= MSGBUF_RD(flow_ring))  {
		inuse = flow_ring->fetch_ptr - MSGBUF_RD(flow_ring);
	} else {
		inuse = MSGBUF_MAX(flow_ring) - MSGBUF_RD(flow_ring) +
			flow_ring->fetch_ptr;
	}
	available = flow_ring->bitmap_size - inuse - 1;
	ret_len = MIN((int)ret_len, (int)available);
#endif /* FLOWRING_SLIDING_WINDOW || FLOWRING_USE_SHORT_BITSETS */

	return ret_len;
} /* pciedev_get_maxpkt_fetch_count */

#ifdef WLATF_DONGLE

/** WLATF_DONGLE specific */
void pciedev_upd_flr_weight(struct dngl_bus * pciedev, uint8 mac_handle, uint8 ac_tid, void *params)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	bool updated = FALSE;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (flow_ring->handle != mac_handle) {
				cur = dll_next_p(cur);
				continue;
			}
			if (flow_ring->flow_info.tid_ac != ac_tid) {
				cur = dll_next_p(cur);
				continue;
			}

			if (params != NULL) {
				uint32 new_w = *(uint32*)params;
				/* updating with new weight value */
				FL_W_NEW(flow_ring) = new_w;
				updated = TRUE;
				break;

			}

			/* Next node */
			cur = dll_next_p(cur);
		}
		if (updated) {
			break;
		}
		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}

/** WLATF_DONGLE specific, rEnable/Disable Fair Fetch Scheduling Feature */
void pciedev_set_ffsched(struct dngl_bus * pciedev, void *params)
{
	if (params != NULL) {
		uint32 on = *(uint32*)params;

		if (on) {
			pciedev_reset_all_flowrings_weight(pciedev);
		}
#ifndef WLATF_DONGLE_DISABLED
		FFSHCED_ENAB(pciedev) = on ? TRUE : FALSE;
#endif /* WLATF_DONGLE_DISABLEd */
	}
}

/** WLATF_DONGLE specific, rReset the weight of the given active flow ring to default */
void
pciedev_reset_flowring_weight(struct dngl_bus *pciedev,
	msgbuf_ring_t *flow_ring)
{
	int weight;

	if (flow_ring->inited) {
		weight = dngl_flowring_update(
			pciedev->dngl, FL_TID_IFINDEX(flow_ring),
			0, FLOW_RING_RESET_WEIGHT, NULL,
			(uint8 *)&(FL_DA(flow_ring)), FL_TID_AC(flow_ring), NULL);
		/* XXX: The weight (airtime) never supposed to be 0.
		 * XXX: The only case it would happen due to another bugs.
		 * XXX: Assigning bare minimum weight.
		 */
		if (weight <= 0) {
			weight = 1;
		}
		FL_W_NEW(flow_ring) = (uint32) weight;
	}
}

/** WLATF_DONGLE specific, reset the weight of all active flow rings to default */
void
pciedev_reset_all_flowrings_weight(struct dngl_bus *pciedev)
{
	uint16 i;
	for (i = 0; i < pciedev->max_flowrings; i++) {
		pciedev_reset_flowring_weight(pciedev, &pciedev->flowrings_table[i]);
	}
}

#endif /* WLATF_DONGLE */

/**
 * Transmit related. 'Flow fetch requests' were queued by firmware. This function takes one such
 * request from the queue and kicks off host->device DMA.
 */
int
pciedev_h2d_start_fetching_host_buffer(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
	uint8 *dest_addr;
	uint8 *host_addr;		/**< lower 32 bits of an address on the host */
	uint16 src_len, ret_len;
	int8 i;
	uint8 ifidx = 0;

	flow_fetch_rqst_t *flowfetchp;
	cir_buf_pool_t *cpool = flow_ring->cbuf_pool;
	uint16 availcnt = CIR_BUFPOOL_AVAILABLE(cpool);

#ifdef WLSQS
	return sqs_sendup_force(pciedev, flow_ring);
#endif // endif

#if defined(BCMHWA) && defined(HWA_TXPOST_BUILD)
	/* availcnt is count of cbuf_pool buffers available for fetching host txpost work items.
	 * the cbuf_pool is shared between flowrings , HWA uses internal memory instead of cbuf_pool
	 * so passing ring_avail in HWA case.
	 */
	if (TRUE) {
		uint16 ring_avail = MIN(availcnt, MSGBUF_READ_AVAIL_SPACE(flow_ring));
		ret_len = pciedev_get_maxpkt_fetch_count(pciedev, flow_ring, ring_avail);

		if (ret_len) {
			ret_len = bcm_count_zeros_sequence((uint32*)flow_ring->inprocess,
					MSGBUF_MODULO_IDX(flow_ring, flow_ring->fetch_ptr),
					ret_len, flow_ring->bitmap_size);
		}

		if (ret_len) {
			int trans_id;

			trans_id = hwa_txpost_schedcmd_request(hwa_dev,
				flow_ring->ringid, flow_ring->fetch_ptr,
				ret_len);

			if (trans_id == HWA_FAILURE) {
				return 0;
			} else {
				set_bitrange(flow_ring->inprocess,
					MSGBUF_MODULO_IDX(flow_ring, flow_ring->fetch_ptr),
					MSGBUF_MODULO_IDX(flow_ring, flow_ring->fetch_ptr +
					ret_len - 1), flow_ring->bitmap_size - 1);

				flow_ring->flow_info.pktinflight += ret_len;
				flow_ring->fetch_ptr = (flow_ring->fetch_ptr + ret_len) %
					MSGBUF_MAX(flow_ring);

				if (MSGBUF_RD(flow_ring) > flow_ring->fetch_ptr) {
					flow_ring->current_phase = flow_ring->current_phase ?
						0 : MSGBUF_RING_INIT_PHASE;
				}

				/* Update per ifidx stats */
				ifidx = flow_ring->flow_info.ifindex;
				pciedev->ifidx_account[ifidx].cur_cnt += ret_len;
				pciedev->pend_user_tx_pkts += ret_len;

				pciedev_update_rdp_ptr_unacked(flow_ring);
			}
		} else {
			DBG_BUS_INC(pciedev, pciedev_h2d_start_fetching_host_buffer);
			pciedev_update_rdp_ptr_unacked(flow_ring);
		}

		return ret_len;
	}
#endif /* BCMHWA && HWA_TXPOST_BUILD */

	PCI_TRACE(("pciedev_h2d_start_fetching_host_buffer\n"));

	/* If there is no flow fetch request come later */
	if ((i = pciedev_allocate_flowring_fetch_rqst_entry(pciedev)) < 0) {
		PCI_INFORM(("Fetch Req alloc error read later %d \n", flow_ring->ringid));
		DBG_BUS_INC(pciedev, pciedev_h2d_start_fetching_host_buffer);
		return 0;
	}

	/* check for availability of local circular buffer */
	if (!availcnt) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"flow_ring_name: %c%c%c%c circular ring bufs not available,"
			"Dont read out from host ring \n",
			flow_ring->name[0], flow_ring->name[1],
			flow_ring->name[2], flow_ring->name[3]);
		goto cleanup;
	}

	flowfetchp = &pciedev->flowring_fetch_rqsts[i];
	flowfetchp->start_ringinx = flow_ring->fetch_ptr;

	ret_len = pciedev_get_maxpkt_fetch_count(pciedev, flow_ring, availcnt);

	/*
	 * If next fetched packets are already inprocess (= acked/nacked, not suppressed, meaning
	 * no need to retransmit) do not fetch them. Only fetch new or previously suppressed packets
	 * from the host.
	 */
	if (ret_len) {
		ret_len = bcm_count_zeros_sequence((uint32*)flow_ring->inprocess,
				MSGBUF_MODULO_IDX(flow_ring, flowfetchp->start_ringinx),
				ret_len, flow_ring->bitmap_size);
	}

	/* Nothing to fetch go cleanup */
	if (!ret_len)
		goto cleanup;

	ret_len *= MSGBUF_LEN(flow_ring);

	/* Update the local copy of the write/end ptr with the shared register's value */

	/* allocs lcl buffers / lcl msg ring items that are going to contain host message */
	host_addr = pciedev_get_msgbuf_host_addr(flow_ring, &src_len, ret_len);
	if (host_addr == NULL) {
		goto cleanup;
	}
	ret_len = src_len / MSGBUF_LEN(flow_ring);

	/* Get address from circular buffer pool */
	dest_addr = pciedev_get_cirbuf_pool(flow_ring, ret_len);

	/* Need to handle if local buffer avail space is < src_len */
	if (dest_addr != NULL) {
		set_bitrange(flow_ring->inprocess,
			MSGBUF_MODULO_IDX(flow_ring, flowfetchp->start_ringinx),
			MSGBUF_MODULO_IDX(flow_ring, (flowfetchp->start_ringinx + ret_len - 1)),
			flow_ring->bitmap_size - 1);

		/* Place the fetch request */
		flowfetchp->rqst.size = src_len;
		flowfetchp->rqst.dest = dest_addr;
		PHYSADDR64HISET(flowfetchp->rqst.haddr,
			(uint32) ltoh32(HADDR64_HI(MSGBUF_HADDR64(flow_ring))));
		PHYSADDR64LOSET(flowfetchp->rqst.haddr,
			(uint32) ltoh32(host_addr));
		flowfetchp->rqst.ctx = (void *)flowfetchp;
		flowfetchp->rqst.next = NULL;
		flowfetchp->rqst.flags = flow_ring->current_phase;
		flowfetchp->msg_ring = flow_ring;
		flowfetchp->offset = 0;
		flowfetchp->flags = 0;
		flowfetchp->next = NULL;
#ifdef BCMPCIEDEV
		if (BCMPCIEDEV_ENAB()) {
			hnd_fetch_rqst(&flowfetchp->rqst);
		}
#endif // endif
		PCIEDEV_FLOWFETCH_RDINDS_TS(flow_ring, src_len, flowfetchp->start_ringinx, 0);
		flow_ring->fetch_pending += ret_len;
		pciedev->fetch_pending += ret_len;
		/* we are wrapping the ring now, update the ring current phase */
		if (flow_ring->fetch_ptr == 0) {
			flow_ring->current_phase =
				flow_ring->current_phase ? 0 : MSGBUF_RING_INIT_PHASE;
			PCI_INFORM(("updating the current phase wrap to 0x%x\n",
				flow_ring->current_phase));
		}

		/* Update per ifidx stats */
		ifidx = flow_ring->flow_info.ifindex;
		pciedev->ifidx_account[ifidx].cur_cnt += ret_len;

		pciedev_update_rdp_ptr_unacked(flow_ring);

		return ret_len;
	}

cleanup:
	DBG_BUS_INC(pciedev, pciedev_h2d_start_fetching_host_buffer);
	pciedev_update_rdp_ptr_unacked(flow_ring);
	pciedev_free_flowring_fetch_rqst_entry(pciedev, i);

	return 0;
} /* pciedev_h2d_start_fetching_host_buffer */

/**
 * Transmit related. Called when a new so-called 'internal' 'host fetch' message is available in
 * device memory, signalling that new data to transmit to the wireless medium is now available in
 * dongle memory. Called back by eg pciedev_process_tx_payload()
 */
void
pciedev_flowring_fetch_cb(struct fetch_rqst *fr, bool cancelled)
{
	flow_fetch_rqst_t *flow_fetch = fr->ctx;
	struct dngl_bus *pciedev = flow_fetch->msg_ring->pciedev;
	uint8 ifidx = 0;
	uint16 retlen = 0, processed_items = 0, offset = 0;
	uint16 i;

	/* Keep the old offset until which we have processed in fetch request */
	offset = flow_fetch->offset * MSGBUF_LEN(flow_fetch->msg_ring);

	/* If cancelled, retry: drop request for now
	 * Might need to retry sending it down HostFetch
	 */
	if (cancelled) {
		PCI_ERROR(("pciedev_flowring_fetch_cb: Request cancelled!...\n"));
		processed_items = (fr->size - offset) / MSGBUF_LEN(flow_fetch->msg_ring);
		pciedev_rewind_flow_fetch_ptr(flow_fetch->msg_ring, flow_fetch->start_ringinx);
		for (i = 0; i < processed_items; i++) {
			clrbit(flow_fetch->msg_ring->inprocess,
				MSGBUF_MODULO_IDX(flow_fetch->msg_ring,
				flow_fetch->start_ringinx + i));
		}
		goto cleanup;
	}

	/* retlen holds how much is still pending for process */
	retlen = pciedev_handle_h2d_msg_txsubmit(pciedev, (void*)((uint8*)fr->dest + offset),
		fr->size - offset, flow_fetch->msg_ring, flow_fetch->start_ringinx, fr->flags);

cleanup:
	/* handle Clean up */
	/* free up local mesage space */
	/* Store how much we processed so far */
	processed_items = ((fr->size - offset) - retlen) / MSGBUF_LEN(flow_fetch->msg_ring);

	/* Update flow ring and global counters */
	flow_fetch->msg_ring->fetch_pending -= processed_items;
	pciedev->fetch_pending -= processed_items;

	if (!retlen) {
		if (!pciedev->fetch_req_pend_list->head ||
			(pciedev->fetch_req_pend_list->head == flow_fetch)) {
			/* We are done with this flow fetch request */
			/* Free up local message space */
			flow_fetch->offset = flow_fetch->flags = 0;
			pciedev_free_cirbuf_pool(flow_fetch->msg_ring, fr->dest, fr->size);
			pciedev_free_flowring_fetch_rqst_entry(pciedev, flow_fetch->index);
			if (pciedev->fetch_req_pend_list->head)
				pciedev_remove_fetch_rqst_head(pciedev);
			flow_fetch->next = NULL;
		} else {
			if (!(flow_fetch->flags & (PCIEDEV_FLOW_FETCH_FLAG_FREE |
				PCIEDEV_FLOW_FETCH_FLAG_REPROCESS))) {
				pciedev_insert_fetch_rqst(pciedev, flow_fetch);
			}
			flow_fetch->offset += processed_items;
			flow_fetch->flags |= PCIEDEV_FLOW_FETCH_FLAG_FREE |
				PCIEDEV_FLOW_FETCH_FLAG_REPROCESS;
		}
	} else {
		/*
		 * Partial flow fetch processing at pciedev_handle_h2d_msg_txsubmit
		 * Update offset from which we need to process on next callback,
		 * Update start index, flags to indicate partial process at fetch.
		 */
		flow_fetch->offset += processed_items;
		flow_fetch->start_ringinx = (flow_fetch->start_ringinx + processed_items) %
			MSGBUF_MAX(flow_fetch->msg_ring);
		if (!(flow_fetch->flags & PCIEDEV_FLOW_FETCH_FLAG_REPROCESS))
			pciedev_insert_fetch_rqst(pciedev, flow_fetch);
		flow_fetch->flags |= PCIEDEV_FLOW_FETCH_FLAG_REPROCESS;
	}

	/* Update per ifidx stats */
	ifidx = flow_fetch->msg_ring->flow_info.ifindex;
	pciedev->ifidx_account[ifidx].cur_cnt -= processed_items;
	if (!flow_fetch->msg_ring->flow_info.pktinflight &&
		!flow_fetch->msg_ring->fetch_pending)
		flow_fetch->msg_ring->status &= ~FLOW_RING_SUP_PENDING;

	if (flow_fetch->msg_ring->status & FLOW_RING_FLUSH_PENDING)
		pciedev_process_pending_flring_resp(pciedev, flow_fetch->msg_ring);
	return;
}

/** Insert pending fetches at the end of req_pend list */
static void
pciedev_insert_fetch_rqst(struct dngl_bus * pciedev, flow_fetch_rqst_t *node)
{
	if (!pciedev->fetch_req_pend_list->head) {
		pciedev->fetch_req_pend_list->head =
		pciedev->fetch_req_pend_list->tail = node;
	} else {
		pciedev->fetch_req_pend_list->tail->next = node;
		pciedev->fetch_req_pend_list->tail = node;
	}
}

/** Remove pending fetches from head of req_pend list */
static void
pciedev_remove_fetch_rqst_head(struct dngl_bus * pciedev)
{
	if (pciedev->fetch_req_pend_list->head == pciedev->fetch_req_pend_list->tail) {
		pciedev->fetch_req_pend_list->head = pciedev->fetch_req_pend_list->tail = NULL;
	} else
		pciedev->fetch_req_pend_list->head = pciedev->fetch_req_pend_list->head->next;
}

/** Process pending flow fetch requests which could have left due to resource crunch. */
void pciedev_process_pending_fetches(struct dngl_bus *pciedev)
{
	flow_fetch_rqst_t *node;
	node = pciedev->fetch_req_pend_list->head;
	while (node && node->flags) {
		flow_fetch_rqst_t *next = node->next;
		pciedev_flowring_fetch_cb(&node->rqst, FALSE);
		node = next;
	}
}

static int8
pciedev_allocate_flowring_fetch_rqst_entry(struct dngl_bus *pciedev)
{
	uint8 i;
	for (i = 0; i < PCIEDEV_MAX_FLOWRINGS_FETCH_REQUESTS; i++)
		if (!pciedev->flowring_fetch_rqsts[i].used) {
			pciedev->flowring_fetch_rqsts[i].used = TRUE;
			return i;
		}
	DBG_BUS_INC(pciedev, pciedev_allocate_flowring_fetch_rqst_entry);
	return BCME_ERROR;
}

static void
pciedev_free_flowring_fetch_rqst_entry(struct dngl_bus *pciedev, uint8 index)
{
	pciedev->flowring_fetch_rqsts[index].used = FALSE;
}

/**
 * Transmit / suppress related. For suppressed packets, dongle can re-fetch the packets/work items
 * from the host as needed.
 *
 * Moves fetch_ptr (read-pending/next start of fetch pointer) 'back' if caller supplied 'index'
 * precedes flow_ring->fetch_ptr. This is to allow all packet suppressed/need-to-be re-fetched
 * until the index points to the index of the suppressed packet.
 */
static void
pciedev_rewind_flow_fetch_ptr(msgbuf_ring_t *flow_ring, uint16 index)
{
	uint16 old_ptr = flow_ring->fetch_ptr;
	uint16 count = 0;

	if ((flow_ring->fetch_ptr >= MSGBUF_RD(flow_ring))) {
		/* No wrap condition */
		if ((index >= MSGBUF_RD(flow_ring)) && (index < flow_ring->fetch_ptr)) {
			count = flow_ring->fetch_ptr - index;
			flow_ring->fetch_ptr = index;
		}
	} else {
		/* Wrap condition */
		if ((index < flow_ring->fetch_ptr) || (index >= MSGBUF_RD(flow_ring))) {
			if (index < flow_ring->fetch_ptr)
				count = flow_ring->fetch_ptr - index;
			else
				count = flow_ring->fetch_ptr + MSGBUF_MAX(flow_ring) - index;
			flow_ring->fetch_ptr = index;
		}
		/* update the current_phase */
		if (flow_ring->fetch_ptr >= MSGBUF_RD(flow_ring)) {
			flow_ring->current_phase =
				flow_ring->current_phase ? 0 : MSGBUF_RING_INIT_PHASE;
			PCI_TRACE(("change phase rdpending suppr adj 0x%04x, rd %d, rdp %d\n",
				flow_ring->current_phase, MSGBUF_RD(flow_ring),
				flow_ring->fetch_ptr));
		}
	}

	BUZZZ_KPI_QUE1(KPI_QUE_BUS_RP_REW, 2, flow_ring->fetch_ptr, flow_ring->ringid);

	BCM_REFERENCE(old_ptr);
	BCM_REFERENCE(count);
#if defined(WLSQS)
	if (count) {
		PCI_INFORM(("%s: fetch_ptr old<%u> new<%u> count<%u> RD<%u> W<%u>\n",
			__FUNCTION__, old_ptr, flow_ring->fetch_ptr, count,
			MSGBUF_RD(flow_ring), MSGBUF_WR(flow_ring)));
		sqs_vpkts_rewind((struct dngl_bus *)flow_ring->pciedev, flow_ring, count);
	}
#endif // endif
}

/**
 * Transmit / packet suppression related. For suppressed packets, dongle has to re-fetch the
 * packets/work items from the host as needed.
 *
 * If first packet to be fetched is already inprocess ('no need to retransmit') then move
 * flow_ring->fetch_ptr to first non-acked packet, skipping packets that are already acked. This
 * can happen if we get status 'ok/non-suppressed' (meaning: no need to retransmit )packet in
 * between the packet stream.
 */
static void
pciedev_update_rdp_ptr_unacked(msgbuf_ring_t *flow_ring)
{
	uint16 i, max_item, start_fetch_ptr;
	uint16 count = 0;

	start_fetch_ptr = flow_ring->fetch_ptr;
	if (isclr(flow_ring->inprocess, MSGBUF_MODULO_IDX(flow_ring, start_fetch_ptr)))
		return;

	max_item = MSGBUF_MAX(flow_ring);

	for (i = 0; i < flow_ring->bitmap_size; i++) {
		if (isset(flow_ring->inprocess,
			MSGBUF_MODULO_IDX(flow_ring, flow_ring->fetch_ptr))) {
			flow_ring->fetch_ptr = (flow_ring->fetch_ptr + 1) % max_item;
			count++;
		} else {
			break;
		}
	}

	/* fetch_ptr wrapped case, so update the current_phase */
	if (start_fetch_ptr > flow_ring->fetch_ptr) {
		PCI_INFORM(("change phase rdpending unack adj 0x%04x, prev rdp %d, rdp %d\n",
			flow_ring->current_phase, start_fetch_ptr, flow_ring->fetch_ptr));
		flow_ring->current_phase = flow_ring->current_phase ? 0 : MSGBUF_RING_INIT_PHASE;
	}
	BCM_REFERENCE(count);
#if defined(WLSQS)
	if (count) {
		PCI_INFORM(("%s: fetch_ptr old<%u> new<%u> count<%u> RD<%u> W<%u>\n",
			__FUNCTION__, start_fetch_ptr, flow_ring->fetch_ptr, count,
			MSGBUF_RD(flow_ring), MSGBUF_WR(flow_ring)));
		sqs_vpkts_forward((struct dngl_bus *)flow_ring->pciedev, flow_ring, count);
	}
#endif // endif
} /* pciedev_update_rdp_ptr_unacked */

#ifdef WL_REUSE_KEY_SEQ
static void
pciedev_alloc_reuse_key_seq_list(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
#ifdef MEM_ALLOC_STATS
	memuse_info_t mu;
#endif // endif
	uint32 reuse_key_seq_list_size = flow_ring->bitmap_size * WL_KEY_SEQ_INFO_SIZE;
	ASSERT(!flow_ring->reuse_key_seq_list);

	if (!(flow_ring->reuse_key_seq_list = (uint8 *)
		MALLOCZ(pciedev->osh, reuse_key_seq_list_size))) {
		PCI_ERROR(("Flow:%d Unable to alloc reuse_key_seq_list\n", flow_ring->ringid));
		return;
	}
	PCI_TRACE(("\n ADDED : reuse_key_seq_list"));
#ifdef MEM_ALLOC_STATS
	hnd_get_heapuse(&mu);
	mu.max_flowring_alloc += reuse_key_seq_list_size;
	hnd_update_mem_alloc_stats(&mu);
#endif /* MEM_ALLOC_STATS */
} /* pciedev_alloc_reuse_seq_key_list */
#endif /* WL_REUSE_KEY_SEQ */

static void
pciedev_alloc_reuse_seq_list(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
#ifdef MEM_ALLOC_STATS
	memuse_info_t mu;
#endif // endif
	uint32 reuse_seq_list_size = flow_ring->bitmap_size * sizeof(uint16);
	ASSERT(!flow_ring->reuse_seq_list);

#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
	if (PCIEDEV_SUP_EXP_ENAB(flow_ring)) {
		ASSERT(!flow_ring->sup_exp_block);
		ASSERT(!flow_ring->sup_exp_tstamp);

		/**
		 * allocate 4 bits per index for cnt.
		 * Only for infra (ifindex=0) flow rings
		 */
		reuse_seq_list_size += flow_ring->bitmap_size >> 1;

		/**
		 * allocate 8 bits per index for tstamp.
		 * Only for infra (ifindex=0) flow rings
		 */
		reuse_seq_list_size += flow_ring->bitmap_size;
	}
#endif /* PCIEDEV_SUPPR_RETRY_TIMEOUT */

	if (!(flow_ring->reuse_seq_list = (uint16 *)
		MALLOCZ(pciedev->osh, reuse_seq_list_size))) {
		PCI_ERROR(("Flow:%d Unable to alloc reuse_seq_list\n", flow_ring->ringid));
		return;
	}

	PCI_TRACE(("\n ADDED : reuse_seq_list"));

#ifdef MEM_ALLOC_STATS
	hnd_get_heapuse(&mu);
	mu.max_flowring_alloc += reuse_seq_list_size;
	hnd_update_mem_alloc_stats(&mu);
#endif /* MEM_ALLOC_STATS */

#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
	if (PCIEDEV_SUP_EXP_ENAB(flow_ring)) {
		flow_ring->sup_exp_block = (uint8*)flow_ring->reuse_seq_list +
			(flow_ring->bitmap_size * sizeof(uint16));
		flow_ring->sup_exp_tstamp = (uint8*)flow_ring->sup_exp_block +
			(flow_ring->bitmap_size >> 1);
	}
#endif // endif

	pciedev->flow_supp_enab++;
	if (pciedev->flow_ageing_timer_on == FALSE) {
		PCI_TRACE(("\n ADDED: flow_ageing_timer"));
		dngl_add_timer(pciedev->flow_ageing_timer,
			pciedev->flow_age_timeout, TRUE);
		pciedev->flow_ageing_timer_on = TRUE;
	}
} /* pciedev_alloc_reuse_seq_list */

void
pciedev_free_reuse_seq_list(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
#ifdef MEM_ALLOC_STATS
	memuse_info_t mu;
#endif // endif
	uint32 reuse_seq_list_size = flow_ring->bitmap_size * sizeof(uint16);
#ifdef WL_REUSE_KEY_SEQ
	uint32 reuse_key_seq_list_size = flow_ring->bitmap_size * WL_KEY_SEQ_INFO_SIZE;
#endif // endif
	ASSERT(flow_ring->ring_ageing_info.sup_cnt == 0);

	PCI_TRACE(("\n DELETING : reuse_seq_list"));
#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
	if (PCIEDEV_SUP_EXP_ENAB(flow_ring)) {
		/**
		 * allocated 4 bits per index for cnt.
		 * Only for infra (ifindex=0) flow rings
		 */
		reuse_seq_list_size += flow_ring->bitmap_size >> 1;

		/**
		 * allocated 8 bits per index for tstamp.
		 * Only for infra (ifindex=0) flow rings
		 */
		reuse_seq_list_size += flow_ring->bitmap_size;

		flow_ring->sup_exp_block = NULL;
		flow_ring->sup_exp_tstamp = NULL;
	}
#endif /* PCIEDEV_SUPPR_RETRY_TIMEOUT */

	MFREE(pciedev->osh, flow_ring->reuse_seq_list, reuse_seq_list_size);
	flow_ring->reuse_seq_list = NULL;

#ifdef MEM_ALLOC_STATS
	hnd_get_heapuse(&mu);
	mu.max_flowring_alloc -= reuse_seq_list_size;
	hnd_update_mem_alloc_stats(&mu);
#endif /* MEM_ALLOC_STATS */

	bzero(flow_ring->reuse_sup_seq, /* d11 seq numbers */
		flow_ring->bitmap_size/NBBY);

	pciedev->flow_supp_enab--;
	/* delete the ageing timer if no flowrings are suppress enab */
	if (!pciedev->flow_supp_enab) {
		PCI_TRACE(("\n DELETING : flow_ageing_timer"));
		dngl_del_timer(pciedev->flow_ageing_timer);
		pciedev->flow_ageing_timer_on = FALSE;
	}

#ifdef WL_REUSE_KEY_SEQ
	PCI_TRACE(("\n DELETING : reuse_key_seq_list"));
	if (flow_ring->reuse_key_seq_list) {
		MFREE(pciedev->osh, flow_ring->reuse_key_seq_list, reuse_key_seq_list_size);
		flow_ring->reuse_key_seq_list = NULL;
	}
	bzero(flow_ring->reuse_sup_key_seq, /* key seq numbers */
		flow_ring->bitmap_size/NBBY);
#ifdef MEM_ALLOC_STATS
	hnd_get_heapuse(&mu);
	mu.max_flowring_alloc -= reuse_key_seq_list_size;
	hnd_update_mem_alloc_stats(&mu);
#endif /* MEM_ALLOC_STATS */
#endif /* WL_REUSE_KEY_SEQ */
} /* pciedev_free_reuse_seq_list */

#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
static uint32
pciedev_flow_ring_age_out_check(struct dngl_bus *pciedev, void *p, uint32 status)
{
	uint16 r_index;
	msgbuf_ring_t *flow_ring;
	uint16 bit_idx;
	uint8 tocks, cnt;

	flow_ring = pciedev->h2d_submitring_ptr[PKTFRAGFLOWRINGID(pciedev->osh, p)];
	if (flow_ring == NULL) {
		OSL_SYS_HALT();
	}

	r_index = PKTFRAGRINGINDEX(pciedev->osh, p);
	bit_idx = MSGBUF_MODULO_IDX(flow_ring, r_index);

	if (isset(flow_ring->reuse_sup_seq, bit_idx)) {
		flow_ring->sup_retries++;
		pciedev->sup_retries++;

		if (PCIEDEV_SUP_EXP_ENAB(flow_ring)) {
			cnt = PCIEDEV_GET_SUP_EXP_CNT(flow_ring, bit_idx);
			cnt += 1;
			/* MAX age out check based on "n" times suppressed  */
			if (cnt >= flow_ring->max_sup_retries) {
				PCI_ERROR(("Flow:%d r_index:%d, dropping packet tried %d times\n",
					flow_ring->ringid, r_index, cnt));
				flow_ring->dropped_counter_sup_retries++;
				pciedev->dropped_counter_sup_retries++;
				return WLFC_CTL_PKTFLAG_DISCARD_NOACK;
			}

			tocks = PCIEDEV_GET_SUP_EXP_ELAPSE(flow_ring, bit_idx);
			/* MAX age out check based on amount of time suppressed  */
			if (tocks >= flow_ring->max_sup_tocks) {
				PCI_ERROR(("Flow:%d r_index:%d, dropping packet tried %d msec\n",
					flow_ring->ringid, r_index, (tocks * TOCK_UNIT)));
				flow_ring->dropped_timer_sup_retries++;
				pciedev->dropped_timer_sup_retries++;
				return WLFC_CTL_PKTFLAG_DISCARD_NOACK;
			}
		}
	}

	return status;
} /* pciedev_flow_ring_age_out_check */
#endif /* PCIEDEV_SUPPR_RETRY_TIMEOUT */
void
pciedev_flush_cached_amsdu_frag(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
	void *p, *n;
	amsdu_sup_pktlist_t *amsdu_sup_part1;

	amsdu_sup_part1 = &flow_ring->amsdu_sup_part1;
	if (amsdu_sup_part1->pkts) {
		p = amsdu_sup_part1->head;
		amsdu_sup_part1->pkts = 0;
		FOREACH_CHAINED_PKT(p, n) {
			PKTCLRCHAINED(pciedev->osh, p);
			pciedev_update_txstatus(pciedev, WLFC_CTL_PKTFLAG_WLSUPPRESS,
				PKTFRAGRINGINDEX(pciedev->osh, p), flow_ring->ringid,
				0, FALSE, NULL);

			BCMPCIE_IPC_HPA_TEST(pciedev, PKTFRAGPKTID(pciedev->osh, p),
				BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_RESPONSE);
			BUZZZ_KPI_PKT1(KPI_PKT_BUS_TXCMPL, 2,
				PKTFRAGPKTID(pciedev->osh, p), flow_ring->ringid);

			PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *)p);
			PKTFREE(pciedev->osh, p, TRUE);
		}
	}
}

/* Put the packet back to flow ring and rewind the fetch pointer
 * Packet FREE to be handled by the caller.
 * XXX lfrag should be fully formed with RINGINDEX setup
 */
void
pciedev_lfrag_suppress_to_host(struct dngl_bus *pciedev, uint16 flowid, void* lfrag)
{

	/* Rewind the fetch pointer */
	pciedev_update_txstatus(pciedev, WLFC_CTL_PKTFLAG_WLSUPPRESS,
		PKTFRAGRINGINDEX(pciedev->osh, lfrag), flowid,
		0, FALSE, NULL);

	/* Host pktid audit */
	BCMPCIE_IPC_HPA_TEST(pciedev, PKTFRAGPKTID(pciedev->osh, lfrag),
		BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_RESPONSE);
	BUZZZ_KPI_PKT3(KPI_PKT_BUS_TXSUPP, 2,
		PKTFRAGPKTID(pciedev->osh, lfrag), flowid);

	/* Dont trigger tx completion on this packet */
	PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *)lfrag);
}
/**
 * Called when eg the wl layer reported status back on the transmission/suppression of a packet.
 *
 * Update the flow ring (in device memory) designated by (flowid) with the caller supplied
 * parameters, so the host can react to the new status of the flow ring. Does not generate an
 * interrupt towards the host. Also, reschedules suppressed packets for retransmission.
 *
 * @param[in] rindex index on the flow ring where the packet was previously fetched from
 */
static int
pciedev_update_txstatus(struct dngl_bus *pciedev, uint32 status, uint16 rindex,
	uint16 flowid, uint16 seq, bool hold, uint8 *key_seq)
{
	msgbuf_ring_t *flow_ring;
	int ret = BCME_OK;
	uint16 bit_idx;
	uint8 ifidx;
#ifdef WL_REUSE_KEY_SEQ
	uint8 *buf;
	int j;
#endif // endif
	PCI_TRACE(("pciedev_update_txstatus: status=%d; rindex=%d, flowid=%d, seq=%d\n",
			status, rindex, flowid, seq));

	flow_ring = pciedev->h2d_submitring_ptr[flowid];

	if (flow_ring == NULL) {
		PCI_ERROR(("update txstatus called with invalid flowid %d\n", flowid));
		DBG_BUS_INC(pciedev, pciedev_update_txstatus);
		OSL_SYS_HALT();
	}

	bit_idx = MSGBUF_MODULO_IDX(flow_ring, rindex);

	flow_ring->flow_info.pktinflight--;

	/* Update per ifidx stats */
	ifidx = flow_ring->flow_info.ifindex;
	pciedev->ifidx_account[ifidx].cur_cnt--;
	pciedev->pend_user_tx_pkts--;

	if (((status == WLFC_CTL_PKTFLAG_D11SUPPRESS) ||
	     (status == WLFC_CTL_PKTFLAG_WLSUPPRESS)) &&
	    (!(flow_ring->status & FLOW_RING_FLUSH_PENDING))) {
		/* proptxstatus: refetch suppressed packets from host for retransmission later on */
		ret = BCME_NOTREADY;
		PCIEDEV_FLOWSUPP_RDINDS_TS(flow_ring, rindex, status, seq, OSL_SYSUPTIME());
		if (flow_ring->flow_info.pktinflight > 0 || flow_ring->fetch_pending > 0) {
			flow_ring->status |= FLOW_RING_SUP_PENDING;
			/* If there was a partial fetched AMSDU then clean that up. */
			pciedev_flush_cached_amsdu_frag(pciedev, flow_ring);
		}
		pciedev_rewind_flow_fetch_ptr(flow_ring, rindex);
		clrbit(flow_ring->inprocess, bit_idx);
		if (GET_DRV_HAS_ASSIGNED_SEQ(seq)) {
			if (!(flow_ring->reuse_seq_list)) {
				pciedev_alloc_reuse_seq_list(pciedev, flow_ring);
			}
			if (flow_ring->reuse_seq_list) {
				flow_ring->ring_ageing_info.sup_cnt++;
				flow_ring->reuse_seq_list[bit_idx] = seq;
				/* Check if this is the first time the reuse_sup_seq bit is set */
				if (!isset(flow_ring->reuse_sup_seq, bit_idx)) {
#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
					if (PCIEDEV_SUP_EXP_ENAB(flow_ring)) {
						PCIEDEV_CLR_SUP_EXP_CNT(flow_ring, bit_idx);
						PCIEDEV_SET_SUP_EXP_TSTAMP(flow_ring, bit_idx);
					}
#endif // endif
					setbit(flow_ring->reuse_sup_seq, bit_idx);
					PCI_TRACE(("PCIE Suppr Seq %d %d %x\n", bit_idx,
					flow_ring->reuse_seq_list[bit_idx],
					WL_SEQ_GET_NUM(flow_ring->reuse_seq_list[bit_idx])));
				}
#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
				if (PCIEDEV_SUP_EXP_ENAB(flow_ring)) {
					PCIEDEV_INC_SUP_EXP_CNT(flow_ring, bit_idx);
				}
#endif // endif
#ifdef WL_REUSE_KEY_SEQ
				if (GET_DRV_HAS_ASSIGNED_KEY_SEQ(key_seq)) {
					if (!(flow_ring->reuse_key_seq_list)) {
						pciedev_alloc_reuse_key_seq_list(pciedev,
							flow_ring);
					}
					if (flow_ring->reuse_key_seq_list) {
						buf = &flow_ring->reuse_key_seq_list[(bit_idx *
							WL_KEY_SEQ_INFO_SIZE)];
						memcpy(&buf[0], &key_seq[0], WL_KEY_SEQ_INFO_SIZE);
						if (!isset(flow_ring->reuse_sup_key_seq, bit_idx)) {
							setbit(flow_ring->reuse_sup_key_seq,
								bit_idx);
							PCI_TRACE(("PCIE Suppr reuse seq key "
							"for D11 seq %x =>",
							WL_SEQ_GET_NUM(flow_ring->reuse_seq_list[
								bit_idx])));
							for (j = WL_KEY_SEQ_INFO_SIZE; j > 0; --j) {
								PCI_TRACE(("%02x", key_seq[j - 1]));
							}
							PCI_TRACE(("\n"));
						}
					}
				}
#endif /* WL_REUSE_KEY_SEQ */
			}
		}
	} else if (isset(flow_ring->status_cmpl, bit_idx) ||
			isclr(flow_ring->inprocess, bit_idx)) {
			DBG_BUS_INC(pciedev, pciedev_update_txstatus);
			/* Should never happen print for now until its stable */
			PCI_ERROR(("I %d RD %d RDp %d W %d\n", rindex, MSGBUF_RD(flow_ring),
				flow_ring->fetch_ptr, MSGBUF_WR(flow_ring)));
			ret = BCME_NOTREADY;
	} else {
		pciedev_update_bitmaps_tracking(pciedev, flow_ring, rindex);
	} /* if */

	if (!flow_ring->flow_info.pktinflight && !flow_ring->fetch_pending)
		flow_ring->status &= ~FLOW_RING_SUP_PENDING;

	if (!hold) {
#if defined(WLSQS)
		pciedev_schedule_sqs_timer(pciedev);
#else
		pciedev_schedule_flow_ring_read_buffer(pciedev);
#endif /* !WLSQS */
	}

	return ret;
} /* pciedev_update_txstatus */

/** Return a buf from circular free list */
uint8*
pciedev_get_cirbuf_pool(msgbuf_ring_t *ring, uint16 len)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;
	uint16 w_ptr = cpool->w_ptr;
	uint16 availcnt = CIR_BUFPOOL_AVAILABLE(cpool);

	/* check for avail space */
	if (availcnt == 0) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"Pool empty Ring name: %c%c%c%c\n",
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]);
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_get_cirbuf_pool);
		return NULL;
	}

	/* we should not get 0 len request (or) more than available count */
	ASSERT(len);
	ASSERT(len <= availcnt);

	/* update w_ptr to next free index & also handle wrap up case */
	if ((cpool->w_ptr + len) == cpool->depth) {
		cpool->w_ptr = 0;
	} else if ((cpool->w_ptr + len) < cpool->depth) {
		cpool->w_ptr += len;
	} else {
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_get_cirbuf_pool);
		return NULL;
	}

	cpool->availcnt -= len;
	/* Return the free buffer from the pool */
	return (uint8*)(cpool->buf + (w_ptr * cpool->item_size));
}

/** Admit the buffer back to free list */
static void
pciedev_free_cirbuf_pool(msgbuf_ring_t *ring, void *p, uint16 len)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;

	/* convert len of bytes into index */
	len /= MSGBUF_LEN(ring);

	/* update read pointer indicating free of memory */
	cpool->r_ptr += len;

	/* wrap around the buffer if index has crossed the depth */
	if (cpool->r_ptr >= cpool->depth)
		cpool->r_ptr = 0;

	cpool->availcnt += len;
	/* wrap back to index 0 if read & write pointers are same */
	if (cpool->availcnt == cpool->depth)
		cpool->r_ptr = cpool->w_ptr = 0;
}

#ifdef BCMPOOLRECLAIM
/** allocate a lcl buf pool to support poolreclaim */
static void *
pciedev_alloc_lclbuf_pool(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
	lcl_buf_pool_t *pool;

	pool = ring->buf_pool;
	return MALLOC(pciedev->osh, pool->item_cnt * MSGBUF_LEN(ring));
}

/** free lcl buf pool used to support poolreclaim */
static void
pciedev_dealloc_lclbuf_pool(struct dngl_bus *pciedev, msgbuf_ring_t *ring, void *p)
{
	lcl_buf_pool_t *pool = ring->buf_pool;

	MFREE(pciedev->osh, p, pool->item_cnt * MSGBUF_LEN(ring));
}
#endif /* BCMPOOLRECLAIM */

/**
 * Allocates one 'local' buffer from the lclbuf pool. One lclbuf can be used to store multiple
 * messages from the host.
 */
void *
pciedev_lclpool_alloc_lclbuf(msgbuf_ring_t *ring)
{
	void *ret;
	lcl_buf_pool_t * pool = ring->buf_pool;

	/* check for avail space */
	if (pool->free == NULL) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"Pool empty Ring name: %c%c%c%c\n",
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]);
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_lclpool_alloc_lclbuf);
		return NULL;
	}

	/* Retrieve back the buffer */
	ret = pool->free->p;
	if (ret) {

		/* Make pkt in cur node as NULL */
		pool->free->p = NULL;

		pool->free = pool->free->nxt;
#ifdef PCIEDEV_DBG_LOCAL_POOL
		pciedev_check_free_p(ring, pool->free, ret, __FUNCTION__);
#endif /* PCIEDEV_DBG_LOCAL_POOL */
		pool->availcnt--;
	} else {
		PCI_ERROR(("Failed to get lcl buf from free list! Ring Name: %c%c%c%c\n",
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]));
		ASSERT(ret != NULL);
	}
	return ret;
} /* pciedev_lclpool_alloc_lclbuf */

/** Return the 'local' buffer back to the lclbuf pool */
static void
pciedev_lclpool_free_lclbuf(msgbuf_ring_t *ring, void *p)
{
	lcl_buf_pool_t * pool = ring->buf_pool;
	lcl_buf_t * lcl_free = pool->free;

	if (lcl_free == pool->head) {
		PCI_ERROR(("pool allready full cant admit %x to ring name: %c%c%c%c\n",
			(uint32)p,
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]));
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_free_lclbuf_pool);
		ASSERT(0);
		return;
	}

	if (lcl_free == NULL) {
		/* If all items are exhausted, free will point to NULL */
		/* restore free to tail the moment we get atleast 1 pkt back */
		lcl_free = pool->tail;
		lcl_free->p = p;
		pool->free = lcl_free;
	} else {
#ifdef PCIEDEV_DBG_LOCAL_POOL
		/* Store pkt in previous node */
		pciedev_check_free_p(ring, lcl_free, p, __FUNCTION__);
#endif /* PCIEDEV_DBG_LOCAL_POOL */
		lcl_free->prv->p = p;
		/* Move free ptr to previous node */
		pool->free = lcl_free->prv;
	}
	/* Increment avail count */
	pool->availcnt++;
} /* pciedev_lclpool_free_lclbuf */

/**
 * Handles one 'rx buffer post' message from the host. This message is consumed from a queue,
 * which can contain multiple messages. One lclbuf can contain multiple host messages. If the lclbuf
 * containing the host message(s) was fully consumed, then it is freed at function exit.
 *
 * Output parameters:
 *     'bufid':     unique identifier, assigned by the host, for rx buffer posted by the host
 *     'haddr':     64 bit host address
 *     'len':       host addr len
 *     'meta_addr': 64 bit metadata addr
 *     'meta_len':  metadata len
 *
 * Return value: 0 on success, -1 on failure.
 */
static int
pciedev_process_rxpost_msg(struct dngl_bus *pciedev, uint32 *bufid, uint16 *len,
	dma64addr_t *haddr)
{
	uint8 r, depth;
	uint8 max_item;
	uint8 cur_item = 0;
	void *lcl_buf = NULL;  /**< lcl buffer containing one or more 'rx buf post' messages */
	uint8 *p = NULL;       /**< points at one 'rx buf post' message in local buffer */
	msgbuf_ring_t *ring = pciedev->htod_rx;
	/* pool/queue containing one or more 'rx buf post' messages to process */
	inuse_lclbuf_pool_t * rxpool = ring->buf_pool->inuse_pool;
	inuse_lclbuf_pool_t *poolreclaim_inuse_pool = NULL;

#if defined(BCMHWA) && defined(HWA_RXPOST_BUILD)
	/* For rx pktfetch case we allocate reserved RPH from HWA */
	if (hwa_rph_allocate(bufid, len, haddr, FALSE) == NULL) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"pciedev_process_rxpost_msg : Out of RPH\n");
		DBG_BUS_INC(pciedev, pciedev_process_rxpost_msg);
		return -1;
	}

	BCMPCIE_IPC_HPA_TEST(pciedev, *bufid,
		BCMPCIE_IPC_PATH_RECEIVE, BCMPCIE_IPC_TRANS_REQUEST);
	BUZZZ_KPI_PKT1(KPI_PKT_MAC_RXFIFO, 1, *bufid);

	return 0;
#endif /* BCMHWA && HWA_RXPOST_BUILD */

	BCM_REFERENCE(poolreclaim_inuse_pool);
#ifdef BCMPOOLRECLAIM
	if (BCMPOOLRECLAIM_ENAB()) {
		poolreclaim_inuse_pool = pciedev->htod_rx_poolreclaim_inuse_pool;
		if (poolreclaim_inuse_pool) {
			/* poolreclaim_inuse_pool is initialized, then consume
			 * poolreclaim_inuse_pool first, before going to inuse_pool of the flow ring
			 */
			rxpool = poolreclaim_inuse_pool;
		}
	}
#endif /* BCMPOOLRECLAIM */

	/* intialize haddr to NULL */
	PHYSADDR64HISET(*haddr, 0);
	PHYSADDR64LOSET(*haddr, 0);

	r = rxpool->r_ptr;
	depth = rxpool->depth;
	/* Check if any buffer locally available */
	if (!NTXPACTIVE(rxpool->r_ptr, rxpool->w_ptr, depth)) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"pciedev_process_rxpost_msg : No active element in local array \n");
		DBG_BUS_INC(pciedev, pciedev_process_rxpost_msg);
		return -1;
	}

	/* Retrieve the head buffer pointer */
	lcl_buf = rxpool->buf[r].p; /* the 'rx buffer post' message is contained in this lcl_buf */
	p = (uint8*)lcl_buf;

	/* max no of host buffers available in this chunk */
	max_item = rxpool->buf[r].max_items;

	cur_item = rxpool->buf[r].used_items;
	if ((cur_item < max_item) && (p != NULL)) {
		/* retrieve individual message from the local buffer */
		p = p + (MSGBUF_LEN(ring) * (cur_item));

		/* distills bufid, len, ... etc fields out of rx post message ('p') */
		pciedev_unpack_rxpost_msg(pciedev, p, bufid, len, haddr);

		BCMPCIE_IPC_HPA_TEST(pciedev, *bufid,
			BCMPCIE_IPC_PATH_RECEIVE, BCMPCIE_IPC_TRANS_REQUEST);

		rxpool->buf[r].used_items++; /* used one 'rx post' message from local buffer */

		if (rxpool->buf[r].used_items == rxpool->buf[r].max_items) {
			rxpool->buf[r].used_items = 0;
			rxpool->buf[r].max_items = 0;
			/* Update read pointer */
			rxpool->r_ptr = NEXTTXP(rxpool->r_ptr, depth);

#ifdef BCMPOOLRECLAIM
			if (BCMPOOLRECLAIM_ENAB() && poolreclaim_inuse_pool) {
				/* local buffer fully used up. free that into pool */
				pciedev_dealloc_lclbuf_pool(pciedev, ring, lcl_buf);

				/* Check if all lcl buffers are freed */
				if (!NTXPACTIVE(rxpool->r_ptr, rxpool->w_ptr, depth)) {
					/* Free poolreclaim inuse_pool as it is empty. */
					pciedev_free_poolreclaim_inuse_bufpool(pciedev,
							poolreclaim_inuse_pool);
					pciedev->htod_rx_poolreclaim_inuse_pool = NULL;
				}
			} else
#endif /* BCMPOOLRECLAIM */
			{
				/* local buffer fully used up. free that into pool */
				pciedev_lclpool_free_lclbuf(ring, lcl_buf);

				/* Check for more rxbuffers in the ring */
				pciedev_msgbuf_intr_process(pciedev);
			}
		}
		return 0;
	}

	DBG_BUS_INC(pciedev, pciedev_process_rxpost_msg);

	return -1;
} /* pciedev_process_rxpost_msg */

#ifdef BCMPOOLRECLAIM
/**
	put back following values taken from rx frag to lcl buffer
	1. 64 bit host address
	2. host addr len
	3. 64 bit metadata addr
	4. metadata len
	5. frag pkt id (bufid)
*/
static int
pciedev_put_host_addr(struct dngl_bus *pciedev, uint32 *bufid, uint16 *len, dma64addr_t *haddr)
{
	uint8 r, depth;
	uint8 max_item;
	uint8 cur_item = 0;
	void *lcl_buf = NULL;
	uint8 *p = NULL;
	msgbuf_ring_t *ring = pciedev->htod_rx;
	inuse_lclbuf_pool_t *rxpool;
	inuse_lclbuf_pool_t *poolreclaim_inuse_pool = pciedev->htod_rx_poolreclaim_inuse_pool;

	if (poolreclaim_inuse_pool) {
		/* poolreclaim_inuse_pool is initialized, so use it for
		 * putting back host addresses.
		 */
		rxpool = poolreclaim_inuse_pool;
	} else {
		/* poolreclaim_inuse_pool is not initialized yet. So see if host addresses
		 * can be put back into inuse_pool of flow ring (htod_rx).
		 */
		rxpool = ring->buf_pool->inuse_pool;
	}
	r = rxpool->r_ptr;
	depth = rxpool->depth;

	/* Check if any buffer locally available */
	if (NTXPACTIVE(rxpool->r_ptr, rxpool->w_ptr, depth)) {
		/* Retrieve the head buffer pointer */
		lcl_buf = rxpool->buf[r].p;
		p = (uint8*)lcl_buf;

		/* max no of host buffers available in this chunk */
		max_item = rxpool->buf[r].max_items;
		cur_item = rxpool->buf[r].used_items;

		if (cur_item != 0) {
			/* some space is available in current lcl buffer
			 * so put back host addr in current lcl buffer
			 */
			goto put_back_hostaddr;
		}
		/* if used_items is 0 in current lcl buffer or if
		 * NTXPACTIVE() is zero -- indicating no valid lcl buffers,
		 * rolling back r_ptr will not help as all lcl buffers might have been full.
		 * So initialize poolreclaim_inuse_pool and put back
		 * host addr in that new lcl buffer.
		 */
	}

	if (poolreclaim_inuse_pool == NULL) {
		/* poolreclaim_inuse_pool is not initialized yet.
		 * So initialise and use to keep host addresses.
		*/
		if (pciedev_init_poolreclaim_inuse_bufpool(pciedev, ring) != BCME_OK) {
			DBG_BUS_INC(pciedev, pciedev_put_host_addr);
			return BCME_ERROR;
		}

		poolreclaim_inuse_pool = pciedev->htod_rx_poolreclaim_inuse_pool;
		rxpool = poolreclaim_inuse_pool;

		r = rxpool->r_ptr;
		depth = rxpool->depth;
	}

	/* Check if lcl buffers are available to put back host addr */
	if (!WRITE_SPACE_AVAIL(rxpool->r_ptr, rxpool->w_ptr, depth)) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_INFO,
			"pciedev_put_host_addr : No free poolreclaim inuse pool entries.\n");
		DBG_BUS_INC(pciedev, pciedev_put_host_addr);
		return BCME_ERROR;
	}

	/* Get free lclbuf. */
	if ((lcl_buf = pciedev_alloc_lclbuf_pool(pciedev, ring)) == NULL) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_INFO,
			"pciedev_put_host_addr : Malloc failed for poolreclaim lcl buffers.\n");
		DBG_BUS_INC(pciedev, pciedev_put_host_addr);
		return BCME_ERROR;
	}

	rxpool->r_ptr = PREVTXP(rxpool->r_ptr, depth);

	r = rxpool->r_ptr;
	/* Retrieve the head buffer pointer */
	rxpool->buf[r].p = lcl_buf;
	p = (uint8*)lcl_buf;

	/* max no of host buffers available in this chunk */
	max_item = ring->buf_pool->item_cnt;

	rxpool->buf[r].max_items = max_item;
	rxpool->buf[r].used_items = max_item;

put_back_hostaddr :

	if (p != NULL) {

		rxpool->buf[r].used_items--;

		cur_item = rxpool->buf[r].used_items;

		/* retrieve individual message */
		p = p + (MSGBUF_LEN(ring) * (cur_item));

		/* process rx post message */
		pciedev_putback_rxpost_msg(pciedev, p, bufid, len, haddr);

		BCMPCIE_IPC_HPA_TEST(pciedev, *bufid,
			BCMPCIE_IPC_PATH_RECEIVE, BCMPCIE_IPC_TRANS_ROLLBACK);

		return BCME_OK;
	}
	return BCME_ERROR;
}
#endif /* BCMPOOLRECLAIM */

int
pciedev_manage_rxcplid(pktpool_t *pool, void *arg, void *p, int rxcpl_needed)
{
#ifdef BCMPOOLRECLAIM
	/* Most frequent path is else path, so rxcpl_needed is checked first
	 * to reduce number of checks
	 */
	if ((rxcpl_needed == REMOVE_RXCPLID) && BCMPOOLRECLAIM_ENAB()) {
		return pciedev_freeup_rxcplid(pool, arg, p, rxcpl_needed);
	} else
#endif /* BCMPOOLRECLAIM */
	{
		return pciedev_fillup_rxcplid(pool, arg, p, rxcpl_needed);
	}

}

/** AMPDU (receive) reordering related. Is called back by the 'pktpool' on a pktpool_get(). */
int
pciedev_fillup_rxcplid(pktpool_t *pool, void *arg, void *p, int dummy)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	rxcpl_info_t *p_rxcpl_info;

	if (PKTRXCPLID(pciedev->osh, p) != 0) {
		PCI_TRACE(("pciedev_fillup_rxcplid: pktrxcplid is zero\n"));
		return 0;
	}
	p_rxcpl_info = bcm_alloc_rxcplinfo();
	if (p_rxcpl_info == NULL) {
		PCI_ERROR(("couldn't allocate rxcpl info for lbuf: %x \n", (uint32)p));
		PCIEDEV_RXCPL_ERR_INCR(pciedev);
		return -1;
	}
	if (p_rxcpl_info->rxcpl_id.idx == 0) {
		PCI_ERROR(("lbuf: got the p_rxcpl_info->rxcpl_id.idx as zero\n"));
		ASSERT(p_rxcpl_info->rxcpl_id.idx != 0);
	}
	PKTSETRXCPLID(pciedev->osh, p, p_rxcpl_info->rxcpl_id.idx);
	return 0;
}

#ifdef BCMPOOLRECLAIM
static int
pciedev_freeup_rxcplid(pktpool_t *pool, void *arg, void *p, int rxcpl_needed)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	rxcpl_info_t *p_rxcpl_info;

	if (PKTRXCPLID(pciedev->osh, p) == 0) {
		/* RXCPLID is not present */
		return BCME_OK;
	}
	p_rxcpl_info = bcm_id2rxcplinfo(PKTRXCPLID(pciedev->osh, p));

	if (p_rxcpl_info == NULL) {
		PCI_ERROR(("couldn't get rxcpl info for lbuf: %x \n", (uint32)p));
		PCIEDEV_RXCPL_ERR_INCR(pciedev);
		return BCME_ERROR;
	}
	if (p_rxcpl_info->rxcpl_id.idx == 0) {
		PCI_ERROR(("lbuf: got the p_rxcpl_info->rxcpl_id.idx as zero\n"));
		ASSERT(p_rxcpl_info->rxcpl_id.idx != 0);
	}

	bcm_free_rxcplinfo(p_rxcpl_info);

	PKTSETRXCPLID(pciedev->osh, p, NULL);

	return BCME_OK;
}
#endif /* BCMPOOLRECLAIM */

/**
 * Callback function, registered with pktpool_hostaddr_fill_register(), gets called back on a
 * RXLFRAG pktpool_get(). Allocates and initializes an rx buffer (by pointing at a buffer located in
 * host memory).
 */
int
pciedev_manage_haddr(pktpool_t *pool, void *arg, void *p, int rxcpl_needed)
{
#ifdef BCMPOOLRECLAIM
	/* Most frequent path is else path, so rxcpl_needed is checked first
	 * to reduce number of checks
	 */
	if ((rxcpl_needed == REMOVE_RXCPLID) && BCMPOOLRECLAIM_ENAB()) {
		return pciedev_freeup_haddr(pool, arg, p, rxcpl_needed);
	} else
#endif // endif
	{
		/* interprets one rxpost message */
		return pciedev_fillup_haddr(pool, arg, p, rxcpl_needed);
	}
}

#ifdef BCMPOOLRECLAIM
/**
 * Takes in a frag as input
 * removes rxcplid and host address & len associated with it and puts back to
 * alternate lcl buffer
 */
static int
pciedev_freeup_haddr(pktpool_t *pool, void *arg, void *frag, int rxcpl_needed)
{

	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	uint16 len;
	dma64addr_t haddr;
	uint32 bufid;

	if (pciedev_freeup_rxcplid(pool, arg, frag, rxcpl_needed) != BCME_OK) {
		/* Freeing rxcplid failed */
		PCI_TRACE(("pciedev_freeup_haddr: freeing rxcplid failed\n"));
		return BCME_ERROR;
	}

	/* Check if lfrag has host address associated with it */
	if (PKTISRXFRAG(pciedev->osh, frag) == 0) {
		/* host address is not associated with lfrag */
		PCI_TRACE(("pciedev_freeup_haddr: haddr not associated with lfrag\n"));
		return BCME_OK;
	}

	/* Load 64 bit host address */
	PHYSADDR64HISET(haddr, PKTFRAGDATA_HI(pciedev->osh, frag, 1));
	PHYSADDR64LOSET(haddr, (PKTFRAGDATA_LO(pciedev->osh, frag, 1) - pciedev->tcmsegsz));

	/* frag len */
	len = PKTFRAGLEN(pciedev->osh, frag, 1) + pciedev->tcmsegsz;
	/* pktid */
	bufid = PKTFRAGPKTID(pciedev->osh, frag);

	/* Check if buffers available locally */
	if (pciedev_put_host_addr(pciedev, &bufid, &len, &haddr)) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_INFO,
			"Putting host addr back to lcl failed.\n");
		return BCME_ERROR;
	}

	/* Reset rxfrag that host addr is valid */
	PKTRESETRXFRAG(pciedev->osh, frag);

	return BCME_OK;
}
#endif /* BCMPOOLRECLAIM */

/**
 * Receive related, is called back by the 'pktpool' on a pktpool_get(). Intention of this function
 * is to allocate a rx buffer and subsequently populate it with e.g. the rx buffer address in host
 * memory. Assumption is that a 'rxpost' message from the host is available in dongle memory when
 * this function is called.
 */
int
pciedev_fillup_haddr(pktpool_t *pool, void *arg, void *frag, int rxcpl_needed)
{

	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	uint16 len;
	dma64addr_t haddr;
	uint32 bufid;
	rxcpl_info_t *p_rxcpl_info = NULL;

	/* Check if lfrag already has host address associated with it */
	if (PKTRXCPLID(pciedev->osh, frag) != 0) {
		PCI_TRACE(("pciedev_fillup_haddr:already host address associated with lfrag\n"));
		rxcpl_needed = FALSE;
	}

	if (rxcpl_needed)  {
		/* Check if we have space for rx completion ID */
		p_rxcpl_info = bcm_alloc_rxcplinfo();
		if (p_rxcpl_info == NULL) {
			PCI_ERROR(("couldn't allocate rxcpl info: %x \n", (uint32)frag));
			PCI_ERROR(("couldn't allocate rxcpl\n"));
			DBG_BUS_INC(pciedev, pciedev_fillup_haddr);
			return -1;
		}
	}
	if (PKTISRXFRAG(pciedev->osh, frag)) {
		if (rxcpl_needed)
			PKTSETRXCPLID(pciedev->osh, frag, p_rxcpl_info->rxcpl_id.idx);
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"Frag allready filled up : %p \n", (uint32)frag);
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
		/* creating a head room of 8 bytes to accomodate cmn_msg_hdr_t.
		 * In non monitor case rxhdr is stripped of in wlc_recv().
		 * So there is enough headroom for cmn_msg_hdr
		 */
		if (pciedev->monitor_mode && !RXFIFO_SPLIT()) {
			PKTPULL(pciedev->osh, frag, sizeof(cmn_msg_hdr_t));
		}
#endif /* WL_MONITOR && WL_MONITOR_DISABLED */
		return 0;
	}

	/* A rxbuf post message from the host should be received previously in dongle memory */
	if (pciedev_process_rxpost_msg(pciedev, &bufid, &len, &haddr)) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_INFO,
			"No lcl RX post message available \n");
		if (!rxcpl_needed) {
			releasing_not_needed_rxcpl_buffer ++;
			return -1;
		}
		if (p_rxcpl_info != NULL)
			bcm_free_rxcplinfo(p_rxcpl_info);
		DBG_BUS_INC(pciedev, pciedev_fillup_haddr);
		return -1;
	}

	/* setup the rx completion ID */
	if (rxcpl_needed) {
		if (p_rxcpl_info->rxcpl_id.idx == 0) {
			PCI_ERROR(("rxlfrag: got the p_rxcpl_info->rxcpl_id.idx as zero\n"));
			ASSERT(p_rxcpl_info->rxcpl_id.idx != 0);
			DBG_BUS_INC(pciedev, pciedev_fillup_haddr);
		}
		PKTSETRXCPLID(pciedev->osh, frag, p_rxcpl_info->rxcpl_id.idx);
	}

	/* Load 64 bit host address */
	PKTSETFRAGDATA_HI(pciedev->osh, frag, 1, PHYSADDR64HI(haddr));
	PKTSETFRAGDATA_LO(pciedev->osh, frag, 1, PHYSADDR64LO(haddr) + pciedev->tcmsegsz);

	/* frag len */
	PKTSETFRAGLEN(pciedev->osh, frag, 1, (len - pciedev->tcmsegsz));
	/* pktid */
	PKTSETFRAGPKTID(pciedev->osh, frag, bufid);

	/* Mark rxfrag that host addr is valid */
	PKTSETRXFRAG(pciedev->osh, frag);

#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
	/* creating a head room of 8 bytes to accomodate cmn_msg_hdr_t.
	 * In non monitor case rxhdr is stripped of in wlc_recv().
	 *				 * So there is enough headroom for cmn_msg_hdr
	 */
	if (pciedev->monitor_mode && !RXFIFO_SPLIT()) {
		PKTPULL(pciedev->osh, frag, sizeof(cmn_msg_hdr_t));
	}
#endif /* WL_MONITOR && WL_MONITOR_DISABLED */
	return 0;
} /* pciedev_fillup_haddr */

/**
 * Called when one or more 'rx buf post' messages have been received from the host in device memory.
 * Queues the messages in an 'inuse' list for (later) processing.
 *
 * Input parameters:
 *    p        : local buffer containing the message(s)
 *    max_items: number of messages
 *
 */
static void
pciedev_add_to_inuselist(struct dngl_bus *pciedev, void *p, uint8 max_items)
{
	msgbuf_ring_t *ring = pciedev->htod_rx;
	/* rxpool: pool of local buffers receiving messages from host */
	inuse_lclbuf_pool_t * rxpool = ring->buf_pool->inuse_pool;
	uint8 w, r, depth;

	w = rxpool->w_ptr;
	depth = rxpool->depth;
	r = rxpool->r_ptr;

	/* check if inuse list has space */
	if (!NTXPAVAIL(r, w, depth)) {
		PCI_TRACE(("pciedev_add_to_inuselist: inuse list has no space\n"));
		DBG_BUS_INC(pciedev, pciedev_add_to_inuselist);
		return;
	}

	/* Store buf pointer & max items in the buff */
	rxpool->buf[w].p = p; /* type of p: 'inuse_lcl_buf_t' */
	rxpool->buf[w].max_items = max_items;

	/* Update write pointer */
	rxpool->w_ptr = NEXTTXP(rxpool->w_ptr, depth);
}

/**
 * XXX: there is an issue handling different burst lengths on H2D tx side and the problem
 * XXX: is identified to be back plane clock related, so the ask is force HT when there
 * XXX: is a pending H2D transaction
 */
void
pciedev_manage_h2d_dma_clocks(struct dngl_bus *pciedev)
{
	if (pciedev->force_ht_on) {
		PCI_TRACE(("%s: force clock on\n", __FUNCTION__));
		OR_REG(pciedev->osh, &pciedev->regs->u.pcie2.clk_ctl_st, CCS_FORCEHT);
	} else {
		PCI_TRACE(("%s: removing the force clock on\n", __FUNCTION__));
		AND_REG(pciedev->osh, &pciedev->regs->u.pcie2.clk_ctl_st, ~CCS_FORCEHT);
	}
	return;
}

/**
 * Distills the fields out of an 'rx buffer post' message received from the host.
 * Function input parameters:
 *     p         : rx buf post message received from host
 * Function output parameters:
 *     bufid     : the id that the host assigned to this new rx buffer
 *                 (also known as host pktid)
 *     len       : the rx buffer length as specified by the host
 *     haddr     : 64 bits host address where rx buffer resides
 */
static void
pciedev_unpack_rxpost_msg(struct dngl_bus *pciedev, void *p, uint32 *bufid, uint16 * len,
	dma64addr_t *haddr)
{
	switch (MSGBUF_ITEM_TYPE(pciedev->htod_rx))
	{
		case MSGBUF_WI_WI64:
		{
			host_rxbuf_post_t *wi64 = (host_rxbuf_post_t *)p;
#ifdef H2D_CHECK_SEQNUM
			uint8 ring_seqnum = pciedev->htod_rx->h2d_seqnum % H2D_EPOCH_MODULO;
			uint8 msg_seqnum = wi64->cmn_hdr.epoch;

			/* check for sequence number sanity */
			if (msg_seqnum == ring_seqnum) {
				pciedev->htod_rx->h2d_seqnum++;
			} else {
				PCI_ERROR(("RXPOST :error in seqnum : got %d exp %d \n",
					msg_seqnum, ring_seqnum));
				ASSERT(0);
			}
#endif /* H2D_CHECK_SEQNUM */

			*len =  ltoh16(wi64->data_buf_len); /* PKTLEN */
			*bufid = ltoh32(wi64->cmn_hdr.request_id); /* BUFID */
			/* Fetch HOST address and mark it a PCIe address */
			PHYSADDR64LOSET(*haddr, HADDR64_LO(wi64->data_buf_haddr64));
			PHYSADDR64HISET(*haddr,
				(HADDR64_HI(wi64->data_buf_haddr64) | PCIE_ADDR_OFFSET));
			break;
		}

		case MSGBUF_WI_CWI32:
		{
			hwa_rxpost_cwi32_t *cwi32 = (hwa_rxpost_cwi32_t *)p;

			*len = pciedev->rxpost_data_buf_len;
			*bufid = cwi32->host_pktid;
			PHYSADDR64LOSET(*haddr, cwi32->data_buf_haddr32);
			/* host_physaddrhi already has PCIE_ADDR_OFFSET */
			PHYSADDR64HISET(*haddr, pciedev->host_physaddrhi);
			break;
		}

		case MSGBUF_WI_CWI64:
		{
			hwa_rxpost_cwi64_t *cwi64 = (hwa_rxpost_cwi64_t *)p;

			*len = pciedev->rxpost_data_buf_len;
			*bufid = cwi64->host_pktid;
			PHYSADDR64LOSET(*haddr, HADDR64_LO(cwi64->data_buf_haddr64));
			PHYSADDR64HISET(*haddr,
				(HADDR64_HI(cwi64->data_buf_haddr64) | PCIE_ADDR_OFFSET));
			break;
		}

		case MSGBUF_WI_ACWI32:
		case MSGBUF_WI_ACWI64:
		default:
			PCI_ERROR(("Invalid RxPost WI format\n"));
	}

	ASSERT((*len != 0) && (*bufid != 0));

} /* pciedev_unpack_rxpost_msg */

inline int
pciedev_dma_queue_avail(dma_queue_t *dmaq)
{
	uint16 rd_idx = dmaq->r_index;
	uint16 wr_idx = dmaq->w_index;

	PCI_TRACE(("pciedev_dma_queue_avail:rd_idx=%d; wr_idx=%d\n", rd_idx, wr_idx));

	/* NOTE: The maximum no. of elements that can be held in the Q
	 * at a time is (MAX_DMA_QUEUE_LEN - 1). When Q is full, wr_idx
	 * will point to an empty Q location, just before rd_idx. So for Q
	 * to have adequate space for enque, this utility function should return > 1
	 */
	return (rd_idx > wr_idx ? (rd_idx - wr_idx) : (dmaq->max_len - wr_idx + rd_idx));
}

static int
pciedev_dispatch_core_fetch_rqst(struct fetch_rqst *fr, struct dngl_bus *pciedev, uint8 dma_ch,
	h2d_dma_fn_t dma_fn, dma_mode_type_t dma_mode)
{
	dma64addr_t src;
	uint16 dmalen;
	int dma_qavail;
	uint32 txdesc, rxdesc;

	ASSERT(fr);
	if (dma_ch >= MAX_DMA_CHAN) {
		DBG_BUS_INC(pciedev, pciedev_dispatch_core_fetch_rqst);
		PCI_ERROR(("ERROR: Invalid dma_ch, %d\n", dma_ch));
		return BCME_ERROR;
	}

	txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, dma_ch, HTOD, TXDESC);
	rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, dma_ch, HTOD, RXDESC);

	if (pciedev_ds_in_host_sleep(pciedev)) {
		DBG_BUS_INC(pciedev, pciedev_dispatch_core_fetch_rqst);
		return BCME_ERROR;
	}

	PHYSADDR64HISET(src, (uint32) ltoh32(PHYSADDR64HI(fr->haddr)));
	PHYSADDR64LOSET(src, (uint32) ltoh32(PHYSADDR64LO(fr->haddr)));
	dmalen = fr->size;

	dma_qavail = pciedev_dma_queue_avail(pciedev->htod_dma_q[dma_ch]);

	if (txdesc >= MIN_TXDESC_AVAIL && rxdesc >= MIN_RXDESC_AVAIL && (dma_qavail > 1)) {
		if (dma_mode == PCIE_DMA_MODE_ASYNCHRONOUS) {
			pciedev_enque_fetch_cmpltq(pciedev, fr, dma_ch);
		}
		/* kicks off dma */
		dma_fn(pciedev, src, dmalen, (uint8*) fr->dest, NULL, MSG_TYPE_HOST_FETCH, dma_ch);
		return BCME_OK;
	} else {
		DBG_BUS_INC(pciedev, pciedev_dispatch_core_fetch_rqst);
		PCI_ERROR(("FE: %d, %d, %d\n", txdesc, rxdesc, dma_qavail));
		return BCME_ERROR;
	}
}

/**
 * Called by the dongle OS, requesting the PCIe subsystem to start fetching the next item (in 'fr')
 * from the host.
 */
int
pciedev_dispatch_fetch_rqst(struct fetch_rqst *fr, void *arg)
{
	struct dngl_bus *pciedev = (struct dngl_bus *)arg;
	return pciedev_dispatch_core_fetch_rqst(fr, pciedev, pciedev->default_dma_ch,
		pciedev_h2dmsgbuf_dma, PCIE_DMA_MODE_ASYNCHRONOUS);
}

static void
pciedev_enque_fetch_cmpltq(struct dngl_bus *pciedev, struct fetch_rqst *fr, uint8 dmach)
{
	pciedev_fetch_cmplt_q_t *fcq = pciedev->fcq[dmach];

	if (fcq->head == NULL)
		fcq->head = fcq->tail = fr;
	else {
		fcq->tail->next = fr;
		fcq->tail = fr;
	}
	fcq->count++;
	fcq->tail->next = NULL;
}

static struct fetch_rqst *
pciedev_deque_fetch_cmpltq(struct dngl_bus *pciedev, uint8 dmach)
{
	pciedev_fetch_cmplt_q_t *fcq = pciedev->fcq[dmach];
	struct fetch_rqst *fr;

	if (fcq->head == NULL) {

		fr = NULL;
	} else if (fcq->head == fcq->tail) {
		ASSERT(fcq->count > 0);
		if (fcq->count == 0) {
			DBG_BUS_INC(pciedev, pciedev_deque_fetch_cmpltq);
		}
		fcq->count--;
		fr = fcq->head;
		fcq->head = fcq->tail = NULL;
	} else {
		ASSERT(fcq->count > 0);
		if (fcq->count == 0) {
			DBG_BUS_INC(pciedev, pciedev_deque_fetch_cmpltq);
		}
		fcq->count--;
		fr = fcq->head;
		fcq->head = fcq->head->next;
		fr->next = NULL;
	}
	return fr;
}

/**
 * Called when a new so-called 'internal' 'host fetch' message is available in device memory,
 * signalling that new data to transmit to the wireless medium is now available in dongle memory.
 */
void
pciedev_process_tx_payload(struct dngl_bus *pciedev, uint8 dmach)
{
	struct fetch_rqst *fr;
	bool cancelled = FALSE;

	fr = pciedev_deque_fetch_cmpltq(pciedev, dmach);
	if (fr == NULL) {
		PCI_TRACE(("pciedev_process_tx_payload: fetch req is NULL\n"));
		DBG_BUS_INC(pciedev, pciedev_process_tx_payload);
		return;
	}

	/*
	* Let the fetch callback be called in the case of PCIe in D3
	* suspend mode. Callback can be safely called Since the mem2mem
	* and WL DMAs are already blocked at this point.
	*/
	if (pciedev_ds_in_host_sleep(pciedev)) {
		DBG_BUS_INC(pciedev, pciedev_process_tx_payload);
		PCI_ERROR(("pciedev_process_tx_payload: pciedev_ds_in_host_sleep cb:0x%x\n",
			(uint32)fr->cb));
		pciedev->process_payload_in_d3 ++;
	}
	/* If the fetch_rst was cancelled while in bus DMA queue,
	 * need to return it back to Host indicating the cancellation
	 */
	if (FETCH_RQST_FLAG_GET(fr, FETCH_RQST_CANCELLED)) {
		cancelled = TRUE;
		/* Clear the fetch_rqst flag now, to avoid misunderstandings later */
		FETCH_RQST_FLAG_CLEAR(fr, FETCH_RQST_CANCELLED);
	}

	FETCH_RQST_FLAG_CLEAR(fr, FETCH_RQST_IN_BUS_LAYER);

	/* Call the registered callback function, e.g. pciedev_flowring_fetch_cb() */
	if (fr->cb)
		fr->cb(fr, cancelled);
	else {
		PCI_ERROR(("pciedev_process_tx_payload: No callback registered for fetch_rqst!\n"));
		ASSERT(0);
		DBG_BUS_INC(pciedev, pciedev_process_tx_payload);
	}
}

/**
 * H2D direction. The 'pciedev' device contains multiple (local) message rings. When a message from
 * the host needs to be (DMA) transferred into local memory, local memory first has to be reserved,
 * after which the host address on which the message resides needs to be determined, so the caller
 * can subsequently program DMA with that address.
 *
 * Returns lower 32 bits of host address of msgbuf to be read from (why is it not a 64 bit
 *     address?)
 *
 * @param[in/out] ring          local ring
 * @param[out]    available_len length in [bytes]
 * @param[in]     max_len       length in [bytes], ignored if '0', suppress related.
 */
static void*
pciedev_get_msgbuf_host_addr(msgbuf_ring_t *ring, uint16 *available_len /* out */, uint16 max_len)
{
	uint16 depth;
	void *ret_addr = NULL; 	/**< host address, not dongle address */
	uint16 availcnt;	/**< unit: [messages] */
	cir_buf_pool_t *cpool = ring->cbuf_pool;

	/* get available count (unit:messages) that can fit in local buffer pool */
	availcnt = (cpool == NULL) ? (ring->buf_pool->item_cnt) : (CIR_BUFPOOL_AVAILABLE(cpool));
	/* If buffer is available, restrict to the limit (if the limit is set).
	* This should ensure that we fetch only one item at a time for control
	*/
	if (availcnt && !max_len &&
		ring->work_item_fetch_limit != PCIEDEV_WORK_ITEM_FETCH_NO_LIMIT) {
		availcnt =  ring->work_item_fetch_limit;
		ring->work_item_fetch_limit = PCIEDEV_WORK_ITEM_FETCH_HOLD;
	}

	depth = MSGBUF_MAX(ring);

	/* First check if there is any (message) space available in the local message ring */
	*available_len = MSGBUF_READ_AVAIL_SPACE(ring);
	if (*available_len == 0) {
		PCI_TRACE(("pciedev_get_msgbuf_host_addr: no space in local ring\n"));
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_get_src_addr);
		return NULL;
	}

	if (max_len) {
		max_len = max_len / MSGBUF_LEN(ring);
		*available_len = MIN(MIN(*available_len, availcnt), max_len);
	} else {
		/* both message ring and buffer pool must have space available */
		*available_len = MIN(*available_len, availcnt);
	}
	ASSERT(*available_len <= depth);

	/*
	 * We don't do dma on wrapped around space. Instead do it in two steps where you read end
	 * region first followed by top region.
	 *
	 * Structure 'ring' contains a host address, which for the common rings was written by the
	 * host directly into device memory, and for flow rings by the 'flow ring create' message.
	 */
	ret_addr = (uint8*)MSGBUF_BASE(ring) + (ring->fetch_ptr * MSGBUF_LEN(ring));

	/*
	 * Please note that we do not update the read pointer here. Only read pending pointer is
	 * updated, so that next reader knows where to read data from. Read pointer can only be
	 * updated when the read is complete.
	 */
	if ((ring->fetch_ptr + *available_len) >= depth)
		ring->fetch_ptr = 0;
	else
		ring->fetch_ptr += *available_len;

	ASSERT(ring->fetch_ptr < depth);

	/* Make it byte count rather than index count */
	*available_len = *available_len * MSGBUF_LEN(ring);

	return ret_addr; /* return *host* address */
} /* pciedev_get_msgbuf_host_addr */

/** H2D direction: update h2d ring read pointer after h2d dma is complete */
static void
pciedev_ring_update_readptr(struct dngl_bus *pciedev, msgbuf_ring_t *ring, uint16 bytes_read)
{
	uint16 index_read;
	uint16 rd_idx;

	if (ring == NULL) {
		return;
	}

	index_read = bytes_read / MSGBUF_LEN(ring);
	ASSERT(index_read <= MSGBUF_MAX(ring));
	if (index_read > MSGBUF_MAX(ring)) {
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_ring_update_readptr);
	}

	if (index_read == 0)
		return;

	rd_idx = MSGBUF_RD(ring);

	/* Update the read pointer */
	if ((rd_idx + index_read) >= MSGBUF_MAX(ring))
		MSGBUF_RD(ring) = 0;
	else
		MSGBUF_RD(ring) = (rd_idx + index_read);

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* Sync read pointers for control post/rxpost rings */
	if (ring->dmaindex_d2h_supported && (pciedev->dmaindex_h2d_rd.inited)) {
		pciedev_sync_h2d_read_ptrs(pciedev, ring);
	}
#endif // endif
}

/** D2H direction: Update write ptr after d2h dma complete */
static void
pciedev_ring_update_writeptr(msgbuf_ring_t *ring, uint16 bytes_written)
{
	uint16 wrt_idx;
	uint16 index;
	struct dngl_bus *pciedev;

	if (ring == NULL) {
		PCI_TRACE(("pciedev_ring_update_writeptr: ring is NULL\n"));
		return;
	}

	index = bytes_written / MSGBUF_LEN(ring);
	ASSERT(index <= MSGBUF_MAX(ring));
	if (index > MSGBUF_MAX(ring)) {
		DBG_BUS_INC((struct dngl_bus*)ring->pciedev, pciedev_ring_update_writeptr);
	}

	if (index == 0)
		return;

	pciedev = ring->pciedev;
	wrt_idx = MSGBUF_WR(ring);

	/* Update Write pointer */
	if ((wrt_idx + index) >= MSGBUF_MAX(ring)) {
		MSGBUF_WR(ring) = 0;
	} else {
		MSGBUF_WR(ring) = (wrt_idx + index);
	}

#if defined(PCIE_DMA_INDEX)
#if defined(SBTOPCIE_INDICES)
	/* If DMAing r/w indices supported, defer doorbell until indices are DMAed */
	if (ring->dmaindex_d2h_supported && pciedev->dmaindex_d2h_wr.inited) {
			/* Sync up D2H write pointers with host */
			pciedev_sync_d2h_write_ptrs(pciedev, ring);
	}
#else
	if (ring->dmaindex_d2h_supported &&
		(pciedev->dmaindex_h2d_rd.inited || pciedev->dmaindex_d2h_wr.inited)) {
			pciedev_dmaindex_put(pciedev, ring);
	}
	else
#endif /* !SBTOPCIE_INDICES */
#endif /* PCIE_DMA_INDEX */
	{
#if defined(PCIE_D2H_DOORBELL_RINGER)
		uint32 ringer_index = ring->ringid - BCMPCIE_H2D_COMMON_MSGRINGS;
		d2h_doorbell_ringer_t *ringer = &pciedev->d2h_doorbell_ringer[ringer_index];
		ringer->db_fn(pciedev, ringer->db_info.value, ringer->db_info.haddr.low);
#else /* ! PCIE_D2H_DOORBELL_RINGER */
#ifdef BCMPCIE_D2H_MSI
		if (PCIE_MSI_ENAB(pciedev->d2h_msi_info)) {
			/**
			 * each message rings to use separate MSI vector offsets
			 * set by MSG_TYPE_D2H_RING_CONFIG
			 */
			pciedev_generate_msi_intr(pciedev, ring->msi_vector_offset);
		} else
#endif /* BCMPCIE_D2H_MSI */
		{
			pciedev_generate_host_db_intr(pciedev,
				PCIE_D2H_DB0_VAL, PCIE_DB_DEV2HOST_0);
		}
#endif /* ! PCIE_D2H_DOORBELL_RINGER */
	}
}

/** D2H direction: return ring ptr to put d2h messages. Update write pending pointers */
static uint32
pciedev_get_ring_space(struct dngl_bus *pciedev, msgbuf_ring_t *ring, uint16 msglen)
{
	uint16 wp_idx = ring->wr_pending;
	uint32 retaddr;
	uint16 index = msglen / MSGBUF_LEN(ring);

	uint16 avail_ring_entry = MSGBUF_CHECK_WRITE_SPACE(ring);

	ASSERT(index < MSGBUF_MAX(ring));

	if (avail_ring_entry < index) {
		PCI_ERROR(("ring name: In ring %c%c%c%c %d"
			"slots not available, cur avail space %d, msglen %d\n",
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3],
			index, avail_ring_entry, msglen));
		DBG_BUS_INC(pciedev, pciedev_get_ring_space);
		return NULL;
	}

	/* Return space */
	retaddr = MSGBUF_BASE(ring) + (wp_idx * MSGBUF_LEN(ring));

	/* Update write pending */
	if ((ring->wr_pending + index) >= MSGBUF_MAX(ring)) {
		ASSERT((ring->wr_pending + index) <= MSGBUF_MAX(ring));
		if ((ring->wr_pending + index) > MSGBUF_MAX(ring)) {
			DBG_BUS_INC(pciedev, pciedev_get_ring_space);
		}
		ring->wr_pending = 0;
	} else {
		ring->wr_pending += index;
	}

	ASSERT(ring->wr_pending < MSGBUF_MAX(ring));
	if (ring->wr_pending >= MSGBUF_MAX(ring)) {
		DBG_BUS_INC(pciedev, pciedev_get_ring_space);
	}

	return retaddr;
}

/*
 * Function	: pciedev_resource_avail_check
 * Input	: pciedev, msgbuf
 * Output	:
 *	 A) when "msgbuf" is not NULL,  Returns TRUE if all resources(below) are available
 *		1. Dma Descriptors available
 *		2. Local buffer is available in lcl/cbuf pool
 *		3. Entries are available in host ring
 *	B) When "msgbuf" is NULL , no other resources are checked except availability of
 *		dma descriptors. Returns TRUE if descriptors are available.
 */

static bool
pciedev_resource_avail_check(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf)
{
	uint32 min_txdesc_needed = (MIN_TXDESC_AVAIL + TXDESC_NEEDED);
	uint32 min_rxdesc_needed = (MIN_RXDESC_AVAIL + RXDESC_NEEDED);

	uint rxavail = PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, RXDESC);
	uint txavail = PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, TXDESC);

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* Check if there is space in D2H Complete ring */
	/* continuous write space available */
	if (MSGBUF_CHECK_NOWRITE_SPACE(msgbuf) && pciedev->dmaindex_d2h_rd.inited) {
		/* if no space sync up read pointers and check once more */
		pciedev_sync_d2h_read_ptrs(pciedev, msgbuf);
	}
#endif /* PCIE_DMA_INDEX */

	if ((txavail < min_txdesc_needed) || (rxavail < min_rxdesc_needed)) {
		/* not enough descriptors to queue request */
		DBG_BUS_INC(pciedev, resource_avail_check_no_dma_descriptor);
		return FALSE;
	}

	if (msgbuf == NULL) {
		return TRUE;
	}

	/* If message buffer is using circular pool */
	if (msgbuf->cbuf_pool != NULL) {
		if (!CIR_BUFPOOL_AVAILABLE(msgbuf->cbuf_pool)) {
			/* not enough resources to queue request */
			DBG_BUS_INC(pciedev, resource_avail_check_cbuf_buf_null);
			return FALSE;
		}
	} else if (msgbuf->buf_pool != NULL) { /* If message buffer is using lcl pool */
		if (!LCL_BUFPOOL_AVAILABLE(msgbuf)) {
			/* not enough resources to queue request */
			DBG_BUS_INC(pciedev, resource_avail_check_cbuf_buf_null);
			return FALSE;
		}
	} else {
		/**
		 * Both msgbuf->cbuf_pool and msgbuf->buf_pool shouldn't be NULL.
		 * Something seriously wrong!
		 */
		TRAP_IN_PCIEDRV(("%s: FATAL condition! cbuf_pool and buf_pool are NULL\n",
				__FUNCTION__));
	}

	if (MSGBUF_CHECK_NOWRITE_SPACE(msgbuf)) {
		/* no space in d2h_txcpl ring to queue  request */
		DBG_BUS_INC(pciedev, resource_avail_check_no_host_space);
		return FALSE;
	}

	return TRUE;
}

#ifdef PCIEDEV_FAST_DELETE_RING
static bool
pciedev_fastdelete_notxstatus(msgbuf_ring_t *flow_ring, uint16 rd_idx)
{
	if (flow_ring->delete_idx > MSGBUF_WR(flow_ring)) {
		if ((rd_idx >= flow_ring->delete_idx) ||
		    (rd_idx <= MSGBUF_WR(flow_ring))) {
			return TRUE;
		}
	} else {
		if ((rd_idx >= flow_ring->delete_idx) &&
		    (rd_idx <= MSGBUF_WR(flow_ring))) {
			return TRUE;
		}
	}
	return FALSE;
}
#endif /* PCIEDEV_FAST_DELETE_RING */

/**
 * Called when a 'dongle->host' message is drained from a firmware internal queue, with the purpose
 * of sending it to the host. This function schedules a transfer.
 */
static bool
pciedev_check_process_d2h_message(struct dngl_bus *pciedev, void *p, d2h_msg_handler *msg_handler)
{
	cmn_msg_hdr_t *msg;

#ifdef BCM_DHDHDR
	/* When BCM_DHDHDR is defined for txfrag packet we save the txstatus in LFRAG_META
	 * we don't leverage the PKTDATA to carry the cmn_msg_hdr_t because we may not have PKTDATA.
	 * for example the second msdu which has free the D3 buffer.
	 * Here if the p is txfrag means it wants to do txmetadata/txstatus
	 */
	if (PKTISTXFRAG(pciedev->osh, p)) {
		/* Check if there are resources to queue txstatus */
		if (!PCIEDEV_TXCPL_RESOURCE_AVAIL_CHECK(pciedev)) {
			return FALSE;
		}

		*msg_handler = pciedev_process_d2h_txmetadata;

		return TRUE;
	}
#endif /* BCM_DHDHDR */

	msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);

	PCI_TRACE(("pciedev_check_process_d2h_message: MSG_TYPE=%d\n", msg->msg_type));

	/**
	 * - Check if dma resources are available to send payload
	 * - Check if dma resources are available to dma msgbuf
	 * - Check if local buffer slot is available
	 */
	switch (msg->msg_type) {
		case MSG_TYPE_WL_EVENT: /* notify host of dongle event */
			if (!pciedev_resource_avail_check(pciedev, pciedev->dtoh_ctrlcpl)) {
				return FALSE;
			}
			/**
			 * XXX olympic wants us to make sure we don't drop events
			 * because we don't have host buffers
			 * may be we should have some escape logic to come out of it,
			 * if we have too many events pending and host not posting buffers
			 */
			if (HOST_DMA_BUF_POOL_EMPTY(pciedev->event_pool)) {
				PCI_TRACE(("not sending it because no host buffer avail\n"));

				pciedev->event_delivery_pend = TRUE;
				return FALSE;
			}
			pciedev->event_delivery_pend = FALSE;

			*msg_handler = pciedev_process_d2h_wlevent;
			break;
		case MSG_TYPE_FIRMWARE_TIMESTAMP:
			if (!pciedev->timesync) {
				PCI_ERROR(("not sending FW Timestamp, "
					"host does not support timesync\n"));
				return FALSE;
			}
			if (!pciedev_resource_avail_check(pciedev, pciedev->dtoh_ctrlcpl)) {
				return FALSE;
			}
			/*
			 * XXX olympic wants us to make sure we don't drop events
			 * because we don't have host buffers
			 * may be we should have some escape logic to come out of it,
			 * if we have too many events pending and host not posting buffers
			 */
			if (HOST_DMA_BUF_POOL_EMPTY(pciedev->ts_pool)) {
				PCI_TRACE(("not sending it because no host buffer avail\n"));
				pciedev->tsinfo_delivery_pend = TRUE;
				return FALSE;
			}
			pciedev->tsinfo_delivery_pend = FALSE;

			*msg_handler = pciedev_process_d2h_tsinfo;
			break;

		case MSG_TYPE_TXMETADATA_PYLD: /* notify host of transmit status */
			/* Check if there are resources to queue txstatus */
			if (!PCIEDEV_TXCPL_RESOURCE_AVAIL_CHECK(pciedev)) {
				return FALSE;
			}
#ifdef BCM_DHDHDR
			/* Should not be here when BCM_DHDHDR is enabled */
			ASSERT(0);
#else
			/* skip common message header before pciedev_process_d2h_txmetadata */
			PKTPULL(pciedev->osh, p, sizeof(cmn_msg_hdr_t));
#endif // endif
			*msg_handler = pciedev_process_d2h_txmetadata;
			break;

		case MSG_TYPE_RX_PYLD:
			/* Check if there are resources to queue rxcpl */
			if (!PCIEDEV_RXCPL_RESOURCE_AVAIL_CHECK(pciedev)) {
				return FALSE;
			}

			/* Pre-request a reserved RPH from HWA before we do msg_handler */
#if defined(BCMHWA) && defined(HWA_RXPOST_BUILD)
			if ((!PKTISHDRCONVTD(pciedev->osh, p) ||
				!PKTISRXFRAG(pciedev->osh, p)) &&
				(hwa_rph_allocate(NULL, NULL, NULL, TRUE) == NULL)) {
				return FALSE;
			}
#endif // endif
			*msg_handler = pciedev_process_d2h_rxpyld;
			break;

#ifdef PCIE_DMAXFER_LOOPBACK
		case MSG_TYPE_LPBK_DMAXFER_PYLD:
			/* passing ring as NULL to check descriptor availability only */
			if (!pciedev_resource_avail_check(pciedev, NULL)) {
				return FALSE;
			}
			*msg_handler = pciedev_process_d2h_dmaxfer_pyld;
			break;
#endif // endif
		default:
			PCI_ERROR(("unknown message on D2H Rxq %d\n", msg->msg_type));
			PKTFREE(pciedev->osh, p, TRUE);
			*msg_handler = NULL;
			return TRUE;
	}

	return TRUE;
} /* pciedev_check_process_d2h_message */

/**
 * Called when data received on the wireless medium has completed DMA into host memory, so host
 * needs to be notified of that event by sending it an 'rx complete' message.
 */
static int
pciedev_process_d2h_rxpyld(struct dngl_bus *pciedev, void *p)
{
	uint8 ifidx = 0;
	uint16 pktlen_new;
	uint16 pktlen;
	uint32 bufid = 0;
	uint16 dataoffset = 0;
	dma64addr_t haddr = { .loaddr = 0, .hiaddr = 0 };
	uint16 len = 0;
	uint16 rxcpl_id;
	rxcpl_info_t *p_rxcpl_info = NULL;
	bool	queue_rxcpl;
	bool	dot11 = TRUE;
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
	uint8 *pval;
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */

#ifdef TEST_DROP_PCIEDEV_RX_FRAMES
	pciedev_test_drop_rxframe++;
	if (!PKTNEEDRXCPL(pciedev->osh, p)) {
		pciedev_test_drop_norxcpl++;
		if (pciedev_test_drop_norxcpl == pciedev_test_drop_norxcpl_max) {
			pciedev_test_drop_norxcpl = 0;
			pciedev_test_dropped_norxcpls++;
			PKTFREE(pciedev->osh, p, TRUE);
			return 0;
		}
	} else if (pciedev_test_drop_rxframe > pciedev_test_drop_rxframe_max) {
		pciedev_test_drop_rxframe = 0;
		pciedev_test_dropped_rxframes++;
		PKTFREE(pciedev->osh, p, TRUE);
		DBG_BUS_INC(pciedev, pciedev_process_d2h_rxpyld);
		return 0;
	}
#endif /* TEST_DROP_PCIEDEV_RX_FRAMES */

	ifidx = PKTIFINDEX(pciedev->osh, p);
	/* remove cmn_msg_hdr_t added from proto_push */
	PKTPULL(pciedev->osh, p, sizeof(cmn_msg_hdr_t));

	/* pull the metadata from the packet */
	/* no rx meta data support */
	if (PKTDATAOFFSET(p)) {
		PKTPULL(pciedev->osh, p, PKTDATAOFFSET(p) * 4);
	}

#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
	if (pciedev->monitor_mode) {
		PKTPUSH(pciedev->osh, p, 4);
		pval = (uint8*)PKTDATA(pciedev->osh, p);
		pval[2] = pciedev->d11rxoffset;
	}
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */

	/* need to look for metadata if present */
	/* if metadata then setup the right length where the info needs to be sent to */
	if (!PKTISHDRCONVTD(pciedev->osh, p) ||!(PKTISRXFRAG(pciedev->osh, p))) {
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
		if (RXFIFO_SPLIT() && PKTISRXFRAG(pciedev->osh, p) && PKTMON(p)) {
			dataoffset = 0;
			pktlen = PKTFRAGUSEDLEN(pciedev->osh, p) + pciedev->d11rxoffset;
			bufid = PKTFRAGPKTID(pciedev->osh, p);
			rxcpl_id = PKTRXCPLID(pciedev->osh, p);
			PKTRESETRXCPLID(pciedev->osh, p);
			p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);
			if (p_rxcpl_info == NULL) {
				PCI_ERROR(("rxcpl_id is %x, and rxcpl_info is NULL, lb is %x\n",
					rxcpl_id, *(int*) p));
				ASSERT(p_rxcpl_info);
				PKTFREE(pciedev->osh, p, TRUE);
				return 0;
			}
			queue_rxcpl = PKTNEEDRXCPL(pciedev->osh, p);

			dot11 = (PKT80211(p)) ? 1 : 0;

			pciedev->pkt_noise = ((int8*)PKTTAG(p))[MON_PKTTAG_NOISE_IDX];
			pciedev->pkt_rssi = ((int8*)PKTTAG(p))[MON_PKTTAG_RSSI_IDX];

			PKTRESETRXFRAG(pciedev->osh, p);
			PKTFREE(pciedev->osh, p, FALSE);
			/* crash any invalid refs after FREE */
			p = NULL;
		} else
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */
		{
			pktlen = PKTLEN(pciedev->osh, p);
			if (PKTISRXFRAG(pciedev->osh, p)) {
				pktlen += PKTFRAGUSEDLEN(pciedev->osh, p);
				haddr = pciedev_get_haddr_from_lfrag(pciedev, p, &bufid,
					&dataoffset);
			} else { /* PKTISHDRCONVTD() is FALSE */
				/* Account for the data offset coming from pcie dma here */
				dataoffset = 0;

				/* host addr not valid, so obtain the host adress from an earlier
				 * rxbuf post message from the host
				 */
				pciedev_process_rxpost_msg(pciedev, &bufid, &len, &haddr);

				/* check this out */
				rxcpl_id = PKTRXCPLID(pciedev->osh, p);
				if ((haddr.loaddr != NULL) && (rxcpl_id == 0)) {
					p_rxcpl_info = bcm_alloc_rxcplinfo();
					if (p_rxcpl_info == NULL) {
						PCI_ERROR(("HOST RX BUF: RXCPL ID not free\n"));
						/* try to send an error to host */
						PKTFREE(pciedev->osh, p, TRUE);
						ASSERT(p_rxcpl_info);
						return 0;
					}
					PKTSETRXCPLID(pciedev->osh, p, p_rxcpl_info->rxcpl_id.idx);
				}
			}
			if (haddr.loaddr == NULL) {
				PCI_ERROR(("HOST RX BUF: ret buf not available \n"));
				/* try to send an error to host */
				PKTFREE(pciedev->osh, p, TRUE);
				return 0;
			}
			rxcpl_id = PKTRXCPLID(pciedev->osh, p);
			PKTRESETRXCPLID(pciedev->osh, p);
			p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);
			if (p_rxcpl_info == NULL) {
				DBG_BUS_INC(pciedev, pciedev_process_d2h_rxpyld);
				PCI_ERROR(("rxcpl_id is %d, and rxcpl_info is NULL, lb is %x\n",
					rxcpl_id, (uint32)p));
				ASSERT(p_rxcpl_info);
				PKTFREE(pciedev->osh, p, TRUE);
				return 0;
			}
			queue_rxcpl = PKTNEEDRXCPL(pciedev->osh, p);
			PCI_TRACE(("PKTDATAOFFSET is %d, pktlen %d, pktlen_new %d,"
				" ifdx %d\n", PKTDATAOFFSET(p), pktlen,
				PKTLEN(pciedev->osh, p), ifidx));

			pktlen_new = PKTLEN(pciedev->osh, p);
			/* pktlen could change due to pad bytes in return_haddr_pool */
			PCI_TRACE(("PKTDATAOFFSET is %d, metadata_len is %d, pktlen %d, "
				"pktlen_new %d, "
				"ifdx %d\n", PKTDATAOFFSET(p), 0, pktlen,
				PKTLEN(pciedev->osh, p), ifidx));

			/* pciedev_tx_pyld starts pumping the message to the host */
			if (!(pciedev_tx_pyld(pciedev, p, (ret_buf_t *)&haddr,
				pktlen_new, MSG_TYPE_RX_PYLD)))
			{
				DBG_BUS_INC(pciedev, pciedev_process_d2h_rxpyld);
				PKTFREE(pciedev->osh, p, TRUE);
				PCI_ERROR(("pciedev_process_d2h_rxpyld: BAD ERROR: "
					"shouldn't happen, pciedev_tx_pyld shouldn't fail\n"));
				ASSERT(0);
			}
			dot11 = (PKT80211(p)) ? 1 : 0;
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
			if (PKTMON(p)) {
				pciedev->pkt_noise = ((int8*)PKTTAG(p))[MON_PKTTAG_NOISE_IDX];
				pciedev->pkt_rssi = ((int8*)PKTTAG(p))[MON_PKTTAG_RSSI_IDX];
			}
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */
		}
	} else { /* header converted  */
		/* remove HW rxstats of 4bytes and padding of 2 bytes */
		dataoffset = pciedev->d11rxoffset + HW_HDR_CONV_PAD;
		pktlen = PKTFRAGUSEDLEN(pciedev->osh, p);
		bufid = PKTFRAGPKTID(pciedev->osh, p);
		rxcpl_id = PKTRXCPLID(pciedev->osh, p);
		PKTRESETRXCPLID(pciedev->osh, p);
		p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);

		if (p_rxcpl_info == NULL) {
			DBG_BUS_INC(pciedev, pciedev_process_d2h_rxpyld);
			PCI_ERROR(("rxcpl_id is %x, and rxcpl_info is NULL, lb is %x\n",
				rxcpl_id, *(int*) p));
			ASSERT(p_rxcpl_info);
			PKTFREE(pciedev->osh, p, TRUE);
			return 0;
		}

		queue_rxcpl = PKTNEEDRXCPL(pciedev->osh, p);
		dot11 = (PKT80211(p)) ? 1 : 0;
		/* HDRCONV path we are done ! So, free up the packet */
		PKTRESETRXFRAG(pciedev->osh, p);
		PKTRESETHDRCONVTD(pciedev->osh, p);
		PKTFREE(pciedev->osh, p, FALSE);
		/* crash any invalid refs after FREE */
		p = NULL;
	}

	BCM_RXCPL_CLR_IN_TRANSIT(p_rxcpl_info);
	BCM_RXCPL_SET_VALID_INFO(p_rxcpl_info);

	p_rxcpl_info->host_pktref = bufid;
	p_rxcpl_info->rxcpl_len.metadata_len_w = 0;
	p_rxcpl_info->rxcpl_len.dataoffset = dataoffset;
	p_rxcpl_info->rxcpl_len.datalen =  pktlen;
	p_rxcpl_info->rxcpl_id.ifidx = (ifidx & BCM_MAX_RXCPL_IFIDX);
	p_rxcpl_info->rxcpl_id.dot11 = dot11 ? 1 : 0;
	PCI_TRACE(("bufid: 0x%04x, datalen %d, offset %d\n", p_rxcpl_info->host_pktref,
		p_rxcpl_info->rxcpl_len.datalen, p_rxcpl_info->rxcpl_len.dataoffset));

	/* Transfer RX complete message with orig len */
	/* Host shouldn't see the pad bytes, so data offset should cover pad too */
	if (queue_rxcpl || BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info)) {
		pciedev_queue_rxcomplete_local(pciedev, p_rxcpl_info, pciedev->dtoh_rxcpl,
			BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info));
	}

	return 0;
} /* pciedev_process_d2h_rxpyld */

/**
 * When transmit packets have been processed by firmware (eg because they have been transmitted
 * to a wireless remote party, or because they have been suppressed by a channel switch), the host
 * may have to be notified, and suppressed packets may have to be refetched from the host for
 * retransmission.
 *
 * @param pciedev
 * @param p         Buffer containing txstatus of *one* packet
 */
static int
pciedev_process_d2h_txmetadata(struct dngl_bus *pciedev, void *p)
{
	uint16 txstatus = 0;
	uint8 ifindx;
	uint32 pktid;
	uint16 ringid;
	msgbuf_ring_t *flow_ring;
#ifdef PCIEDEV_FAST_DELETE_RING
	uint16 rd_idx;
#endif /* PCIEDEV_FAST_DELETE_RING */

	ipc_timestamp_t ts;

	/* assert that the packet has metadata on it */
	/* assert that the packet has big enough metadata len to carry it */
	ASSERT(PKTISTXFRAG(pciedev->osh, p));
	ASSERT(PKTHASMETADATA(pciedev->osh, p));

	/* When BCM_DHDHDR is defined for txfrag packet we save the txstatus in LFRAG_META,
	 * we don't leverage the PKTDATA to carry the txstatus because we may not have
	 * PKTDATA. For example the second msdu which has free the D3 buffer.
	 * Here use the saved txstatus from LFRAG_META.
	 */
	/* Now no matter DHDHDR is enable or not we can get txstatus from generic txstatus field */
	txstatus = PKTFRAGTXSTATUS(pciedev->osh, p);

	PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *)p);
	ifindx = PKTIFINDEX(pciedev->osh, p);
	pktid = PKTFRAGPKTID(pciedev->osh, p);
	ringid = PKTFRAGFLOWRINGID(pciedev->osh, p);
#ifdef PCIEDEV_FAST_DELETE_RING
	rd_idx = PKTFRAGRINGINDEX(pciedev->osh, p);
#endif /* PCIEDEV_FAST_DELETE_RING */

	if (PKTISTXTSINSERTED(pciedev->osh, p)) {
		ts.ts_low = PKTFRAGDATA_LO(pciedev->osh, p, 1);
		ts.ts_high = PKTFRAGDATA_HI(pciedev->osh, p, 1);
		PKTRESETTXTSINSERTED(pciedev->osh, p);
	}

	/* no need for the callback now */
	PKTFREE(pciedev->osh, p, FALSE);

	/* Transfer RX complete message with orig len */
	/* Host shouldn't see the pad bytes. so data offset should cover pad too */
	PCI_TRACE(("generating the txstatus for pktid 0x%04x, ringid %d, ifindx %d\n",
		pktid, ringid, ifindx));
	flow_ring = pciedev->h2d_submitring_ptr[ringid];

	/* Flowring might have been freed already from h2d path */
	if (!flow_ring) {
		PCI_ERROR(("Flow ring [%d] has been freed already from h2d path\n", ringid));
		return 0;
	}

	ASSERT(flow_ring->d2h_q_txs_pending != 0);
	flow_ring->d2h_q_txs_pending--;

#ifdef PCIEDEV_FAST_DELETE_RING
	if ((flow_ring->status & FLOW_RING_DELETE_RESP_PENDING) &&
	    (pciedev->fastdeletering)) {
		/* Check if the rd_idx of the packet is "after" ring delete_idx */
		if (pciedev_fastdelete_notxstatus(flow_ring, rd_idx)) {
			goto process_possible_flush;
		}
	}
#endif /* PCIEDEV_FAST_DELETE_RING */
	PCIEDEV_QUEUE_TXSTATUS(pciedev, pktid, ifindx, ringid, txstatus, &ts);

#ifdef PCIEDEV_FAST_DELETE_RING
process_possible_flush:
#endif /* PCIEDEV_FAST_DELETE_RING */
	if (flow_ring->status & FLOW_RING_FLUSH_PENDING)
		pciedev_process_pending_flring_resp(pciedev, flow_ring);
	return 0;
} /* pciedev_process_d2h_txmetadata */

/**
 * A queue is used to keep track of what is pending on the PCIe DMA engine(s), so that when the DMA
 * engine signals completion, the properties of the completed item can be retrieved.
 */
void
pciedev_enqueue_dma_info(dma_queue_t *dmaq, uint16 msglen, uint8 msgtype,
	uint8 flags, msgbuf_ring_t *ring, uint32 addr, uint32 haddr)
{
	dma_item_info_t *item;

	/* Check if space is available to queue dma info */
	if (NEXTTXP(dmaq->w_index, dmaq->max_len) == dmaq->r_index) {
		PCI_ERROR(("pciedev_enqueue_dma_info: ERROR: No space available in dmaq\n"));
		ASSERT(0);
		return;
	}

	/* get available slot of item from dmaq at w_index */
	item = &dmaq->dma_info[dmaq->w_index];
	dmaq->w_index = NEXTTXP(dmaq->w_index, dmaq->max_len);

	/* queue up the dma info into the item */
	item->len = msglen;
	item->flags = flags;
	item->msg_type = msgtype;
	item->ring = ring;
	item->addr = addr;
	item->haddr = haddr;

	/* Log the timestamp */
	PCIEDEV_TS_LOGGING(item);
}

/**
 * A queue is used to keep track of what is pending on the PCIe DMA engine(s), so that when the DMA
 * engine signals completion, the properties of the completed item can be retrieved.
 */
dma_item_info_t*
pciedev_dequeue_dma_info(dma_queue_t *dmaq)
{
	dma_item_info_t *item;

	/* check if anything is queued to dequeue */
	if (dmaq->r_index == dmaq->w_index) {
		PCI_TRACE(("pciedev_dequeue_dma_info: nothing to dequeue\n"));
		return NULL;
	}

	/* retrieve the item at r_index in the queue */
	item = &dmaq->dma_info[dmaq->r_index];
	dmaq->r_index = NEXTTXP(dmaq->r_index, dmaq->max_len);

	/* return the item */
	return item;
}

/**
 * Called when host needs to be notified of received payload available in host memory. Because of
 * AMPDU reordering, the sequence in which rxcpl messages are send to the host is important.
 */
void
pciedev_queue_rxcomplete_local(struct dngl_bus *pciedev, rxcpl_info_t *p_rxcpl_info,
	msgbuf_ring_t *ring, bool check_flush)
{
	uint32 count = 0;
	uint16 next_idx = 0;

	if (p_rxcpl_info == NULL) {
		PCI_ERROR(("Rxcomplete queue with bogus rxcplID\n"));
		ASSERT(0);
		DBG_BUS_INC(pciedev, pciedev_queue_rxcomplete_local);
	}

	while (p_rxcpl_info != NULL) {
		if (BCM_RXCPL_IN_TRANSIT(p_rxcpl_info)) {
			BCM_RXCPL_SET_FRST_IN_FLUSH(p_rxcpl_info);
			if (pciedev->rxcpl_list_t)
				pciedev->rxcpl_list_t->rxcpl_id.next_idx = 0;
			p_rxcpl_info = NULL;
			continue;
		}
		next_idx = p_rxcpl_info->rxcpl_id.next_idx;
		if (BCM_RXCPL_VALID_INFO(p_rxcpl_info)) {
			if (pciedev->rxcpl_list_h == NULL) {
				pciedev->rxcpl_list_h = p_rxcpl_info;
				pciedev->rxcpl_list_t = p_rxcpl_info;
			} else {
				pciedev->rxcpl_list_t->rxcpl_id.next_idx =
					p_rxcpl_info->rxcpl_id.idx;
				pciedev->rxcpl_list_t = p_rxcpl_info;
			}
			count++;
		} else {
			/* Should we free the rxcpl if not valid ? */
			bcm_free_rxcplinfo(p_rxcpl_info);
		}
		p_rxcpl_info = bcm_id2rxcplinfo(next_idx);
	}

	pciedev->rxcpl_pend_cnt += count;

	/* need to check if this need to be queued or not */
	pciedev_queue_rxcomplete_msgring(pciedev, ring);
} /* pciedev_queue_rxcomplete_local */

#if defined(BCMHWA) && defined(HWA_RXCPLE_BUILD)
static void
pciedev_queue_rxcomplete_msgring(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
	uint16 flags;
	uint32 pktid;
	rxcpl_info_t *rxcpl_info;

	while (pciedev->rxcpl_list_h != NULL) {
		rxcpl_info = pciedev->rxcpl_list_h;

		pktid = rxcpl_info->host_pktref;
		flags = (!rxcpl_info->rxcpl_id.dot11) ?
			BCMPCIE_PKT_FLAGS_FRAME_802_3 : // 0x1 = bit 0
			BCMPCIE_PKT_FLAGS_FRAME_802_11; // 0x2 = bit 1

		if (hwa_rxcple_wi_add(hwa_dev, rxcpl_info->rxcpl_id.ifidx, flags,
			rxcpl_info->rxcpl_len.dataoffset, rxcpl_info->rxcpl_len.datalen, pktid)) {
			PCI_INFORM(("Queue_rxcomplete: alloc wi space failed for HWA2b RxCPLE\n"));
			DBG_BUS_INC(pciedev, pciedev_queue_rxcomplete_msgring);
			return;
		}

		BCMPCIE_IPC_HPA_TEST(pciedev, pktid,
			BCMPCIE_IPC_PATH_RECEIVE, BCMPCIE_IPC_TRANS_RESPONSE);
		BUZZZ_KPI_PKT1(KPI_PKT_BUS_RXCMPL, 1, pktid);

		if (rxcpl_info == pciedev->rxcpl_list_t)
			pciedev->rxcpl_list_h = pciedev->rxcpl_list_t = NULL;
		else
			pciedev->rxcpl_list_h = bcm_id2rxcplinfo(rxcpl_info->rxcpl_id.next_idx);

		pciedev->rxcpl_pend_cnt--;

		bcm_free_rxcplinfo(rxcpl_info);
	} /* while loop */
}
#else /* !(BCMHWA && HWA_RXCPLE_BUILD) */
static void
pciedev_queue_rxcomplete_msgring(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
	void * wi;
	uint16 flags;
	uint32 pktid;
	lcl_buf_pool_t *pool = ring->buf_pool;
	uint32 avail_ring_entry;
	rxcpl_info_t *rxcpl_info;
	uint32 avail_host_ring_entry = MSGBUF_CHECK_WRITE_SPACE(ring);

	if (IS_RXCPL_PENDING(pciedev)|| !avail_host_ring_entry ||
		pciedev_ds_in_host_sleep(pciedev)) {
		DBG_BUS_INC(pciedev, pciedev_queue_rxcomplete_msgring);
#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES) && defined(BCMHWA) && \
	defined(HWA_RXPOST_BUILD)
		if (!avail_host_ring_entry && pciedev->dmaindex_d2h_rd.inited)
			pciedev_sync_d2h_read_ptrs(pciedev, ring);
#endif // endif
		return;
	}

	while (pciedev->rxcpl_list_h != NULL) {
		if (pool->local_buf_in_use == NULL) {
			pool->local_buf_in_use = pciedev_lclpool_alloc_lclbuf(ring);
			if (pool->local_buf_in_use == NULL) {
				DBG_BUS_INC(pciedev, pciedev_queue_rxcomplete_msgring);
				PCI_ERROR(("Queue_rxcomplete: all local buffers in Use\n"));
				return;
			}
			pool->pend_item_cnt = 0;
		}
		ASSERT(pool->pend_item_cnt < pool->item_cnt);
		if (pool->pend_item_cnt >= pool->item_cnt) {
			DBG_BUS_INC(pciedev, pciedev_queue_rxcomplete_msgring);
			PCI_ERROR(("ERROR: rxcpl count overflowing locl buf, %d, %d..\n",
				pool->pend_item_cnt, pool->item_cnt));
			ASSERT(0);
		}

		wi = (void *)(pool->local_buf_in_use +
			(MSGBUF_LEN(ring) * pool->pend_item_cnt));

		rxcpl_info = pciedev->rxcpl_list_h;
		if (rxcpl_info == pciedev->rxcpl_list_t)
			pciedev->rxcpl_list_h = pciedev->rxcpl_list_t = NULL;
		else
			pciedev->rxcpl_list_h = bcm_id2rxcplinfo(rxcpl_info->rxcpl_id.next_idx);

		pciedev->rxcpl_pend_cnt--;
		pool->pend_item_cnt++;

		pktid = rxcpl_info->host_pktref;

		BCMPCIE_IPC_HPA_TEST(pciedev, pktid,
			BCMPCIE_IPC_PATH_RECEIVE, BCMPCIE_IPC_TRANS_RESPONSE);
		BUZZZ_KPI_PKT1(KPI_PKT_BUS_RXCMPL, 1, pktid);

		flags = (!rxcpl_info->rxcpl_id.dot11) ?
			BCMPCIE_PKT_FLAGS_FRAME_802_3 : // 0x1 = bit 0
			BCMPCIE_PKT_FLAGS_FRAME_802_11; // 0x2 = bit 1

		switch (MSGBUF_ITEM_TYPE(ring))
		{
			case MSGBUF_WI_WI64:
			{
				host_rxbuf_cmpl_t *wi64 = (host_rxbuf_cmpl_t *)wi;
				wi64->cmn_hdr.msg_type = MSG_TYPE_RX_CMPLT;
				wi64->rx_status_0 = 0;
				wi64->rx_status_1 = 0;
				wi64->compl_hdr.status = 0;
				wi64->compl_hdr.flow_ring_id = 0;
				wi64->flags = htol16(flags);
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
				if (rxcpl_info->rxcpl_id.dot11) {
					int8 *pval = (int8*)&wi64->rx_status_0;
					pval[0] = pciedev->pkt_noise;
					pval[1] = pciedev->pkt_rssi;
					pval[2] = pciedev->d11rxoffset;
				}
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */
				wi64->cmn_hdr.flags =
					(wi64->cmn_hdr.flags & (~MSGBUF_RING_INIT_PHASE)) |
					(ring->current_phase & MSGBUF_RING_INIT_PHASE);
				wi64->cmn_hdr.if_id = rxcpl_info->rxcpl_id.ifidx;
				wi64->cmn_hdr.request_id = htol32(pktid);
				wi64->data_offset = htol16(rxcpl_info->rxcpl_len.dataoffset);
				wi64->data_len = htol16(rxcpl_info->rxcpl_len.datalen);
				wi64->metadata_len =
					htol16((rxcpl_info->rxcpl_len.metadata_len_w << 2));
#if defined(PCIE_M2M_D2H_SYNC)
				PCIE_M2M_D2H_SYNC_MARKER_INSERT(wi64, MSGBUF_LEN(ring),
					pciedev->rxbuf_cmpl_epoch);
#endif /* PCIE_M2M_D2H_SYNC */
				break;
			}

			case MSGBUF_WI_CWI:
			{
				hwa_rxcple_cwi_t *cwi = (hwa_rxcple_cwi_t *)wi;
				cwi->u8[0] = (uint8)
					BCM_SBF(rxcpl_info->rxcpl_id.ifidx, HWA_RXCPLE_IFID) | // 5b
					BCM_SBF(flags, HWA_RXCPLE_FLAGS); // 3b
				cwi->host_pktid = pktid;
				// FIXME TBD
				// When .11 to .3 is in MAC, ASSERT rxcpl_info dataoffset
				// against the value in pcie_ipc_rings_t::rxcpln_dataoffset
				ASSERT(rxcpl_info->rxcpl_len.dataoffset <= 0xFF);
				cwi->data_offset = rxcpl_info->rxcpl_len.dataoffset; // 8b
				// Host may ASSERT cwi::data_len against the length contained
				// in the RxStatus of .3 packet DMAed to host.
				cwi->data_len = rxcpl_info->rxcpl_len.datalen;
				break;
			}

			default:
			{
				PCI_ERROR(("Invalid RxCpl itemtype %u\n", MSGBUF_ITEM_TYPE(ring)));
				ASSERT(0);
			}

		} /* switch MSGBUF_ITEM_TYPE */

		bcm_free_rxcplinfo(rxcpl_info);

		ASSERT(pool->pend_item_cnt <= avail_host_ring_entry);

		avail_ring_entry = MIN(pool->item_cnt, avail_host_ring_entry);

		if (pool->pend_item_cnt >= avail_ring_entry) {
			ASSERT(pool->pend_item_cnt <= avail_ring_entry);
			PCIEDEV_XMIT_RXCOMPLETE(pciedev);
			/* pciedev_xmit_rxcomplte updates wr_pending, so update host ring entry */
			avail_host_ring_entry = MSGBUF_CHECK_WRITE_SPACE(ring);
			if (IS_RXCPL_PENDING(pciedev) || !avail_host_ring_entry)
				break;
		}
	} /* while loop */
} /* pciedev_queue_rxcomplete_msgring */
#endif /* !(BCMHWA && HWA_RXCPLE_BUILD) */

/** D2H transfers for message packets */
uint8
pciedev_xmit_msgbuf_packet(struct dngl_bus *pciedev, void *p,
	uint8 msg_type, uint16 msglen, msgbuf_ring_t *ring)
{
	ret_buf_t ret_buf = { .low_addr = 0, .high_addr = 0};
	uint8 current_phase = ring->current_phase;

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* Update host read pointer before trying to post */
	if (pciedev->dmaindex_d2h_rd.inited) {
		pciedev_sync_d2h_read_ptrs(pciedev, ring);
	}
#endif // endif

	/* get ring space from d2h ring */
	ret_buf.low_addr = (uint32)pciedev_get_ring_space(pciedev, ring, msglen);

	if (ret_buf.low_addr == NULL) {
		DBG_BUS_INC(pciedev, pciedev_xmit_msgbuf_packet);
		PCI_ERROR(("ring name: DtoH%c pciedev_xmit_msgbuf_packet:"
			"DTOH ring not available \n", ring->name[3]));
		return FALSE;
	}
	ret_buf.high_addr = HADDR64_HI(MSGBUF_HADDR64(ring));

	/* Only D2H rings supporting Legacy message formats have a cmn_msg_hdr_t */
	if (MSGBUF_ITEM_TYPE(ring) == MSGBUF_WI_LEGACY)
	{
		cmn_msg_hdr_t *msg;

		/* Inject the phase bit into flags in cmn_msg_hdr_t
		 * queued MSG_TYPE_RX_CMPLT and MSG_TYPE_TX_STATUS
		 * have the phase bit set already
		 */
		msg = (cmn_msg_hdr_t *)p;
		msg->flags = (msg->flags & (~MSGBUF_RING_INIT_PHASE)) |
			(current_phase & MSGBUF_RING_INIT_PHASE);

#if defined(PCIE_M2M_D2H_SYNC)
		if (msg_type == MSG_TYPE_TX_STATUS) {
			PCIE_M2M_D2H_SYNC_MARKER_REPLACE((host_txbuf_cmpl_t *)p,
				MSGBUF_LEN(ring));
		} else if (msg_type == MSG_TYPE_RX_CMPLT) {
			PCIE_M2M_D2H_SYNC_MARKER_REPLACE((host_rxbuf_cmpl_t *)p,
				MSGBUF_LEN(ring));
		} else if (msg_type == MSG_TYPE_INFO_BUF_CMPLT) {
			PCIE_M2M_D2H_SYNC_MARKER_INSERT((info_buf_resp_t *)p,
				MSGBUF_LEN(ring), pciedev->info_buf_compl_epoch);
		} else {
			PCIE_M2M_D2H_SYNC_MARKER_INSERT((ctrl_compl_msg_t *)p,
				MSGBUF_LEN(ring), pciedev->ctrl_compl_epoch);
		}
#endif /* PCIE_M2M_D2H_SYNC */
	}

	pciedev_tx_msgbuf(pciedev, (void *) p, &ret_buf, msg_type, msglen, ring);

	/*
	 * Update IPC Data
	 * queued rx_cmplt and tx_status are already counted
	 * through pciedev.c
	 */
	if ((msg_type != MSG_TYPE_TX_STATUS) && (msg_type != MSG_TYPE_RX_CMPLT))
		pciedev->metrics->num_completions++;

	/* Change the phase bit here... */
	if (ring->wr_pending == 0) {
		ring->current_phase = (~current_phase) & MSGBUF_RING_INIT_PHASE;
		PCI_TRACE(("ring name:  DtoH flipping the phase from 0x%02x to 0x%02x\n",
			current_phase, ring->current_phase));
	}

	return TRUE;
} /* pciedev_xmit_msgbuf_packet */

int
pciedev_create_d2h_messages_tx(struct dngl_bus *pciedev, void *p)
{
#ifdef PCIE_PWRMGMT_CHECK
	if (pciedev_ds_in_host_sleep(pciedev) && (pciedev->no_device_inited_d3_exit)) {
		PKTFREE(pciedev->osh, p, TRUE);
		DBG_BUS_INC(pciedev, pciedev_process_d2h_rxpyld);
		return 0;
	}
#endif /* PCIE_PWRMGMT_CHECK */

	/* XXX: Memory leak due to logtrace repeated pkt allocation
	 * XXX: while Host is not ready
	 * XXX: After FW download and no ifconfig up
	 */
	if (pciedev->common_rings_attached == FALSE) {
		PCI_TRACE(("pciedev_create_d2h_messages_tx:Host is not ready\n"));
		pciedev->dropped_txpkts++;
		PKTFREE(pciedev->osh, p, TRUE);
		return 0;
	}

	return (pciedev_create_d2h_messages(pciedev, p, pciedev->dtoh_rxcpl));
}

/**
 * Prepare the Messages for D2H message transfers
 * Add header info according to message type and trigger actual dma
 */
int
pciedev_create_d2h_messages(struct dngl_bus *bus, void *p, msgbuf_ring_t *unused)
{
	int ret = TRUE;
	uint16  msglen;
	uint8 msgtype, prio = D2H_REQ_PRIO_0;
	void * pkt;
	struct dngl_bus *pciedev = (struct dngl_bus *)bus;
	cmn_msg_hdr_t * msg;
	bool schedule_q = TRUE;

#ifdef PCIE_PWRMGMT_CHECK
	/* check to do host_wake if in real D3 */
	if (pciedev->real_d3) {
		pciedev_check_host_wake(pciedev, p);
	}
#endif /* PCIE_PWRMGMT_CHECK */

	msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);
	msgtype = msg->msg_type;
	PCI_TRACE(("pciedev_create_d2h_messages: MSG_TYPE=%d\n", msgtype));

	switch (msgtype) {
		case MSG_TYPE_LOOPBACK:
			PCI_TRACE(("MSG_TYPE_LOOPBACK: \n"));
			msglen = MSGBUF_LEN(pciedev->dtoh_ctrlcpl);
			pkt = MALLOC(pciedev->osh, msglen);
			if (pkt == NULL) {
				DBG_BUS_INC(pciedev, pciedev_create_d2h_messages);
				PCI_ERROR(("Could not allocate memory, malloc failed\n"));
				PCIEDEV_MALLOC_ERR_INCR(pciedev);
				return FALSE;
			}
			bcopy(PKTDATA(pciedev->osh, p), pkt, msglen);
			PKTFREE(pciedev->osh, p, TRUE);
			if (!pciedev_xmit_msgbuf_packet(pciedev, pkt, msgtype, msglen,
				pciedev->dtoh_ctrlcpl)) {
				DBG_BUS_INC(pciedev, pciedev_create_d2h_messages);
				MFREE(pciedev->osh, pkt, msglen);
				return FALSE;
			}
			return TRUE;
			break;
		case MSG_TYPE_WL_EVENT:
			{
			bcm_event_t *evtmsg = (bcm_event_t *)&msg[1];
			uint32 event_type = ntoh32_ua(&evtmsg->event.event_type);
			PCI_TRACE(("MSG_TYPE_WL_EVENT: \n"));
#ifdef UART_TRANSPORT
			/* check for MSGTRACE event and send that up to host over UART */
			if (pciedev->uarttrans_enab) {
				if (event_type < WLC_E_LAST &&
				    isset(pciedev->uart_event_inds_mask, event_type)) {
					h5_send_msgbuf((uchar *)msg, PKTLEN(pciedev->osh, p),
					               msgtype, pciedev->event_seqnum);
					++pciedev->event_seqnum;
					if (event_type == WLC_E_TRACE) {
						msgtrace_hdr_t *trace_msg;
						trace_msg = (msgtrace_hdr_t*)&evtmsg[1];
#ifdef LOGTRACE
						if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_LOG)
							logtrace_sent();
#endif // endif
#ifdef MSGTRACE
						if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_MSG)
							msgtrace_sent();
#endif // endif
					}
					PKTFREE(pciedev->osh, p, TRUE);
					return TRUE;
				}
			}
#endif /* UART_TRANSPORT */

			 /* Handle d3 pkt count seperately since UART_TRANSPORT is
			  * compile time option.
			  */
			if (pciedev_ds_in_host_sleep(pciedev) &&
#ifdef UART_TRANSPORT
				!(pciedev->uarttrans_enab) &&
#endif // endif
				TRUE) {
				++pciedev->in_d3_pktcount_evnt;
			}
			if (pciedev_ds_in_host_sleep(pciedev) &&
				pciedev->in_d3_pktcount_evnt > PCIE_IN_D3_SUSP_EVNTMAX) {
				DBG_BUS_INC(pciedev, pciedev_create_d2h_messages);
				PCI_PRINT(("D3 DropEvent. evnt cnt %d  type %d \n",
					pciedev->in_d3_pktcount_evnt, event_type));

				pciedev->d3_drop_cnt_evnt++;
				PKTFREE(pciedev->osh, p, TRUE);
				return FALSE;
			}
			prio = D2H_REQ_PRIO_1;
			break;
			}
		case MSG_TYPE_RX_PYLD:
			PCI_TRACE(("MSG_TYPE_RX_PYLD\n"));
			if (pciedev_ds_in_host_sleep(pciedev) &&
				++pciedev->in_d3_pktcount_rx > PCIE_IN_D3_SUSP_PKTMAX) {
				DBG_BUS_INC(pciedev, pciedev_create_d2h_messages);
				PCI_PRINT(("In D3 Suspend. Dropping data packet..\n"));
				pciedev->d3_drop_cnt_rx++;
				PKTFREE(pciedev->osh, p, TRUE);
				return FALSE;
			}

			/* Chained rx packets should not come to this layer */
			if (PKTNEXT(pciedev->osh, p) != NULL) {
				PCI_TRACE(("pciedev_create_d2h_messages: "
					"Chained rx packets should not come here\n"));
				DBG_BUS_INC(pciedev, pciedev_create_d2h_messages);
				pciedev_chained_rxpkt(pciedev, p);
				return FALSE;
			}
			schedule_q = FALSE;
			break;
		case MSG_TYPE_FIRMWARE_TIMESTAMP:
			PCI_TRACE(("MSG_TYPE_FIRMWARE_TIMESTAMP\n"));
			if (pciedev_ds_in_host_sleep(pciedev) &&
				++pciedev->in_d3_pktcount_rx > PCIE_IN_D3_SUSP_PKTMAX) {
				PCI_TRACE(("In D3 Suspend. Dropping data packet..\n"));
				pciedev->d3_drop_cnt_fw_ts++;
				PKTFREE(pciedev->osh, p, TRUE);
				return FALSE;
			}
			prio = D2H_REQ_PRIO_2;
			break;
		default:
			PCI_TRACE(("pciedev_create_d2h_messages: Unknown msgtype %d \n", msgtype));
			break;
	}

	(void)pciedev_queue_d2h_req(pciedev, p, prio);

	if (schedule_q && !IS_IOCTLREQ_PENDING(pciedev))
		pciedev_queue_d2h_req_send(pciedev);

	return ret;
} /* pciedev_create_d2h_messages */

static void
pciedev_chained_rxpkt(struct dngl_bus *pciedev, void *p)
{

	PCI_TRACE(("%s: Drop chained Pkt! PktId:0x%d, NextPkt:%p\n",
		__FUNCTION__, PKTID(p), PKTNEXT(pciedev->osh, p)));

	pciedev->dropped_chained_rxpkts++;
	PKTFREE(pciedev->osh, p, TRUE);

}

/**
 * Return host address & bufid stored in the rx frag
 * Returned address is used by PCIe dma to transfer pending portions of payload + .3 hdr
 * returned address should account for local pktlength + pad + dma offset
 * Returned dataoffset specifies the start-addr of the payload in the host buffer
 */
static dma64addr_t
pciedev_get_haddr_from_lfrag(struct dngl_bus *pciedev, void *p, uint32 *bufid,
	uint16 *dataoffset)
{
	dma64addr_t haddr;
	uint32 addr_offset;
	uint32 pktlen = PKTLEN(pciedev->osh, p);

	/* Packet structure in host
	 * Unused area
	 * alignment for 4 bytes(pad)
	 * pending pkt from TCM + .3 hdr(pktlen)
	 * pkt dma'ed by d11 dma(start addr at addr_offset)
	 * -----------------
	*/
	if (PKTFRAGUSEDLEN(pciedev->osh, p)) {
		/* Some part of host buffer already contains partial payload. So, we */
		/* need to stitch the packet up from the start of the existing payload. */

		/* We had reserved PKTRXFRAGSZ to be stitched back by mem2mem dma */
		/* But after hdr conversion, if length is less than that */
		/* account for unused area in data offset */
		/* Retrieve host address stored in rx frag */
		addr_offset = PKTFRAGDATA_LO(pciedev->osh, p, 1);
		if (RXFIFO_SPLIT() && pciedev->copycount) {
			/* Copy count bytes come from dongle */
			addr_offset = addr_offset + pciedev->copycount;
			*dataoffset = pciedev->copycount - pktlen;
		} else if (pciedev->tcmsegsz) {
			*dataoffset = pciedev->tcmsegsz - pktlen;
		} else {
			PCI_ERROR(("Both segsize & copycount not programmed \n"));
		}
		haddr.hiaddr = PKTFRAGDATA_HI(pciedev->osh, p, 1);
		/* account for data offset coming from pcie dma here */
		haddr.loaddr = (addr_offset - pktlen);
	} else {
		/* No part of host buffer has been used yet. */
		/* account for data offset coming from pcie dma here */
		*dataoffset = 0;
		haddr.hiaddr = PKTFRAGDATA_HI(pciedev->osh, p, 1);
		haddr.loaddr = PKTFRAGDATA_LO(pciedev->osh, p, 1) - pciedev->tcmsegsz;
	}

	/* Buffer id */
	*bufid = PKTFRAGPKTID(pciedev->osh, p);

	/* host address for this frag was used by d11 dma */
	/* reset host addr avail flag */
	PKTRESETRXFRAG(pciedev->osh, p);

	/* Return dest addr for PCIe dma */
	return haddr;
} /* pciedev_get_haddr_from_lfrag */

void
pciedev_xmit_txstatus(struct dngl_bus *pciedev)
{
	msgbuf_ring_t *ring = pciedev->dtoh_txcpl;
	lcl_buf_pool_t *pool = ring->buf_pool;

	if (pool->pend_item_cnt == 0) {
		PCI_TRACE(("pciedev_xmit_txstatus: pool pend_item_cnt zero\n"));
		return;
	}

	if (pciedev_ds_in_host_sleep_no_bus_access(pciedev)) {
		PCI_ERROR(("%s:CANNOT POST TXSTATUS WHEN IN HOST SLEEP\n", __FUNCTION__));
		pciedev->txstatus_in_host_sleep ++;
		return;
	}

	if (IS_IOCTLREQ_PENDING(pciedev)) {
		SET_TXSTS_PENDING(pciedev);
		PCI_TRACE(("pciedev_xmit_txstatus: txstatus pending\n"));
		return;
	}

	ASSERT(pool->pend_item_cnt <= MSGBUF_CHECK_WRITE_SPACE(ring));

	if ((PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, RXDESC) <
		MIN_RXDESC_AVAIL) ||
		(PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, TXDESC) <
		MIN_TXDESC_AVAIL)) {
		SET_TXSTS_PENDING(pciedev);
		PCI_TRACE(("tx cplt message failed (not enough dma des), not handled now: len %d\n",
			(MSGBUF_LEN(ring) * pool->pend_item_cnt)));
		DBG_BUS_INC(pciedev, pciedev_xmit_txstatus);
		return;
	}

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	pciedev_sync_flowring_read_ptrs(pciedev, ring);
#endif /* PCIE_DMA_INDEX && SBTOPCIE_INDICES */

	if (!pciedev_xmit_msgbuf_packet(pciedev, pool->local_buf_in_use,
	        MSG_TYPE_TX_STATUS, (MSGBUF_LEN(ring) * pool->pend_item_cnt), ring))
	{
		DBG_BUS_INC(pciedev, pciedev_xmit_txstatus);
		SET_TXSTS_PENDING(pciedev);
		PCI_ERROR(("tx cplt message failed : not handled now , len %d\n",
			(MSGBUF_LEN(ring) * pool->pend_item_cnt)));
		ASSERT(0);
		return;
	}

	/* Update IPC Stats */
	pciedev->metrics->num_txstatus_drbl++;
	pciedev->metrics->num_txstatus += pool->pend_item_cnt;
	pciedev->metrics->num_completions += pool->pend_item_cnt;

	pool->pend_item_cnt = 0;
	pool->local_buf_in_use = NULL;
	CLR_TXSTS_PENDING(pciedev);
}

/**
 * Called back by the 'lbuf' subsystem for every pkt free
 * for tx frag, send out tx status.
 * for rx frags with host addr valid, reclaim host addresses
 */
bool
pciedev_lbuf_callback(void *arg, void *p)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	uint32 status = 0;
	uint16 seq = 0;
	uint8 key_seq[WL_KEY_SEQ_INFO_SIZE] = {0};
	if (PKTISTXFRAG(pciedev->osh, p)) {
#ifndef BCM_DHDHDR
		cmn_msg_hdr_t *cmn_msg;
#endif // endif
		uint16 ringid;
		msgbuf_ring_t *flow_ring;
		if (!PKTHASMETADATA(pciedev->osh, (struct lbuf *)p))
			return FALSE;

		/* Check if PKT has been TXstatus processed */
		if (PKTISTXSPROCESSED(pciedev->osh, p)) {
			/* Reset TXstatus processed state */
			PKTRESETTXSPROCESSED(pciedev->osh, p);
			pciedev_tx_processed(pciedev, p, &status, &seq, &key_seq[0]);
		}
		ringid = PKTFRAGFLOWRINGID(pciedev->osh, p);
		if (pciedev_update_txstatus(pciedev, status, PKTFRAGRINGINDEX(pciedev->osh, p),
			ringid, seq, PKTISTXSHOLD(pciedev->osh, p), &key_seq[0]))
		{
			BCMPCIE_IPC_HPA_TEST(pciedev, PKTFRAGPKTID(pciedev->osh, p),
				BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_RESPONSE);
			BUZZZ_KPI_PKT1(KPI_PKT_BUS_TXCMPL, 2,
				PKTFRAGPKTID(pciedev->osh, p), ringid);
			return FALSE;
		}
		PKTRESETTXSHOLD(pciedev->osh, p);

		flow_ring = pciedev->h2d_submitring_ptr[ringid];
		ASSERT(flow_ring);

		/* Enqueue tx status quickly if its a success packet */
		if (status == 0 && !pciedev_ds_in_host_sleep(pciedev) &&
			!IS_IOCTLREQ_PENDING(pciedev) &&
			PCIEDEV_TXCPL_RESOURCE_AVAILABLE(pciedev)) {
			ipc_timestamp_t ts;

			if (pciedev->timesync && PKTISTXTSINSERTED(pciedev->osh, p)) {
				ts.ts_low = PKTFRAGDATA_LO(pciedev->osh, p, 1);
				ts.ts_high = PKTFRAGDATA_HI(pciedev->osh, p, 1);
				PKTRESETTXTSINSERTED(pciedev->osh, p);
			}

#ifdef PCIEDEV_FAST_DELETE_RING
			if ((flow_ring->status & FLOW_RING_DELETE_RESP_PENDING) &&
			    (pciedev->fastdeletering)) {
				/* Check if the rd_idx of the packet is "after" ring delete_idx */
				if (pciedev_fastdelete_notxstatus(flow_ring,
					PKTFRAGRINGINDEX(pciedev->osh, p))) {
					goto process_possible_flush;
				}
			}
#endif /* PCIEDEV_FAST_DELETE_RING */
			if (PCIEDEV_QUEUE_TXSTATUS(pciedev, PKTFRAGPKTID(pciedev->osh, p),
				PKTIFINDEX(pciedev->osh, p), ringid, status, &ts) == BCME_OK) {
#ifdef PCIEDEV_FAST_DELETE_RING
process_possible_flush:
#endif /* PCIEDEV_FAST_DELETE_RING */
				if (flow_ring->status & FLOW_RING_FLUSH_PENDING) {
					pciedev_process_pending_flring_resp(pciedev, flow_ring);
				}
				return FALSE;
			}
		}
#ifndef BCM_DHDHDR
		/* check for tx frag */
		if (PKTHEADROOM(pciedev->osh, p) < 8) {
			DBG_BUS_INC(pciedev, pciedev_lbuf_callback);
			PCI_ERROR(("PKTHEADROOM is less than needed 8, %d\n",
				PKTHEADROOM(pciedev->osh, p)));
			ASSERT(0);
		}
		PKTPUSH(pciedev->osh, p, sizeof(cmn_msg_hdr_t));
		cmn_msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);

		cmn_msg->msg_type = MSG_TYPE_TXMETADATA_PYLD;
		/* Passing tx status on request_id as it is unused internally */
		cmn_msg->request_id = status;
#endif /* !BCM_DHDHDR */

		/* Now no matter DHDHDR is enable or not we can use generic txstats field
		 * to save it.
		 */
		PKTFRAGSETTXSTATUS(pciedev->osh, p, status);

		/* Keeping count of how many host tx lfrags are freed while in D3 */
		if (pciedev_ds_in_host_sleep(pciedev))
			pciedev->num_tx_status_in_d3 ++;

		/* Why do we need this check */
		/* if (pktid != DMA_XFER_PKTID) */
		/* if the txstatus is pending for a packet should we reduce the count here */
		if (pciedev_queue_d2h_req(pciedev, p, D2H_REQ_PRIO_0) == BCME_OK)
			flow_ring->d2h_q_txs_pending++;

		if (dll_empty(&pciedev->active_prioring_list) &&
			PCIEDEV_TXCPL_PEND_ITEM_CNT(pciedev)) {
			PCIEDEV_XMIT_TXSTATUS(pciedev);
		}
		return TRUE;
	} else {
		uint16 rxcpl_id;
		rxcpl_info_t *p_rxcpl_info;

		/* Reset converted flag */
		if (PKTISRXFRAG(pciedev->osh, p)) {
			PKTRESETHDRCONVTD(pciedev->osh, p);
			if (PKTFRAGTOTNUM(pciedev->osh, p)) {
#if defined(BCMHWA) && defined(HWA_RXFILL_BUILD)
				OSL_SYS_HALT();
#endif /* BCMHWA */
				/* PKTFRAGTOTNUM is non-zero only for an rxfrag used in txpath
				 * forwarding. In such a case, PKTFRAGLEN[2]
				 * is used to store full size of host buffer, which is
				 * restored to PKTFRAGLEN[1], here.
				 */
				if (PKTFRAGLEN(pciedev->osh, p, 2)) {
					/* Reverting the frag params */
					PKTSETFRAGTOTNUM(pciedev->osh, p, 0);
					PKTSETFRAGLEN(pciedev->osh, p, 1,
							PKTFRAGLEN(pciedev->osh, p, 2));
					PKTSETFRAGLEN(pciedev->osh, p, 2, 0);
				}
			}
		}

		rxcpl_id = PKTRXCPLID(pciedev->osh, p);
		if (rxcpl_id == 0)
			return FALSE;

		/* that means pkt did not go through pciedev rx path */
		/* see if this is carrying a chain of rxcplids  */
		p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);

		if (!BCM_RXCPL_IN_TRANSIT(p_rxcpl_info)) {
			PCI_TRACE(("pciedev_lbuf_callback: chain of rxcplids\n"));
			return FALSE;
		}

		PKTRESETRXCPLID(pciedev->osh, p);
		BCM_RXCPL_CLR_IN_TRANSIT(p_rxcpl_info);
		BCM_RXCPL_CLR_VALID_INFO(p_rxcpl_info);
		/* call the queue logic, let it handle the dropping of rxcpl info */
		if (PKTNEEDRXCPL(pciedev->osh, p) || BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info)) {
#if defined(BCMHWA) && defined(HWA_RXFILL_BUILD) && HWA_REVISION_LE_130
			/* XXX, WAR for HWA free rxbuffer in paired type issue
			 * I encounter below errors which I need this WAR.
			 * cmn::errorstatusreg report 3
			 * rx::debug_errorstatus 4
			 * RxBM audit: Get duplicate rxbuffer<154> from RxBM
			 * which rxbuffer idx is just freed to freeindexQ in paired type.
			 * XXX, CRBCAHWA-558
			 */
			if (PKTISRXFRAG(pciedev->osh, p)) {
				/* Save pktid for rxcpl */
				p_rxcpl_info->host_pktref = PKTFRAGPKTID(pciedev->osh, p);
				/* Set datalen to 0 so that Host will drop it */
				p_rxcpl_info->rxcpl_len.datalen = 0;
				/* Set it as valid info to send the pktid to host */
				BCM_RXCPL_SET_VALID_INFO(p_rxcpl_info);
				/* Reset RXFLAG so that HWA free rxbuffer in simple type */
				PKTRESETRXFRAG(pciedev->osh, p);
			}
#endif /* BCMHWA && HWA_RXFILL_BUILD */
			pciedev_queue_rxcomplete_local(pciedev, p_rxcpl_info, pciedev->dtoh_rxcpl,
				BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info));
		}
#if defined(BCMHWA) && defined(HWA_RXFILL_BUILD)
		else if (PKTISHWAHOSTREORDER(pciedev->osh, p) && PKTISRXFRAG(pciedev->osh, p)) {
			PKTRESETHWAHOSTREORDER(pciedev->osh, p);
			PKTRESETRXFRAG(pciedev->osh, p);
			pciedev_hwa_queue_rxcomplete_fast(pciedev, PKTFRAGPKTID(pciedev->osh, p));
		}
#endif // endif
	}

	return FALSE;
} /* pciedev_lbuf_callback */

#ifndef HWA_TXCPLE_BUILD
static int
pciedev_queue_txstatus(struct dngl_bus *pciedev, uint32 pktid, uint8 ifindx, uint16 ringid,
	uint16 txstatus, ipc_timestamp_t *ts)
{
	void * wi;
	msgbuf_ring_t *ring = pciedev->dtoh_txcpl;
	lcl_buf_pool_t *pool = ring->buf_pool;
	uint32 avail_ring_entry;

	if (pool->local_buf_in_use == NULL) {
		pool->local_buf_in_use = pciedev_lclpool_alloc_lclbuf(ring);
		if (pool->local_buf_in_use == NULL) {
			PCI_TRACE(("pciedev_queue_txstatus: alloc failed\n"));
			DBG_BUS_INC(pciedev, pciedev_queue_txstatus);
			return BCME_ERROR;
		}

		ASSERT(pool->local_buf_in_use);
		pool->pend_item_cnt = 0;
	}
	ASSERT(pool->pend_item_cnt < pool->item_cnt);

	wi = (void *)(pool->local_buf_in_use +
		(MSGBUF_LEN(ring) * pool->pend_item_cnt));
	pool->pend_item_cnt++;

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	pciedev_upd_last_queued_flowring(pciedev, ringid);
#endif /* PCIE_DMA_INDEX && SBTOPCIE_INDICES */

	BCMPCIE_IPC_HPA_TEST(pciedev, pktid,
		BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_RESPONSE);
	BUZZZ_KPI_PKT1(KPI_PKT_BUS_TXCMPL, 2, pktid, ringid);

	/* Now compose the TxComplete Work item */
	switch (MSGBUF_ITEM_TYPE(ring))
	{
		case MSGBUF_WI_WI64:
		{
			host_txbuf_cmpl_t *wi64 = (host_txbuf_cmpl_t*)wi;
			wi64->cmn_hdr.msg_type = MSG_TYPE_TX_STATUS;
			wi64->cmn_hdr.if_id = ifindx;
			wi64->cmn_hdr.flags =
				(wi64->cmn_hdr.flags & (~MSGBUF_RING_INIT_PHASE)) |
				(ring->current_phase & MSGBUF_RING_INIT_PHASE);
			wi64->compl_hdr.status = 0;
			wi64->compl_hdr.flow_ring_id = ringid;
			wi64->cmn_hdr.request_id = htol32(pktid);
			wi64->metadata_len = 0;
			wi64->tx_status = htol16(txstatus);
#if defined(PCIE_M2M_D2H_SYNC)
			PCIE_M2M_D2H_SYNC_MARKER_INSERT(wi64, MSGBUF_LEN(ring),
				pciedev->txbuf_cmpl_epoch);
#endif /* PCIE_M2M_D2H_SYNC */
			break;
		}

		case MSGBUF_WI_CWI:
		{
#if defined(HWA_TXCPLE_CWI_4B_T)
	        /* For backward compatibility to support 4Byte Txcple used by host */
	        /* To be removed later */
			if (MSGBUF_ITEM_SIZE(ring) == HWA_TXCPLE_CWI_4_BYTES)
			{
				hwa_txcple_cwi_4b_t *cwi = (hwa_txcple_cwi_4b_t*)wi;
				cwi->host_pktid = htol32(pktid);
			}
			else
#endif /* HWA_TXCPLE_CWI_4B_T */
			{
				hwa_txcple_cwi_t *cwi = (hwa_txcple_cwi_t*)wi;
				cwi->host_pktid = htol32(pktid);
				cwi->flow_ring_id = htol16(ringid);
				cwi->if_id = ifindx;
			}
			break;
		}

		default:
		{
			PCI_ERROR(("Invalid TxCpl itemtype %u\n", MSGBUF_ITEM_TYPE(ring)));
			ASSERT(0);
		}

	} /* switch MSGBUF_ITEM_TYPE */

	ASSERT(pool->pend_item_cnt <= MSGBUF_CHECK_WRITE_SPACE(ring));

	avail_ring_entry = MIN(pool->item_cnt, MSGBUF_CHECK_WRITE_SPACE(ring));

	if ((pool->pend_item_cnt >= avail_ring_entry) ||
		(pool->pend_item_cnt && dll_empty(&pciedev->active_prioring_list))) {
		PCIEDEV_XMIT_TXSTATUS(pciedev);
	}
	return BCME_OK;
} /* pciedev_queue_txstatus */
#endif /* !HWA_TXCPLE_BUILD */

void
pciedev_xmit_rxcomplete(struct dngl_bus *pciedev)
{
	msgbuf_ring_t *ring = pciedev->dtoh_rxcpl;
	lcl_buf_pool_t *pool = ring->buf_pool;

	if (pool->pend_item_cnt == 0) {
		PCI_TRACE(("pciedev_xmit_rxcomplete: pool pend_item_cnt zero\n"));
		return;
	}

	if (pciedev_ds_in_host_sleep_no_bus_access(pciedev)) {
		PCI_ERROR(("%s:CANNOT POST RXCOMPLETE WHEN IN HOST SLEEP\n", __FUNCTION__));
		pciedev->rxcomplete_in_host_sleep ++;
		return;
	}

	if (IS_IOCTLREQ_PENDING(pciedev)) {
		PCI_TRACE(("pciedev_xmit_rxcomplete: rxcpl pending\n"));
		SET_RXCPL_PENDING(pciedev);
		return;
	}

	if ((PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, RXDESC) <
		MIN_RXDESC_AVAIL) ||
		(PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, TXDESC) <
		MIN_TXDESC_AVAIL))
	{
		DBG_BUS_INC(pciedev, pciedev_xmit_rxcomplete);
		SET_RXCPL_PENDING(pciedev);
		PCI_ERROR(("rx cplt message failed : size %d \n",
			(MSGBUF_LEN(ring) * pool->pend_item_cnt)));
		return;
	}
	if (!pciedev_xmit_msgbuf_packet(pciedev, pool->local_buf_in_use,
	        MSG_TYPE_RX_CMPLT, (MSGBUF_LEN(ring) * pool->pend_item_cnt), ring))
	{
		DBG_BUS_INC(pciedev, pciedev_xmit_rxcomplete);
		SET_RXCPL_PENDING(pciedev);
		PCI_ERROR(("rx cplt message failed : not handled now, size %d \n",
			(MSGBUF_LEN(ring) * pool->pend_item_cnt)));
		ASSERT(0);
		return;
	}

	/* Update IPC Stats */
	pciedev->metrics->num_rxcmplt_drbl++;
	pciedev->metrics->num_rxcmplt += pool->pend_item_cnt;
	pciedev->metrics->num_completions += pool->pend_item_cnt;

	pool->pend_item_cnt = 0;
	pool->local_buf_in_use = NULL;
	CLR_RXCPL_PENDING(pciedev);
}

void
pciedev_queue_d2h_req_send(struct dngl_bus *pciedev)
{
	d2h_msg_handler msg_handler;
	uint prio_bmap;
	int prio;
	void *p;

	if (pciedev_ds_in_host_sleep(pciedev)) {
		/* Do not service the queue if the device is in D3 suspend */
		PCI_TRACE(("pciedev_queue_d2h_req_send:In D3 Suspend\n"));
		DBG_BUS_INC(pciedev, pciedev_queue_d2h_req_send);
		return;
	}

	if (pciedev->active_pending) {
		if (IS_IOCTLREQ_PENDING(pciedev)) {
			/* ioctl is pending waiting for resources to be avilable */
			/* so check it the IOCTL could be processed now */
			/* all the needed from the IOCTL request is already in dongle memory */
			pciedev_process_ioctl_pend(pciedev);
			if (IS_IOCTLREQ_PENDING(pciedev)) {
				DBG_BUS_INC(pciedev, pciedev_queue_d2h_req_send);
				PCI_ERROR(("not enough resources to process ioctl request\n"));
				return;
			}
		}
		if (IS_IOCTLRESP_PENDING(pciedev)) {
			pciedev_send_ioctl_completion(pciedev);
		}

		/* If local pciedev_ring_cmplt_queue_t is NOT empty, process it */
		if (IS_RXCTLRESP_PENDING(pciedev)) {
			pciedev_process_ctrl_cmplt(pciedev);
		}
		if (IS_MBDATA_PENDING(pciedev)) {
			pciedev_send_d2h_mb_data_ctrlmsg(pciedev);
		}

		if (IS_RXCPL_PENDING(pciedev)) {
			/* tried to send rxcompletion but looks like failed with no resources */
			/* so try it now */
			PCIEDEV_XMIT_RXCOMPLETE(pciedev);
		}
		if (IS_TXSTS_PENDING(pciedev)) {
			/* tried to send txcompletion but looks like failed with no resources */
			/* so try it now */
			PCIEDEV_XMIT_TXSTATUS(pciedev);
		}
	}

#ifdef PCIE_DMA_INDEX
#ifndef SBTOPCIE_INDICES
	if ((pciedev->dmaindex_d2h_pending == 1) && pktq_empty(&pciedev->d2h_req_q)) {
		/* DMAing indices is pending waiting for resources to be available */
		/* so try it now */
		pciedev_dmaindex_put(pciedev, NULL);

	}
#endif /* !SBTOPCIE_INDICES */
#endif /* PCIE_DMA_INDEX */

	/* check if there are any pending rxcompletes waiting for space on local buf */
	pciedev_queue_rxcomplete_msgring(pciedev, pciedev->dtoh_rxcpl);

	/* Process pending fetches */
	if  (pciedev->fetch_req_pend_list->head) {
		pciedev_process_pending_fetches(pciedev);
	}
	if (!pktq_empty(&pciedev->d2h_req_q)) {
		prio_bmap = D2H_REQ_PRIO_BMAP;

		while ((p = pktq_mdeq(&pciedev->d2h_req_q, prio_bmap, &prio))) {
			if (!pciedev_check_process_d2h_message(pciedev, p, &msg_handler)) {
				PCI_TRACE(("not enough resources to send event to Host\n"));
				pktq_penq_head(&pciedev->d2h_req_q, prio, p);
				prio_bmap &= ~(1 << prio);
				continue;
			}
			if (msg_handler) {
				(msg_handler)(pciedev, p);
			}
			/* Make this part of the ring data structure */
			if (pciedev->active_pending) {
				PCI_TRACE(("pendstatus 0x%04x\n", pciedev->active_pending));
				break;
			}
		} /* while(p) */
	}

#if defined(BCMHWA) && defined(HWA_RXCPLE_BUILD)
	/* No glom_timer when HWA RCPLE enabled */
	if (PCIEDEV_RXCPL_PEND_ITEM_CNT(pciedev)) {
		PCIEDEV_XMIT_RXCOMPLETE(pciedev);
	}
#else
	/* Send rx completes for the chain of pkts */
	if (PCIEDEV_RXCPL_PEND_ITEM_CNT(pciedev) < PCIEDEV_MIN_RXCPLGLOM_COUNT) {
		PCIEDEV_XMIT_RXCOMPLETE(pciedev);
	} else {
		if (!pciedev->glom_timer_active) {
			dngl_add_timer(pciedev->glom_timer, MSGBUF_TIMER_DELAY, FALSE);
			pciedev->glom_timer_active = TRUE;
		}
	}
#endif // endif
} /* pciedev_queue_d2h_req_send */

int
pciedev_queue_d2h_req(struct dngl_bus *pciedev, void *p, uint8 prio)
{
	if (pktq_full(&pciedev->d2h_req_q)) {
		PCI_ERROR(("PANIC: pciedev_queue_d2h_req full\n"));

		/* Reset the Metadata flag to avoid recursion of PKTFREE */
		PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *) p);
		/* Callbacks should not be invoked again */
		PKTFREE(pciedev->osh, p, FALSE);
		return BCME_ERROR;
	}
	/* Chained packet should not comne.
	* If the packet is chained, it's a FATAL condition.
	*/
	if (PKTLINK(p) != NULL) {
		PCI_ERROR(("%s:%p:Chained Packet:%p!!\n",
			__FUNCTION__, __builtin_return_address(0), p));
		OSL_SYS_HALT();
	}
	pktq_penq(&pciedev->d2h_req_q, prio, p);
	return BCME_OK;
}

/**
 * Starts pumping a message to the host. Is called when eg host needs to be notified of payload
 * that was received by the dongle. Calls both dma_rxfast() and dma_txfast() because of M2M DMA.
 */
int
pciedev_tx_pyld(struct dngl_bus *pciedev, void *p, ret_buf_t *data_buf, uint16 data_len,
	uint8 msgtype)
{
	dma_queue_t *dmaq = pciedev->dtoh_dma_q[pciedev->default_dma_ch];
	hnddma_t *di = pciedev->d2h_di[pciedev->default_dma_ch];
	m2m_desc_t *desc = pciedev->m2m_desc;
	dma64addr_t addr = { .low_addr = 0, .high_addr = 0 };
	uint16 msglen;

	if ((data_len > 0) && (data_len < PCIE_MEM2MEM_DMA_MIN_LEN)) {
		data_len = PCIE_MEM2MEM_DMA_MIN_LEN;
	}

	msglen = data_len;
	if (msglen < PCIE_MEM2MEM_DMA_MIN_LEN) {
		DBG_BUS_INC(pciedev, pciedev_tx_pyld);
		PCI_ERROR(("msglen is %d, data_len %d\n", msglen, data_len));
		ASSERT(0);
	}

	INIT_M2M_DESC(desc);

	/* Program RX descriptor for rxoffset */
	/* offset is handled through dataoffset portion of rx completion for rx payloads */
	/* None of the other messages have this dataoffset field */

	/* check if there is a need to transfer real data */
	if (data_len) {
		data_buf->high_addr = data_buf->high_addr & ~PCIE_ADDR_OFFSET;
		PHYSADDR64HISET(addr, data_buf->high_addr);
		PHYSADDR64LOSET(addr, data_buf->low_addr);
		SETUP_RX_DESC(desc, addr, data_len);
	}

	/* how do we make sure pktlen of p is greater than metadata len */
	/* should we check the pkttotlen instead */
	if ((PKTLEN(pciedev->osh, p) < msglen) && (msglen == PCIE_MEM2MEM_DMA_MIN_LEN)) {
		PCI_INFORM(("ALIGN: dmalen(%d) < minimum(%d), tailroom %d, msgtype %d\n",
			PKTLEN(pciedev->osh, p), msglen,  PKTTAILROOM(pciedev->osh, p), msgtype));
		/* to cover one of the HW wars we need packet len to be a min of 8 */
		if (PKTTAILROOM(pciedev->osh, p) < msglen - pkttotlen(pciedev->osh, p)) {
			PCI_ERROR(("BAD CASE: dmalen(%d) < minimum(%d), tailroom %d, msgtype %d\n",
				PKTLEN(pciedev->osh, p), msglen,
				PKTTAILROOM(pciedev->osh, p), msgtype));
			DBG_BUS_INC(pciedev, pciedev_tx_pyld);
		}
		PKTSETLEN(pciedev->osh, p, msglen);
	}

	PHYSADDR64HISET(addr, 0);
	PHYSADDR64LOSET(addr, (uint32)p); /* XFER_FROM_LBUF */
	SETUP_TX_DESC(desc, addr, PKTLEN(pciedev->osh, p));
	SETUP_XFER_FLAGS(desc, XFER_FROM_LBUF);

	if (DMA_M2M_SUBMIT(di, desc)) {
		DBG_BUS_INC(pciedev, pciedev_tx_pyld);
		TRAP_DUE_DMA_RESOURCES(("pciedev_tx_pyld: "
			"m2m dma failed, rx descs avail = %d tx desc avail = %d\n",
			PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, RXDESC),
			PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, TXDESC)));
		goto exit;
	}

	/* updates bookkeeping */
	pciedev_enqueue_dma_info(dmaq, msglen, msgtype, 0, NULL, 0, 0);

	return TRUE;

exit:
	return FALSE;
} /* pciedev_tx_pyld */

int
pciedev_tx_pyld_from_src_addr(struct dngl_bus *pciedev, void *p, ret_buf_t *data_buf,
	uint16 data_len, uint8 msgtype, msgbuf_ring_t *msgring, uint8 dmach, dma64addr_t src_addr)
{
	dma64addr_t addr = { .low_addr = 0, .high_addr = 0 };
	uint16 msglen;
	hnddma_t *di;

	di = pciedev->d2h_di[dmach];
	ASSERT(di);

	if ((data_len > 0) && (data_len < PCIE_MEM2MEM_DMA_MIN_LEN)) {
		data_len = PCIE_MEM2MEM_DMA_MIN_LEN;
	}

	msglen = data_len;
	if (msglen < PCIE_MEM2MEM_DMA_MIN_LEN) {
		PCI_ERROR(("msglen is %d, data_len %d\n", msglen, data_len));
		ASSERT(0);
	}

	/* updates bookkeeping */
	pciedev_enqueue_dma_info(pciedev->dtoh_dma_q[dmach],
		msglen, msgtype, 0, msgring, (uint32)p, data_buf->low_addr);

	/* check if there is a need to transfer real data */
	if (data_len) {
		data_buf->high_addr = data_buf->high_addr & ~PCIE_ADDR_OFFSET;
		PHYSADDR64HISET(addr, data_buf->high_addr);
		PHYSADDR64LOSET(addr, data_buf->low_addr);
		if (dma_rxfast(di, addr, (uint32)data_len)) {
			TRAP_DUE_DMA_RESOURCES(("pciedev_tx_pyld : "
				"dma_rxfast failed for data, descs avail %d\n",
				PCIEDEV_GET_AVAIL_DESC(pciedev, dmach,
				DTOH, RXDESC)));
		}
	}

	dma_ptrbuf_txfast(di, src_addr, p, TRUE, msglen, TRUE, TRUE);

	return TRUE;
} /* pciedev_tx_pyld_from_src_addr */

/** Handle D2H dma transfers for messages and payload. Program m2m dma RX and TX descriptors */
static int
pciedev_tx_msgbuf(struct dngl_bus *pciedev, void *p, ret_buf_t *ret_buf,
	uint8 msg_type, uint16 msglen, msgbuf_ring_t *ring)
{
	dma_queue_t *dmaq = pciedev->dtoh_dma_q[pciedev->default_dma_ch];
	hnddma_t *di = pciedev->d2h_di[pciedev->default_dma_ch];
	m2m_desc_t *desc = pciedev->m2m_desc;
	dma64addr_t addr = { .low_addr = 0, .high_addr = 0 };
	uint32 phase_offset = 0;
	uint8 flags = 0;

	/* We could just check the ring null or not */
	if (ring->phase_supported) {
		phase_offset = sizeof(cmn_msg_hdr_t);
		flags |= PCIEDEV_INTERNAL_SENT_D2H_PHASE;
	}

	ASSERT(msglen >= (phase_offset + PCIE_MEM2MEM_DMA_MIN_LEN));

	/* for pcie dma devices, outbound dma is hardwired */
	/* to take in only pcie addr as dest space. so no need to give extra offset */
	ret_buf->high_addr = ret_buf->high_addr & ~PCIE_ADDR_OFFSET;

	INIT_M2M_DESC(desc);

	/* Program RX descriptor */

	/* rx buffer for data excluding the phase bits */
	PHYSADDR64HISET(addr, ret_buf->high_addr);
	PHYSADDR64LOSET(addr, ret_buf->low_addr + phase_offset);
	SETUP_RX_DESC(desc, addr, msglen - phase_offset);

	/* rx buffer for phase offset */
	if (phase_offset) {
		PHYSADDR64LOSET(addr, ret_buf->low_addr);
		SETUP_RX_DESC(desc, addr, phase_offset);
	}

	if (phase_offset) {
		/* tx for data excluding the phase offset */
		PHYSADDR64HISET(addr, (uint32) 0);
		PHYSADDR64LOSET(addr, (uint32)p + phase_offset);
		SETUP_TX_DESC(desc, addr, msglen - phase_offset);

		/* tx for phase offset */
		PHYSADDR64LOSET(addr, (uint32)p);
		SETUP_TX_DESC(desc, addr, phase_offset);
	} else {
		PHYSADDR64HISET(addr, (uint32) 0);
		PHYSADDR64LOSET(addr, (uint32)p);
		SETUP_TX_DESC(desc, addr, msglen);
	}

	if (DMA_M2M_SUBMIT(di, desc)) {
		DBG_BUS_INC(pciedev, pciedev_tx_msgbuf);
		TRAP_DUE_DMA_RESOURCES(("pciedev_tx_msgbuf: DtoH%c"
			"m2m dma failed, rx descs avail = %d tx desc avail = %d\n", ring->name[3],
			PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, RXDESC),
			PCIEDEV_GET_AVAIL_DESC(pciedev, pciedev->default_dma_ch, DTOH, TXDESC)));
		goto fail;
	}

	pciedev_enqueue_dma_info(dmaq, msglen, msg_type, flags, ring, 0, 0);

	return TRUE;

fail:
	return FALSE;
} /* pciedev_tx_msgbuf */

void
pciedev_ts_logging_min(dma_item_info_t *item)
{
	if (!item) {
		PCI_ERROR(("NULL item\n"));
		return;
	}

	item->timestamp  = 0;
	if (item->msg_type == MSG_TYPE_RX_PYLD) {
		return;
	}
	if (item->msg_type == MSG_TYPE_IOCTLPTR_REQ_ACK ||
		item->msg_type == MSG_TYPE_IOCT_PYLD ||
		item->msg_type == MSG_TYPE_D2H_MAILBOX_DATA ||
		item->msg_type == MSG_TYPE_RING_CREATE_CMPLT ||
		item->msg_type == MSG_TYPE_RING_DELETE_CMPLT ||
		item->msg_type == MSG_TYPE_FLOW_RING_FLUSH_CMPLT ||
		item->msg_type == MSG_TYPE_WL_EVENT ||
		item->msg_type == MSG_TYPE_INFO_BUF_CMPLT ||
		item->msg_type == MSG_TYPE_HOSTTIMSTAMP_CMPLT ||
		item->msg_type == MSG_TYPE_FIRMWARE_TIMESTAMP ||
		item->msg_type == MSG_TYPE_SNAPSHOT_CMPLT ||
		item->msg_type == MSG_TYPE_H2D_RING_DELETE_CMPLT ||
		item->msg_type == MSG_TYPE_D2H_RING_DELETE_CMPLT ||
		item->msg_type == MSG_TYPE_IOCTL_CMPLT) {
		item->timestamp  = OSL_SYSUPTIME();
	}
}

/**
 * Called after a h2d DMA interrupt, when a new so-called 'internal' (=firmware generated) message
 * is available in device memory.
 */
static uint16
pciedev_handle_h2d_pyld(struct dngl_bus *pciedev, uint8 msgtype, void *p,
	uint16 pktlen, msgbuf_ring_t *ring, uint8 dmach)
{
	switch (msgtype) {
		case MSG_TYPE_HOST_FETCH:
			pciedev_process_tx_payload(pciedev, dmach);
			break;
		case MSG_TYPE_LPBK_DMAXFER_PYLD:
			break;
		default:
			PCI_ERROR(("Unknown internal a message type 0x%02x\n", msgtype));
			DBG_BUS_INC(pciedev, pciedev_handle_h2d_pyld);
			break;
	}
	return 0;
}

/**
 * Called when one or more new 'receive submit buffer' message from the host (in argument 'p') have
 * been received in device memory.
 *     p:      points at first message
 *     pktlen: length of the packet that was received (containing messages)
 */
uint16
pciedev_handle_h2d_msg_rxsubmit(struct dngl_bus *pciedev, void *p,
	uint16 pktlen, msgbuf_ring_t *ring)
{
	uint8 msglen = MSGBUF_LEN(pciedev->htod_rx);

	ASSERT((pktlen % msglen) == 0);

	/* Increment IPC Data for rxsubmit */
	pciedev->metrics->num_submissions += pktlen / msglen;
	PCI_INFORM(("h2d_drbl:%d, new rxsubmit:%d, tot_num_submissions:%d\n",
		pciedev->metrics->num_h2d_doorbell, pktlen/msglen,
		pciedev->metrics->num_submissions));

	pciedev_add_to_inuselist(pciedev, p, pktlen/msglen); /* queues messages for processing */

	/* Invoke dma rxfill every time you get host rx buffers */
	if (BCMSPLITRX_ENAB()) {
#if defined(BCMHWA) && defined(HWA_RXPOST_BUILD)
		/* Should not be here when HWA_RXPOST_BUILD is TRUE */
		ASSERT(0);
#else
		pktpool_invoke_dmarxfill(pciedev->pktpool_rxlfrag);
#endif // endif
	}

	return 0;
}

#ifdef WLCFP

/** --- Section: Cached Flow Processing in PCIe Bus Layer. ------------------ */
#define FD_CFP_UNIT 0  /* single radio, unit #0 */

/** Given the CFP TCB state, determine if the TCB is established for fastpath */
#define CFP_TCB_INI	(0x01)		/* Bit position in state for ini */
#define CFP_TCB_CACHE	(0x02)		/* Bit position in state for cache */

#define CFP_NOT_CAPABLE(state, prio)        ((state[prio] & CFP_TCB_INI) != 0)

/** CFP LLCSNAP Insertion support: Used in 32bit copy with 32bit endian */
static const union {
	uint32 u32;
	char u8[4];
} _ctl_oui3 = { .u8 = {0x00, 0x00, 0x00, 0x03} };

/** LLCSNAP: OUI[2] setting for Bridge Tunnel (Apple ARP and Novell IPX) */
#define ETHER_TYPE_APPLE_ARP    0x80f3 /* Apple Address Resolution Protocol */
#define ETHER_TYPE_NOVELL_IPX   0x8137 /* Novel IPX Protocol */

#define BRIDGE_TUNNEL_OUI2      0xf8 /* OUI[2] value for Bridge Tunnel */

#define IS_BRIDGE_TUNNEL(et) \
	(((et) == ETHER_TYPE_APPLE_ARP) || ((et) == ETHER_TYPE_NOVELL_IPX))

/** Misc CFP classification functions */
static inline void cfp_flows_sendup(uint16 cfp_flowid, cfp_pktlist_t *pktlist);

/** Integrate CFP Flow classification within PCIe bus layer */
static inline void pciedev_classify(struct dngl_bus *pciedev, uint16 cfp_flowid,
	void *pkt, uint8 pkt_prio, bool cfp_not_capable);

static inline void pciedev_dispatch_pktlists(struct dngl_bus *pciedev,
	msgbuf_ring_t *msgbuf);

/** CFP Flow Classification and Sendup integration into PCIe Bus Layer */

static inline void /** Sendup a CFP capable packet list to CFP fastpath. */
cfp_flows_sendup(uint16 cfp_flowid, cfp_pktlist_t *pktlist)
{
	ASSERT(pktlist->head != NULL);

	PKTSETCLINK(pktlist->tail, NULL); /* terminate tail pkt::next now! */

	/* Pass the CFP capable packet list to CFP transmit fastpath entry point. */
	wlc_cfp_tx_sendup(FD_CFP_UNIT, cfp_flowid, pktlist->prio,
		pktlist->head, pktlist->tail, pktlist->pkts);

	CFP_FLOW_STATS_INCR(pktlist->dispatch_stats);

	/* Reset this pktlist */
	pktlist->pkts = 0;
	pktlist->head = NULL;
}

/**
 * Classify packets and link into appropriate CFP pktlist based on attached
 * CFP's Transmit Control Block per priority established state.
 */
static inline void
pciedev_classify(struct dngl_bus *pciedev, uint16 cfp_flowid,
	void *pkt, uint8 pkt_prio, bool cfp_not_capable)
{
	cfp_pktlist_t *pktlist;
	cfp_flows_t *cfp_flows = pciedev->cfp_flows;

	/* Check whether CFP fastpath is established for SCB and per priority ini */
	if (cfp_not_capable) {

		PKTCLRCFPFLOWID(pkt, CFP_FLOWID_INVALID);
		pktlist = &cfp_flows->pktlists[DNGL_SENDUP_PKTLIST_IDX]; /* slowpath */

	} else {

		ASSERT_CFP_PRIO(pkt_prio);
		ASSERT_CFP_FLOWID(cfp_flowid);

		/* Save CFP flowid into pkt */
		PKTSETCFPFLOWID(pkt, cfp_flowid); /* CFP packets have LLCSNAP */
		wlc_cfp_pkt_prepare(FD_CFP_UNIT, cfp_flowid, pkt_prio,
			pkt, pciedev->cfp_exptime);

		pktlist = &cfp_flows->pktlists[pkt_prio]; /* per priority fastpath */
	}

	/* Enqueue into CFP capable fastpath or default slowpath pktlist */
	if (pktlist->head == NULL) { /* empty list */
		cfp_flows->curr = pktlist;
		cfp_flows->pend += pktlist->pend; /* pending list(s) tracking */

		pktlist->head = pkt;
		pktlist->tail = pkt;
	} else {
		PKTSETCLINK(pktlist->tail, pkt);
		pktlist->tail = pkt;
	}

	/* tail pkt::next is not terminated, as yet */

	pktlist->pkts++;

	CFP_FLOW_STATS_INCR(pktlist->enqueue_stats);
}

/**
 * Dispatch all accumulated packets via slowpath or fastpath.
 *
 * CFP capable packets are sorted into the per prio packet list for CFP sendup.
 *
 * If all packets belong to a single priority and are CFP capable, then only a
 * single pktlist would have been constructed and cfp_pktlist_curr would be
 * pointing to this list, allowing quick sendup without needing to search
 * for non empty priority pktlists for sendup.
 *
 * If more than one pktlist was created or some packets were not CFP capable
 * then all priority packets lists that have packets accumulated are sent up
 * via the CFP fast path, and finally the non-CFP capable packets accumulated
 * in the packet chaining list are sendup via the normal slow path.
 *
 * Packet lists are ONLY maintained by priority and not by MacDA.
 */
static inline void
pciedev_dispatch_pktlists(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf)
{
	uint16 cfp_flowid;
	cfp_flows_t *cfp_flows;
	cfp_pktlist_t *pktlist;

	cfp_flowid = msgbuf->cfp_flowid;
	cfp_flows = pciedev->cfp_flows;

	if (cfp_flows->pend == 1) { /* check if only one pktlist is pending */
		pktlist = cfp_flows->curr; /* locate pktlist to sendup */

		CFP_FLOWS_TRACE(("CFP1 TX ringid<%u> cfp_flowid<%u> pend<%u> "
			" prio<%u> pkts<%u> list<%p,%p>\n",
			msgbuf->ringid, cfp_flowid, cfp_flows->pend,
			pktlist->prio, pktlist->pkts, pktlist->head, pktlist->tail));

		cfp_flows_sendup(cfp_flowid, pktlist); /* CFP fastpath sendup */

		CFP_FLOW_STATS_INCR(cfp_flows->pend_dispatch_stats);

	} else { /* more than one CFP pktlist, and/or non-CFP packet chain */

		cfp_pktlist_t *def_pktlist; /* slowpath default packet list */

		pktlist = &cfp_flows->pktlists[0];
		def_pktlist = pktlist + DNGL_SENDUP_PKTLIST_IDX;

		while (pktlist < def_pktlist) { /* traverse CFP capable packet lists */

			if (pktlist->head != NULL) { /* non-empty pktlist */

				CFP_FLOWS_TRACE(("CFP# TX ringid<%u> cfp_flowid<%u> pend<%u> "
					"prio<%u> pkts<%u> list<%p,%p>\n",
					msgbuf->ringid, cfp_flowid, cfp_flows->pend,
					pktlist->prio, pktlist->pkts, pktlist->head,
					pktlist->tail));

				cfp_flows_sendup(cfp_flowid, pktlist); /* CFP fastpath sendup */

				CFP_FLOW_STATS_INCR(cfp_flows->prio_dispatch_stats);
			}

			pktlist++; /* next prio pktlist */
		} /* while CFP capable pktlist */

		/* Now sendup non-CFP chained packets, if any */
		if (def_pktlist->head != NULL) {
			CFP_FLOWS_TRACE(("DNGL TX ringid<%u> cfp_flowid<%u> "
				"pkts<%u> list<%p,%p>\n",
				msgbuf->ringid, cfp_flowid,
				def_pktlist->pkts, def_pktlist->head, def_pktlist->tail));

			PKTSETCLINK(def_pktlist->tail, NULL); /* terminate tail pkt::next */

			dngl_sendup(pciedev->dngl, def_pktlist->head); /* slowpath sendup */

			CFP_FLOW_STATS_INCR(def_pktlist->dispatch_stats);

			/* Reset this default slowpath pktlist. */
			def_pktlist->pkts = 0;
			def_pktlist->head = NULL;

			CFP_FLOW_STATS_INCR(cfp_flows->dngl_dispatch_stats);
		}
	}

	cfp_flows->pend = 0; /* No more pending pktlists */
}

#if defined(CFP_FLOWS_STATS)
void
cfp_flows_dump(void *arg)
{
	int i;
	cfp_flows_t *cfp_flows = (cfp_flows_t*)arg;
	cfp_pktlist_t *pktlist;

	printf("CFP Flow dispatch<%u>: pend<%u> prio<%u> dngl<%u>\n"
		"\t#\tEnqueues  \tDispatches\tEnq/Disp\n",
		(cfp_flows->pend_dispatch_stats + cfp_flows->prio_dispatch_stats +
		 cfp_flows->dngl_dispatch_stats),
		cfp_flows->pend_dispatch_stats, cfp_flows->prio_dispatch_stats,
		cfp_flows->dngl_dispatch_stats);

	for (i = 0; i < CFP_PKTLISTS_TOTAL; i++) {
		pktlist = &cfp_flows->pktlists[i];
		printf("\t%d\t%10u\t%10u\t%u\n",
			i, pktlist->enqueue_stats, pktlist->dispatch_stats,
			pktlist->enqueue_stats / pktlist->dispatch_stats);
	}
}
#endif /* CFP_FLOWS_STATS */

#endif /* WLCFP */

/**
 * Called when a new so-called 'internal' 'host fetch' message is available in device memory,
 * signalling that new data to transmit to the wireless medium is now available in dongle memory.
 * Parameter 'p' points at the related 'txbuf post' message.
 */
uint16
pciedev_handle_h2d_msg_txsubmit(struct dngl_bus *pciedev, void *p,
	uint16 pktlen, msgbuf_ring_t *ring, uint16 fetch_ptr, uint32 flags)
{
	uint8 msglen = MSGBUF_LEN(ring);
	uint8 ifidx; /* used in stats update */
	uint32 cfp_enabled = 0U;
	amsdu_sup_pktlist_t *amsdu_sup_part1 = &ring->amsdu_sup_part1;

	ASSERT((pktlen % msglen) == 0);

	BCM_REFERENCE(flags);

#if defined(BCMHWA) && defined(HWA_TXPOST_BUILD)
	ASSERT(0);
#endif // endif

#ifdef WLCFP
	/* CFP can only be enabled if there is no reuse sequenced
	 * packets pending and its enabled for this particular scb
	 */
	cfp_enabled = (!FLOWRING_HAS_REUSED_SEQ_PKT(ring)) &&
		wlc_cfp_tx_enabled(FD_CFP_UNIT, ring->cfp_flowid, &pciedev->cfp_exptime);
#endif // endif

#ifdef WLSQS
	SQS_ASSERT();
#endif // endif

	/* Increment IPC Data for txsubmit */
	pciedev->metrics->num_submissions += pktlen / msglen;
	PCI_INFORM(("h2d_drbl:%d, new txsubmit:%d, tot_num_submissions:%d\n",
		pciedev->metrics->num_h2d_doorbell, pktlen/msglen,
		pciedev->metrics->num_submissions));

	while (pktlen) {

		if (ring->status & FLOW_RING_FLUSH_PENDING) {
			/* Check if there are resources to queue txstatus */
			if (IS_IOCTLREQ_PENDING(pciedev) ||
				!PCIEDEV_TXCPL_RESOURCE_AVAILABLE(pciedev)) {
				DBG_BUS_INC(pciedev, pciedev_handle_h2d_msg_txsubmit);
				PCI_TRACE(("%s: ring->status: 0x%x, pktlen: 0x%x\n",
					__FUNCTION__, ring->status, pktlen));
				return pktlen;
			} else {
				uint8 if_id; /* Interface Id in TxPost work item */
				uint32 pktid; /* host pktid in TxPost work item */
#ifdef PCIEDEV_FAST_DELETE_RING
				/**
				 * if flowring is to be deleted and fast delete is enabled
				 * then ignore the transfer completely unless fast delete
				 * is not active yet. Limit check to DELETE_ACTIVE.
				 */
				if (ring->status & FLOW_RING_FAST_DELETE_ACTIVE) {
					goto skip_update;
				}
#endif /* PCIEDEV_FAST_DELETE_RING */

				switch (MSGBUF_ITEM_TYPE(ring))
				{
					case MSGBUF_WI_WI64:
					{
						host_txbuf_post_t *wi64 = (host_txbuf_post_t *)p;
						if_id = wi64->cmn_hdr.if_id;
						pktid = ltoh32(wi64->cmn_hdr.request_id);
						break;
					}
					case MSGBUF_WI_CWI32:
					case MSGBUF_WI_CWI64: // Same layout in cwi32 and cwi64
					{
						hwa_txpost_cwi32_t *cwi32 = (hwa_txpost_cwi32_t *)p;
						if_id = BCM_GBF(cwi32->u32[1], HWA_TXPOST_IFID);
						pktid = ltoh32(cwi32->host_pktid);
						break;
					}
					default:
					{
						PCI_ERROR(("Invalid TxPost itemtype %u\n",
							MSGBUF_ITEM_TYPE(ring)));
						ASSERT(0);
						return pktlen;
					}
				}

				BCMPCIE_IPC_HPA_TEST(pciedev, pktid,
					BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_REQUEST);
				BUZZZ_KPI_PKT1(KPI_PKT_BUS_TXPOST, 2, pktid, ring->ringid);

				/* update per ifidx stats */
				ring->flow_info.pktinflight++;

				ifidx = ring->flow_info.ifindex;
				pciedev->ifidx_account[ifidx].cur_cnt++;
				pciedev->pend_user_tx_pkts++;

				if (!pciedev_update_txstatus(pciedev, BCMPCIE_PKT_FLUSH, fetch_ptr,
					ring->ringid, 0, FALSE, NULL)) {
#ifdef PCIEDEV_FAST_DELETE_RING
					if ((ring->status & FLOW_RING_DELETE_RESP_PENDING) &&
					    (pciedev->fastdeletering) &&
					    (pciedev_fastdelete_notxstatus(ring, fetch_ptr))) {
						goto skip_update;
					}
#endif /* PCIEDEV_FAST_DELETE_RING */
					PCIEDEV_QUEUE_TXSTATUS(pciedev, pktid, if_id,
						ring->ringid, BCMPCIE_PKT_FLUSH, NULL);
				}
			}
		} else if (!(ring->status & FLOW_RING_SUP_PENDING)) {
			if (amsdu_sup_part1->pkts) {
				void *lfrag, *n;

				/* Sending stored AMSDU: break_chain is FALSE. cfp_not_capable is
				 * TRUE, since is suppressed AMSDU, otherwise it wouldn't have
				 * been stored.
				 */
				lfrag = amsdu_sup_part1->head;
				FOREACH_CHAINED_PKT(lfrag, n) {
					pciedev_sendup_or_chain_lfrag(pciedev, lfrag,
						ring, FALSE, TRUE);
				}
				amsdu_sup_part1->pkts = 0;
			}
			pciedev_process_tx_post(pciedev, p, msglen, ring->ringid, fetch_ptr,
				cfp_enabled, (pktlen == msglen));
		} else {
			/* If we are already in suppress, no need to push these packets: rewind
			 * fetch_ptr and move on.
			 */
			pciedev_rewind_flow_fetch_ptr(ring, fetch_ptr);
			clrbit(ring->inprocess, MSGBUF_MODULO_IDX(ring, fetch_ptr));
		}
#ifdef PCIEDEV_FAST_DELETE_RING
skip_update:
#endif /* PCIEDEV_FAST_DELETE_RING */
		fetch_ptr = (fetch_ptr + 1) %  MSGBUF_MAX(ring);
		pktlen = pktlen - msglen;
		p = (uint8 *)p + msglen;
	}

#ifdef WLCFP
	/* Sendup all accumulated CFP capable and non-CFP capable pktlists */
	pciedev_dispatch_pktlists(pciedev, ring);
#else /* ! WLCFP */
#ifdef PKTC_TX_DONGLE
	/* We can send the chained packets here */
	if (pciedev->pkthead) {
		/* Forward the tx packets to the wireless subsystem */
		dngl_sendup(pciedev->dngl, pciedev->pkthead);
		pciedev->pkt_chain_cnt = 0;
		pciedev->pkthead = pciedev->pkttail = NULL;
	}
#endif /* PKTC_TX_DONGLE */
#endif /* ! WLCFP */

	return pktlen;
} /* pciedev_handle_h2d_msg_txsubmit */

static void
pciedev_sendup_or_chain_lfrag(struct dngl_bus *pciedev, void *lfrag, msgbuf_ring_t *flow_ring,
	bool break_chain, bool cfp_not_capable)
{
	/* Sendup 802.1x packets immediately, queue up the rest */
	if (break_chain) {
#ifdef WLCFP
		/*
		 * Prevent Out of Order issues.
		 * Sendup all accumulated CFP capable and non-CFP capable pktlists.
		 */
		pciedev_dispatch_pktlists(pciedev, flow_ring);
#endif /* WLCFP */
		PKTSETCLINK(lfrag, NULL);
		dngl_sendup(pciedev->dngl, lfrag);
		return;
	}

#ifdef WLCFP
	pciedev_classify(pciedev, flow_ring->cfp_flowid, lfrag, PKTPRIO(lfrag), cfp_not_capable);
#else /* ! WLCFP */
	BCM_REFERENCE(cfp_not_capable);
#ifdef PKTC_TX_DONGLE
	if (pciedev->pkttail) {
		PKTSETCLINK(pciedev->pkttail, lfrag);
		pciedev->pkttail = lfrag;
	} else {
		pciedev->pkthead = pciedev->pkttail = lfrag;
	}
	PKTSETCLINK(pciedev->pkttail, NULL);
	pciedev->pkt_chain_cnt++;
#else /* ! PKTC_TX_DONGLE */
	/* No packet chaining. Send it out immediately */
	PKTSETCLINK(lfrag, NULL);
	dngl_sendup(pciedev->dngl, lfrag); /* forward the tx packets to the wireless subsystem */
#endif /* ! PKTC_TX_DONGLE */
#endif /* ! WLCFP */
}

/**
 * Called to determine if current fetch pointer index belongs to a suppressed amsdu packet.
 * If this is the case, enqueues the packet to amsdu_sup_pktc.
 * When all packets that belong to the same amsdu have been queued, it will copy the result to
 * amsdu_sup_part1.
 * Parameter 'lfrag' points to the packet being processed and will be set to NULL when packets is
 *    queued on amsdu_sup_pktc.
 * Returns TRUE if packet is enqueued to amsdu_sup_pktc and still has the same sequence number.
 */
static inline bool
pciedev_process_tx_post_reuse_seq(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring,
	void **lfrag, uint16 fetch_ptr, bool last_packet)
{
	bool reuse_seq = FALSE;

	ASSERT(!(flow_ring->status & FLOW_RING_SUP_PENDING));

	/* Check for reuse seq number */
	if (isset(flow_ring->reuse_sup_seq, MSGBUF_MODULO_IDX(flow_ring, fetch_ptr))) {
		reuse_seq = TRUE;
	}

	if ((flow_ring->flow_info.flags & FLOW_RING_FLAG_PKT_REQ) || reuse_seq) {
		pciedev_push_pkttag_tlv_info(pciedev, *lfrag, flow_ring, fetch_ptr);
	}

#if defined(WLCFP) || defined(PKTC_TX_DONGLE)
	/* Suppressed AMSDU check. Store suppressed amsdu packet.
	 * If last packet of fetch is suppressed and is not last part of AMSDU
	 * then store the packet chain in flowring,
	 * It will then be picked up next round and put in
	 * front of the chain (of either cfp or pktc).
	 */
	if (reuse_seq) {
		uint16 next_fetch_ptr;
		uint16 r_seq;
		uint16 nex_r_seq;
		amsdu_sup_pktlist_t *amsdu_sup_pktc = &flow_ring->amsdu_sup_pktc;

		r_seq = flow_ring->reuse_seq_list[MSGBUF_MODULO_IDX(flow_ring, fetch_ptr)];
		if (WL_SEQ_IS_AMSDU(r_seq)) {
			next_fetch_ptr = MSGBUF_MODULO_IDX(flow_ring, fetch_ptr + 1);
			if (isset(flow_ring->reuse_sup_seq, next_fetch_ptr)) {
				nex_r_seq = flow_ring->reuse_seq_list[next_fetch_ptr];
			} else {
				nex_r_seq = 0;
			}

			PKTCENQTAIL(amsdu_sup_pktc->head, amsdu_sup_pktc->tail, *lfrag);
			amsdu_sup_pktc->pkts++;
			*lfrag = NULL;

			if (r_seq == nex_r_seq) {
				if (last_packet) {
					bcopy(amsdu_sup_pktc, &flow_ring->amsdu_sup_part1,
						sizeof(amsdu_sup_pktlist_t));
					bzero(amsdu_sup_pktc, sizeof(amsdu_sup_pktlist_t));
				}

				return TRUE;
			}
		}
	}
#endif /* WLCFP || PKTC_TX_DONGLE */

	return FALSE;
}

static uint16
pciedev_reuse_seq_amsdu_cnt(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
	uint16 sdu_count = 1;
#if defined(WLCFP) || defined(PKTC_TX_DONGLE)
	uint16 fetch_ptr = flow_ring->fetch_ptr;

	/* Suppressed AMSDU check. Count packets needed to fetch a complete amsdu packet.
	 */
	while (isset(flow_ring->reuse_sup_seq, MSGBUF_MODULO_IDX(flow_ring, fetch_ptr))) {
		uint16 next_fetch_ptr;
		uint16 r_seq;
		uint16 nex_r_seq;

		r_seq = flow_ring->reuse_seq_list[MSGBUF_MODULO_IDX(flow_ring, fetch_ptr)];
		if (WL_SEQ_IS_AMSDU(r_seq)) {
			next_fetch_ptr = MSGBUF_MODULO_IDX(flow_ring, fetch_ptr + 1);
			if (isset(flow_ring->reuse_sup_seq, next_fetch_ptr)) {
				nex_r_seq = flow_ring->reuse_seq_list[next_fetch_ptr];
			} else {
				nex_r_seq = 0;
			}

			if (r_seq == nex_r_seq) {
				sdu_count++;
			} else {
				break;
			}
		} else {
			break;
		}
		fetch_ptr = next_fetch_ptr;
	}
#endif /* WLCFP || PKTC_TX_DONGLE */

	return sdu_count;
}

/* Returns TRUE if pkt da (last params) doesn't match with flowring da */
static INLINE bool
pciedev_flow_mismatch(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring, uchar *da)
{
	if (flow_ring->flow_info.iftype_ap &&
		(!(ETHER_ISNULLDEST(da) || ETHER_ISMULTI(da))) &&
		(eacmp(da, flow_ring->flow_info.da))) {
	    return TRUE;
	}

	return FALSE;
}

/**
 * Called when new transmit payload data is available in dongle memory, accompanied by a message
 * (in parameter 'p') that originated from firmware. This means that payload data can be forwarded
 * towards the wireless subsystem.
 *
 * 1. Decode message in 'p'
 * 2. Get an lfrag from the pool, this lfrag will be fed to the wireless subsystem
 * 3. Update lfrag details
 * 4. Forward lfrag to wl layer
 *
 * @param[in/out] pciedev   handle to pciedev subsystem
 * @param[in]     p         pointer to host message
 * @param[in]     msglen    unused parameter
 * @param[in]     fetch_ptr context to be stored in packet, so it can be determined later on from
 *                          which element in the flow ring the packet was fetched (suppress related)
 */

/* Need 202 bytes of headroom for TXOFF, 22 bytes for amsdu path */
#define HEADROOM_TXLBUF         224

static void
pciedev_process_tx_post(struct dngl_bus *pciedev, void *p, uint16 msglen,
	uint16 ringid, uint16 fetch_ptr, uint32 cfp_enabled, bool last_packet)
{
	msgbuf_ring_t *flow_ring;
	void *lfrag, *lfrag_amsduc;
	uint16 headroom = HEADROOM_TXLBUF;
	/* fields from txpost */
	uint32 pktid;
	uint8 priority, flags, exempt, if_id;
	struct ether_header *eh;
	uint16 data_buf_hlen;
	uint32 data_buf_haddr64_lo, data_buf_haddr64_hi;
#if defined(WLCFP) || defined(PKTC_TX_DONGLE)
	uint16 ether_type;
#endif // endif
	bool cfp_not_capable = TRUE;
	bool chainable = FALSE;
	amsdu_sup_pktlist_t *amsdu_sup_pktc;

#ifdef WLSQS
	SQS_ASSERT();
#endif // endif

	PCI_TRACE(("pciedev_process_tx_post: msglen=%d, ringid=%d, fetch_ptr=%d\n",
			msglen, ringid, fetch_ptr));
	flow_ring = pciedev->h2d_submitring_ptr[ringid];
	ASSERT(flow_ring);

	if (flow_ring == NULL) {
		OSL_SYS_HALT();
	}

#if defined(BCMHWA) && defined(HWA_TXPOST_BUILD)
	ASSERT(0);
	OSL_SYS_HALT();
#endif // endif

	amsdu_sup_pktc = &flow_ring->amsdu_sup_pktc;

	/* nsegs supported is 1. */

	/* Allocate a lbuf_frag  with HEADROOM_TXLBUF + ETHER_HDR_LEN space */
#ifndef BCMFRAGPOOL
	lfrag = PKTGETLF(pciedev->osh, headroom + ETHER_HDR_LEN, TRUE, lbuf_frag);
	if (lfrag == NULL) {
		/* No free packets in heap. Just drop this packet. */
		PCI_TRACE(("pciedev_process_tx_post: No free packets"
		"in the pool. Dropping packets here\n"));
		pciedev->dropped_txpkts ++;
		return;
	}
#else
	lfrag = LFRAG_POOL_GET(pciedev);
#ifdef BCMRESVFRAGPOOL
	if (BCMRESVFRAGPOOL_ENAB() && lfrag == NULL) {
		lfrag = rsvpool_get(pciedev->pktpool_resv_lfrag);
	}
#endif // endif
	if (lfrag == NULL) {
		/* No free packets in the pool. Just drop this packet. */
		PCI_ERROR(("pciedev_process_tx_post: No free packets in the pool. "
			"Dropping packets here\n"));
		flow_ring->flow_info.pktinflight++;
		pciedev->ifidx_account[flow_ring->flow_info.ifindex].cur_cnt++;
		pciedev->pend_user_tx_pkts++;
		pciedev_update_txstatus(pciedev, WLFC_CTL_PKTFLAG_WLSUPPRESS,
			fetch_ptr, ringid, 0, FALSE, NULL);
		pciedev->dropped_txpkts++;
		return;
	}
#ifdef BCM_DHDHDR
	headroom = PKTLEN(pciedev->osh, lfrag) - ETHER_HDR_LEN;
#endif /* BCM_DHDHDR */
#endif /* BCMFRAGPOOL */

	/* Unpack the TxPost work item */
	switch (MSGBUF_ITEM_TYPE(flow_ring))
	{
		case MSGBUF_WI_WI64:
		{
			host_txbuf_post_t *wi64 = (host_txbuf_post_t *)p;
			eh = (struct ether_header *)wi64->txhdr;
			pktid = ltoh32(wi64->cmn_hdr.request_id);
			priority = BCM_GBF(wi64->flags, BCMPCIE_PKT_FLAGS_PRIO);
			flags = wi64->flags;
			if_id = wi64->cmn_hdr.if_id;
			data_buf_hlen = ltoh16(wi64->data_len);
			data_buf_haddr64_lo = HADDR64_LO_LTOH(wi64->data_buf_haddr64);
			data_buf_haddr64_hi = HADDR64_HI_LTOH(wi64->data_buf_haddr64);
			break;
		}

		case MSGBUF_WI_CWI32:
		{
			hwa_txpost_cwi32_t *cwi32 = (hwa_txpost_cwi32_t *)p;
			eh = (struct ether_header *)cwi32->eth_sada.u8;
			pktid = ltoh32(cwi32->host_pktid);
			priority = BCM_GBF(cwi32->u32[1], HWA_TXPOST_PRIO);
			flags = BCM_GBF(cwi32->u32[1], HWA_TXPOST_FLAGS);
			if_id = BCM_GBF(cwi32->u32[1], HWA_TXPOST_IFID);
			data_buf_hlen = ltoh16(cwi32->data_buf_hlen);
			data_buf_haddr64_lo = ltoh32(cwi32->data_buf_haddr32);
			data_buf_haddr64_hi = pciedev->host_physaddrhi; // fixed hi addr
			break;
		}

		case MSGBUF_WI_CWI64:
		{
			hwa_txpost_cwi64_t *cwi64 = (hwa_txpost_cwi64_t *)p;
			eh = (struct ether_header *)cwi64->eth_sada.u8;
			pktid = ltoh32(cwi64->host_pktid);
			priority = BCM_GBF(cwi64->u32[1], HWA_TXPOST_PRIO);
			flags = BCM_GBF(cwi64->u32[1], HWA_TXPOST_FLAGS);
			if_id = BCM_GBF(cwi64->u32[1], HWA_TXPOST_IFID);
			data_buf_hlen = ltoh16(cwi64->data_buf_hlen);
			data_buf_haddr64_lo = HADDR64_LO_LTOH(cwi64->data_buf_haddr64);
			data_buf_haddr64_hi = HADDR64_HI_LTOH(cwi64->data_buf_haddr64);
			break;
		}

		default:
		{
			PCI_ERROR(("Invalid TxPost item_type %u\n", MSGBUF_ITEM_TYPE(flow_ring)));
			ASSERT(0);
			return;
		}
	} /* switch */

	BCMPCIE_IPC_HPA_TEST(pciedev, pktid,
		BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_REQUEST);
	BUZZZ_KPI_PKT1(KPI_PKT_BUS_TXPOST, 2, pktid, flow_ring->ringid);

	exempt = (flags >> BCMPCIE_PKT_FLAGS_FRAME_EXEMPT_SHIFT) &
		BCMPCIE_PKT_FLAGS_FRAME_EXEMPT_MASK;

#if defined(WLCFP) || defined(PKTC_TX_DONGLE)
	ether_type = ntoh16(eh->ether_type);

	/* Allow only IP/Unicast packets to go through chained TX path */
	if ((!ETHER_ISNULLDEST(eh->ether_dhost)) && (!ETHER_ISMULTI(eh->ether_dhost)) &&
		((ether_type == ETHER_TYPE_IP) || (ether_type == ETHER_TYPE_IPV6))) {
		chainable = TRUE;
	}
#endif /* !WLCFP || PKTC_TX_DONGLE */

	/* Push headroom, copy ether header to lfrag and set length */
#ifdef WLCFP
	/* Test whether packet is candidate for fastpath based on CFP TCB state */
	/* Re-use seq pkts not allowed for CFP */
	cfp_not_capable = ((cfp_enabled == 0U) |	/* Given SCB is CFP capable */
			    ((flow_ring->tcb_state == NULL) ||
			     CFP_NOT_CAPABLE(flow_ring->tcb_state, priority)));

	/* If llcsnap already exists "or" CFP not_capable, do not insert llcsnap */
	if ((!chainable) | (cfp_not_capable)) {
		PKTPULL(pciedev->osh, lfrag, headroom);	/* Push headroom */
		ehcopy32(eh, PKTDATA(pciedev->osh, lfrag)); /* Ether hdr */
		PKTSETLEN(pciedev->osh, lfrag, ETHER_HDR_LEN);	/* Set Len */
	} else {
#ifdef BCM_DHDHDR
		/* ==================== DHDHDR + CFP ==================================
		 *
		 * 1. LLC SNAP inserted by host, So adjust frag_data_low & data_len here
		 * 2. Copy ether header to local lfrag. Needed for dot11 header fixup.
		 * 3. Non AMSDU path [wlc_cfp_ampdu_prepare]
		 * 	3.a Swap d11 buffer
		 * 	3.b Copy SA/DA for dot11 header fixup from local lfrag
		 * 	3.c PKTPULL 14 bytes to release ether header
		 * 4. AMSDU path
		 * 	4.a subframe header inserted by host.
		 * 	4.b Drop d3 buffer for subframes [wlc_cfp_ampdu_release]
		 * 	4.c Adjust Frag addr & len for head frame [wlc_tx_adjust_hostfrag]
		 * 	4.d Swap D11 buffer [wlc_cfp_ampdu_prepare]
		 * 	4.e Drop 14 bytes ether header from head frame [wlc_cfp_ampdu_prepare]
		 */
		data_buf_haddr64_lo = data_buf_haddr64_lo - DOT11_LLC_SNAP_HDR_LEN;
		data_buf_hlen = data_buf_hlen + DOT11_LLC_SNAP_HDR_LEN;
		ASSERT(PKTLEN(pciedev->osh, lfrag) == ETHER_HDR_LEN);

		ehcopy32(eh, PKTDATA(pciedev->osh, lfrag)); /* Copy ether hdr */
#else
		/*
		 * Copy Ethernet header alongwith LLCSNAP.
		 * Handcoded construction of dot3_mac_llc_snap_header.
		 *    6B 802.3 MacDA
		 *    6B 802.3 MacSA
		 *    2B Ethernet Data Length (includes 8B LLCSNAP header length)
		 *    2B LLC DSAP SSAP
		 *    1B LLC CTL
		 *    3B SNAP OUI
		 *    2B EtherType
		 */
		uint8 *s, *d;
		uint16 tot_len;

		/* Prepare for CFP path by composing a dot3_mac_llc_snap_header */
		s = (uint8 *)eh;
		d = PKTPULL(pciedev->osh, lfrag, headroom - DOT11_LLC_SNAP_HDR_LEN);

		/* Fetch ethernet payload length in host */
		tot_len = DOT11_LLC_SNAP_HDR_LEN + data_buf_hlen; /* exclude ETHER_HDR_LEN */

		/* 802.3 MacDA MacSA: 12B copy, using three 32b aligned 4B copy */
		((uint32 *)(d))[0] = ((const uint32 *)(s))[0];
		((uint32 *)(d))[1] = ((const uint32 *)(s))[1];
		((uint32 *)(d))[2] = ((const uint32 *)(s))[2];

		/* ethernet payload length: 2B */
		((uint16 *)(d))[6] = HTON16(tot_len);

		/* Reference bcm_proto.h: static const uint8 llc_snap_hdr[] */
		/* dsap = 0xaa ssap = 0xaa: 2B copy */
		((uint16 *)(d))[7] = (uint16)0xAAAA; /* no need for htons16 */

		/* LLC ctl = 0x03, out[3] = { 0x00 0x00 0x00}: 32b aligned 4B copy */
		((uint32 *)(d))[4] = hton32(_ctl_oui3.u32);

		/* Set OUI[2] for Bridge Tunnel */
		if (IS_BRIDGE_TUNNEL(ether_type)) {
			((struct dot3_mac_llc_snap_header*)(d))->oui[2] = BRIDGE_TUNNEL_OUI2;
		}

		/* Original Ether Type following LLCSNAP */
		((uint16 *)(d))[10] = ((const uint16 *)(s))[6];

		PKTSETLEN(pciedev->osh, lfrag, ETHER_HDR_LEN + DOT11_LLC_SNAP_HDR_LEN);
#endif /* BCM_DHDHDR */
	}

#else /* ! WLCFP */
	PKTPULL(pciedev->osh, lfrag, headroom); /* Push headroom */
	ehcopy32(eh, PKTDATA(pciedev->osh, lfrag)); /* Copy ether hdr */
	PKTSETLEN(pciedev->osh, lfrag, ETHER_HDR_LEN);	/* Set Len */
#endif /* ! WLCFP */

	/* Set frag params */
	PKTSETEXEMPT(lfrag, exempt);
	PKTSETFRAGPKTID(pciedev->osh, lfrag, pktid);
	PKTSETFRAGTOTNUM(pciedev->osh, lfrag, 1);
	PKTSETPRIO(lfrag, priority);
	PKTSETIFINDEX(pciedev->osh, lfrag, if_id);
	PKTSETFRAGDATA_HI(pciedev->osh, lfrag, 1, (data_buf_haddr64_hi | 0x80000000));
	PKTSETFRAGDATA_LO(pciedev->osh, lfrag, 1, data_buf_haddr64_lo);
	PKTSETFRAGLEN(pciedev->osh, lfrag, 1, data_buf_hlen);

	/* Set tot len and tot fragment count */
	PKTSETFRAGTOTLEN(pciedev->osh, lfrag, data_buf_hlen);
	PKTSETFRAGFLOWRINGID(pciedev->osh, lfrag, ringid);
	PKTFRAGSETRINGINDEX(pciedev->osh, lfrag, fetch_ptr);

	flow_ring->flow_info.pktinflight++;
	pciedev->pend_user_tx_pkts++;

	/* per ifidx stats */
	pciedev->ifidx_account[flow_ring->flow_info.ifindex].cur_cnt++;

	/* Metadata is handled, doesn't matter if the host has given buffer or not */
	PKTSETHASMETADATA(pciedev->osh, (struct lbuf *)lfrag);

	PCI_TRACE(("frag tot len %d, totlen %d, lfrag is %x\n",
		PKTFRAGTOTLEN(pciedev->osh, lfrag),
		pkttotlen(pciedev->osh, lfrag), (uint32)lfrag));

	/* Ensure pkt are enqueued to right flowring */
	if (pciedev->lpback_test_mode ||
		pciedev_flow_mismatch(pciedev, flow_ring, eh->ether_dhost)) {
		pciedev->lpback_test_drops++;
		PKTFREE(pciedev->osh, lfrag, TRUE);
		return;
	}

	if (pciedev_process_tx_post_reuse_seq(pciedev, flow_ring, &lfrag, fetch_ptr, last_packet)) {
		return;
	}

	if (amsdu_sup_pktc->pkts) {
		void *n;

		/* Sending stored AMSDU: break_chain is FALSE. cfp_not_capable is
		 * TRUE, since is suppressed AMSDU, otherwise it wouldn't have
		 * been stored.
		 */
		lfrag_amsduc = amsdu_sup_pktc->head;
		FOREACH_CHAINED_PKT(lfrag_amsduc, n) {
			pciedev_sendup_or_chain_lfrag(pciedev, lfrag_amsduc,
				flow_ring, FALSE, TRUE);
		}
		bzero(amsdu_sup_pktc, sizeof(amsdu_sup_pktlist_t));
	}
	if (lfrag != NULL) {
		pciedev_sendup_or_chain_lfrag(pciedev, lfrag, flow_ring,
			!chainable, cfp_not_capable);
	}
} /* pciedev_process_tx_post */

/**
 * After the host notified the device that new message(s) or non inline ioctl requests are available
 * in host memory, the device needs to pull these message(s) into local memory. This function kicks
 * off the DMA transfer for that.
 */
static void pciedev_h2dmsgbuf_dma(struct dngl_bus *pciedev, dma64addr_t src,
	uint16 src_len, uint8 *dst, msgbuf_ring_t *ring, uint8 msgtype, uint8 dma_ch)
{
	m2m_desc_t *desc = pciedev->m2m_desc;
	dma64addr_t addr = { .low_addr = 0, .high_addr = 0 };
	uint16 dma_len, rx_len;
#ifdef PCIE_INDUCE_ERRS
	uint32 bar0_addr;
#endif // endif
	hnddma_t *di = pciedev->h2d_di[dma_ch];

	/* CRWLPCIEGEN2-97 */
	if (src_len  <  PCIE_MEM2MEM_DMA_MIN_LEN)
		dma_len = PCIE_MEM2MEM_DMA_MIN_LEN;
	else
		dma_len = src_len;

	rx_len = dma_len;

#ifdef PCIE_PWRMGMT_CHECK
	if (pciedev_ds_in_host_sleep(pciedev)) {
		PCI_TRACE(("pciedev_h2dmsgbuf_dma:  IN D3 Suspend!\n"));
		ASSERT(0);

		return;
	}
#endif /* PCIE_PWRMGMT_CHECK */

	if (!pciedev->force_ht_on) {
		PCI_TRACE(("%s: requesting force HT for this core\n", __FUNCTION__));
		pciedev->force_ht_on = TRUE;
		pciedev_manage_h2d_dma_clocks(pciedev);
	}

	INIT_M2M_DESC(desc);

	PHYSADDR64LOSET(addr, (uint32)dst);
	SETUP_RX_DESC(desc, addr, rx_len);

	/* Src_len can't really be less than 8...should we check */
	/* TX descriptor */
#ifdef PCIE_INDUCE_ERRS
	if (pciedev->unsupport_req_err == TRUE) {
		/* Inducing UR error by programming inbound dma tx descriptor buf addr with BAR0 */
		bar0_addr = R_REG_CFG(pciedev, PCIECFGREG_BASEADDR0);
		src.loaddr = 0x00000000;
		src.hiaddr = bar0_addr;
		pciedev->unsupport_req_err = FALSE;
	}
#endif // endif

	SETUP_TX_DESC(desc, src, dma_len);

	if (DMA_M2M_SUBMIT(di, desc)) {
		DBG_BUS_INC(pciedev, pciedev_h2dmsgbuf_dma);
		PCI_ERROR(("pciedev_h2dmsgbuf_dma : dma fill failed  \n"));
		goto exit;
	}

	/* Queue up dma info */
	/* since we are using dummy rxoffset region, length is not obtained from rxoffset */
	pciedev_enqueue_dma_info(pciedev->htod_dma_q[dma_ch], src_len, msgtype, 0,
			ring, (uint32)dst, src.loaddr);

exit:
	return;
} /* pciedev_h2dmsgbuf_dma */

/**
 * Processing for d2h dma completion
 * Free up the packet if its payload or update circular buf pointers
 */
static uint8
pciedev_handle_d2h_dmacomplete(struct dngl_bus *pciedev, void *buf, uint8 dma_ch)
{
	uint8 ignore_cnt = 0;
	dma_item_info_t *item;

	/* Retrieve queued up dma info item */
	item = pciedev_dequeue_dma_info(pciedev->dtoh_dma_q[dma_ch]);
	if (item == NULL) {
		PCI_ERROR(("No items to dequeue from dtoh_dma_q chan: %d\n", dma_ch));
		ASSERT(0);
		return ignore_cnt;
	}

#if defined(BCM_BUZZZ_STREAMING_BUILD)
	if (item->msg_type == MSG_TYPE_BUZZZ_STREAM) {
		bcm_buzzz_d2h_done(item->len, item->flags, (uint32)item->ring);
		return 0;
	}
#endif /* BCM_BUZZZ_STREAMING_BUILD */

#if defined(MSGTRACE) || defined(LOGTRACE)
	if (MESSAGE_PAYLOAD(item->msg_type)) {
		if (item->msg_type == MSG_TYPE_EVENT_PYLD) {
			bcm_event_t *bcm_hdr;

			bcm_hdr = (bcm_event_t*)PKTDATA(pciedev->osh, buf);
			if (bcm_hdr->event.event_type == hton32(WLC_E_TRACE)) {
					msgtrace_hdr_t *trace_msg;

					trace_msg = (msgtrace_hdr_t*)&bcm_hdr[1];
#ifdef LOGTRACE
					if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_LOG) {
						logtrace_sent();
					}
#endif // endif
#ifdef MSGTRACE
					if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_MSG) {
						msgtrace_sent();
					}
#endif // endif
			}
		}
	}

#endif /* defined(MSGTRACE) || defined(LOGTRACE) */
	if (MESSAGE_PAYLOAD(item->msg_type)) {
		/* Payload */
#ifdef HMAPTEST
		if (item->msg_type == MSG_TYPE_HMAPTEST_PYLD) {
			pciedev_hmaptest_m2mwrite_dmaxfer_done(pciedev, buf);
		} else
#endif /* HMAPTEST */
#ifdef PCIE_DMAXFER_LOOPBACK
		if (item->msg_type == MSG_TYPE_LPBK_DMAXFER_PYLD) {
			pciedev_process_do_dest_lpbk_dmaxfer_done(pciedev, buf);
		} else
#endif /* PCIE_DMAXFER_LOOPBACK */
		/* ifdef PCIE_DMA_INDEX */
		/* If write index of completions are updated generate doorbell */
		if (item->msg_type == MSG_TYPE_INDX_UPDATE) {
			ignore_cnt++;
#if defined(PCIE_D2H_DOORBELL_RINGER)
			if (item->ring) {
				uint32 index;
				d2h_doorbell_ringer_t *ringer;
				index = item->ring->ringid - BCMPCIE_H2D_COMMON_MSGRINGS;
				ringer = &pciedev->d2h_doorbell_ringer[index];
				ringer->db_fn(pciedev, ringer->db_info.value,
					ringer->db_info.haddr.low);
			}
			else
			/* could be multiple msgbufs ... wake DHD and let DHD figure */
#endif /* PCIE_D2H_DOORBELL_RINGER */
			{
#ifdef BCMPCIE_D2H_MSI
				if (PCIE_MSI_ENAB(pciedev->d2h_msi_info)) {
					pciedev_generate_msi_intr(pciedev,
						((msgbuf_ring_t *)item->ring)->msi_vector_offset);
				} else
#endif /* BCMPCIE_D2H_MSI */
				{
					pciedev_generate_host_db_intr(pciedev,
						PCIE_D2H_DB0_VAL, PCIE_DB_DEV2HOST_0);
				}
			}
		} else
		/* endif of PCIE_DMA_INDEX */
		if (item->msg_type != MSG_TYPE_IOCT_PYLD) {
			/* IOCTL Payload free is handled with the IOCTL completion message */
			PKTFREE(pciedev->osh, buf, FALSE);
		}
	} else { /* control message was received from host */
		PCI_TRACE(("D2H DMA done for item %d\n", item->msg_type));
		/* free local message space */
		pciedev_ring_update_writeptr(item->ring, item->len);
		if (item->flags & PCIEDEV_INTERNAL_SENT_D2H_PHASE) {
			ignore_cnt++;
			pciedev_lclpool_free_lclbuf(item->ring,
			                            (uint8 *)buf - sizeof(cmn_msg_hdr_t));
		} else {
			pciedev_lclpool_free_lclbuf(item->ring, buf);
		}
		if (item->msg_type == MSG_TYPE_IOCTL_CMPLT) {
			pciedev_process_ioctl_done(pciedev);
		}
	}
	return ignore_cnt;
} /* pciedev_handle_d2h_dmacomplete */

bool
pciedev_handle_d2h_dma(struct dngl_bus *pciedev, uint8 dma_ch)
{
	void * prev;
	uint8 ignore_txd;
	hnddma_t *di = pciedev->d2h_di[dma_ch];

	while ((prev = dma_getnexttxp(di, HNDDMA_RANGE_TRANSMITTED))) {
		ignore_txd = pciedev_handle_d2h_dmacomplete(pciedev, prev, dma_ch);
		if (ignore_txd > 0) {
			/* to support phase bit for msgbufs same buffer was split up into 2 */
			while (ignore_txd--) {
				prev = dma_getnexttxp(di, HNDDMA_RANGE_TRANSMITTED);
			}
		}
	}

	/* free up the rx descriptors */
	dma_clearrxp(di);

	pciedev_queue_d2h_req_send(pciedev);
	return FALSE;
}

void
pciedev_handle_d2h_dma_error(struct dngl_bus *pciedev, uint8 dmach, uint32 d2hintstatus)
{
	dma_item_info_t *item;

	/* Retrieve queued up dma info item */
	item = pciedev_dequeue_dma_info(pciedev->dtoh_dma_q[dmach]);
	if (item == NULL) {
		PCI_ERROR(("No items to dequeue from dtoh_dma_q chan: %d\n", dmach));
		ASSERT(0);
	}
}

/**
 * The PCIe core receives an h2d DMA interrupt when a message has been copied from host memory into
 * a so-called 'local buffer' (lclbuf) in device memory. The firmware now has to decode the host
 * message and take action on it. Has to decode message or forward tx payload towards antenna.
 * 1. get pkt address from dma module
 * 2. Decode message
 * 3. forward tx payload towards antenna
 */
bool
pciedev_handle_h2d_dma(struct dngl_bus *pciedev, uint8 dma_ch)
{
	void *rxpkt;
	msgbuf_ring_t *ring;
	uint16 dmalen;
	uint8 msgtype;
	hnddma_t *di = pciedev->h2d_di[dma_ch];
	dma_item_info_t *item;

	PCI_TRACE(("handle the h2d interrupt\n"));

	while (1) {
		/* Rx offset Pkt */
		rxpkt = dma_getnextrxp(di, FALSE);
		dma_getnexttxp(di, HNDDMA_RANGE_TRANSMITTED);
		if (rxpkt == NULL)
			break;

		/* retrieve queued up dma info item */
		item = pciedev_dequeue_dma_info(pciedev->htod_dma_q[dma_ch]);
		if (item == NULL) {
			PCI_ERROR(("No items to dequeue from htod_dma_q chan: %d\n", dma_ch));
			ASSERT(0);
			break;
		}

		/* retrieve back details from dma info item */
		msgtype = item->msg_type;
		dmalen = item->len;
		ring = item->ring;

		/* internal payload frames */
		if (msgtype & MSG_TYPE_INTERNAL_USE_START) {
			/* txbufpost / payload to forward towards antenna */
			pciedev_handle_h2d_pyld(pciedev, msgtype, rxpkt, dmalen, ring, dma_ch);
			continue;
		} else {
			ASSERT(ring);
			/*
			 * Invokes message handler specific for the ring used. e.g.
			 * pciedev_handle_h2d_msg_ctrlsubmit(), or pciedev_handle_h2d_msg_rxsubmit()
			 */
			(ring->process_rtn)(pciedev, rxpkt, dmalen, ring);

			/* Update read pointer */
			pciedev_ring_update_readptr(pciedev, ring, dmalen);

			/* free up local message space if inuse pool is not used */
			if (!INUSEPOOL(ring)) {
				pciedev_lclpool_free_lclbuf(ring, rxpkt);
			}
			pciedev->msg_pending--;
		}
	}

	/* Release PCIe DMA tx descriptors so they can be reused */
	dma_cleartxp(di);

	/* see if there is a pending event packet waiting for host buffer */
	pciedev_queue_d2h_req_send(pciedev);

	/* Call the generic HNDRTE layer callback signalling that bus layer
	 * DMA descrs were freed / something was dequeued from the bus layer DMA queue
	 */
	hnd_dmadesc_avail_cb();

	/* There might be new messages in the circular buffer. Schedule DMA for those too. */
	pciedev_msgbuf_intr_process(pciedev);

	if (h2d_dma_not_pending(pciedev, dma_ch) &&
#ifdef PCIE_DMAXFER_LOOPBACK
	    pciedev->dmaxfer_loopback->lpbk_dmaxfer_fetch_pend == 0 &&
	    pciedev->dmaxfer_loopback->lpbk_dmaxfer_push_pend == 0 &&
#endif /* PCIE_DMAXFER_LOOPBACK */
	    1) {
#if defined(BCMPCIE_IDMA)
		if (PCIE_IDMA_ACTIVE(pciedev)) {
			if (pciedev->idma_dmaxfer_active == TRUE) {
				return FALSE;
			}
		}
#endif /* BCMPCIE_IDMA */
		if (pciedev->force_ht_on) {
			PCI_TRACE(("%s: no more H2D DMA, so no more force HT\n", __FUNCTION__));
			pciedev->force_ht_on = FALSE;
			pciedev_manage_h2d_dma_clocks(pciedev);
		}
	}

	return FALSE;
} /* pciedev_handle_h2d_dma */

/** Transmit / PropTx related */
void pciedev_upd_flr_port_handle(struct dngl_bus * pciedev, uint8 handle, bool open, uint8 ps)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	uint32  time_now = OSL_SYSUPTIME();

	/* loop through nodes, weighted ordered queue implementation */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (flow_ring->handle != handle) {
				cur = dll_next_p(cur);
				continue;
			}
			if (!open) {
				flow_ring->status |= FLOW_RING_PORT_CLOSED;
				PCIEDEV_FLOW_PORT_CLOSE_TS(flow_ring, time_now);
				if (ps == WLFC_MAC_OPEN_CLOSE_FROM_PS &&
					(flow_ring->flow_info.pktinflight > 0 ||
					flow_ring->fetch_pending > 0)) {
					flow_ring->status |= FLOW_RING_SUP_PENDING;
					/* If there was a partial fetched AMSDU, clean that up. */
					pciedev_flush_cached_amsdu_frag(pciedev, flow_ring);
				}
			} else {
				flow_ring->status &= ~FLOW_RING_PORT_CLOSED;
				PCIEDEV_FLOW_PORT_OPEN_TS(flow_ring, time_now);
				/*
				 * When the port opens, we need to trigger
				 * fetch once again.
				 */
#if defined(WLSQS)
				/* If SQS enabled, do not stride resume from this path.
				 * ASSERT may happened if STA is doing wlc_apps_scb_ps_off().
				 */
				pciedev_schedule_sqs_timer(pciedev);
#else
				pciedev_schedule_flow_ring_read_buffer(pciedev);
#endif /* !WLSQS */
			}
			flow_ring->flow_info.flags &= ~FLOW_RING_FLAG_LAST_TIM;

			/* next node */
			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}

#ifdef WLSQS
void
pciedev_sqs_stride_resume(struct dngl_bus *pciedev, uint8 handle)
{
	pciedev_schedule_sqs_timer(pciedev);
}
#endif /* WLSQS */

void
pciedev_upd_flr_flowid_state(struct dngl_bus *pciedev, uint16 flowid, bool open)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	uint32  time_now = OSL_SYSUPTIME();

	/* loop through nodes, weighted ordered queue implementation */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if (flow_ring->ringid == flowid) {
				if (!open) {
					flow_ring->status |= FLOW_RING_PORT_CLOSED;
					PCIEDEV_FLOW_PORT_CLOSE_TS(flow_ring, time_now);
				} else {
					flow_ring->status &= ~FLOW_RING_PORT_CLOSED;
					PCIEDEV_FLOW_PORT_OPEN_TS(flow_ring, time_now);
					/*
					 * When the port opens, we need to trigger
					 * fetch once again.
					 */
#if defined(WLSQS)
					pciedev_schedule_sqs_timer(pciedev);
#else
					pciedev_schedule_flow_ring_read_buffer(pciedev);
#endif /* !WLSQS */
				}
				return;
			}
			/* next node */
			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}

/** Transmit / PropTx related */
void pciedev_upd_flr_if_state(struct dngl_bus * pciedev, uint8 ifindex, bool open)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	uint32  time_now = OSL_SYSUPTIME();

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			/* XXX There could be a case that it has different
			 * handle even it has same ifindex.EX)TDLS
			 * It is needed to distinguish between them.
			 * handle value of STA and GC always zero
			 */
			if ((flow_ring->flow_info.ifindex != ifindex) ||
					(flow_ring->handle != 0xff)) {
				cur = dll_next_p(cur);
				continue;
			}
			if (!open) {
				flow_ring->status |= FLOW_RING_IF_CLOSED;
				PCIEDEV_FLOW_PORT_CLOSE_TS(flow_ring, time_now);
			} else {
				flow_ring->status &= ~FLOW_RING_IF_CLOSED;
				PCIEDEV_FLOW_PORT_OPEN_TS(flow_ring, time_now);
				/*
				 * When the port opens, we need to trigger
				 * fetch once again.
				 */
#if defined(WLSQS)
				pciedev_schedule_sqs_timer(pciedev);
#else
				pciedev_schedule_flow_ring_read_buffer(pciedev);
#endif /* !WLSQS */
			}
			/* next node */
			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}

/** Transmit / PropTx related */
void pciedev_upd_flr_tid_state(struct dngl_bus * pciedev, uint8 tid, bool open)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	uint32  time_now = OSL_SYSUPTIME();

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if (flow_ring->flow_info.tid_ac != tid) {
				cur = dll_next_p(cur);
				continue;
			}
			if (!open) {
				flow_ring->status |= FLOW_RING_PORT_CLOSED;
				PCIEDEV_FLOW_PORT_CLOSE_TS(flow_ring, time_now);
			} else {
				flow_ring->status &= ~FLOW_RING_PORT_CLOSED;
				PCIEDEV_FLOW_PORT_OPEN_TS(flow_ring, time_now);
				/*
				 * When the port opens, we need to trigger
				 * fetch once again.
				 */
#if defined(WLSQS)
				pciedev_schedule_sqs_timer(pciedev);
#else
				pciedev_schedule_flow_ring_read_buffer(pciedev);
#endif /* !WLSQS */
			}
			/* next node */
			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}

/** Transmit / PropTx related */
void pciedev_upd_flr_hanlde_map(struct dngl_bus * pciedev, uint8 handle, uint8 ifindex,
	bool add, uint8 *addr)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if ((flow_ring->flow_info.ifindex != ifindex) ||
				(memcmp(flow_ring->flow_info.da, addr, ETHER_ADDR_LEN))) {
				cur = dll_next_p(cur);
				continue;
			}

			if (add)
				flow_ring->handle = handle;
			else
				flow_ring->handle = 0xff;

			/* Next node */
			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while */
}

/* Clear the pkt pending reaqest flag */
void pciedev_update_flr_flags(struct dngl_bus * pciedev, uint8 *addr, uint8 flags)
{
	msgbuf_ring_t *flow_ring;
	struct dll *cur, *prio;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (memcmp(flow_ring->flow_info.da, addr,
				ETHER_ADDR_LEN)) {
				cur = dll_next_p(cur);
				continue;
			}

			/* clear the flags of all the flowrings corresponding to MAC address */
			flow_ring->flow_info.flags = flags;

			/* Next node */
			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}

/* delete the supression data allocated in the flowring */
void pciedev_clear_flr_supr_info(struct dngl_bus * pciedev, uint8 *addr, uint8 tid)
{
	msgbuf_ring_t *flow_ring;
	struct dll *cur, *prio;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if ((flow_ring->flow_info.tid_ac != tid) ||
				memcmp(flow_ring->flow_info.da, addr, ETHER_ADDR_LEN)) {
				cur = dll_next_p(cur);
				continue;
			}
			if (flow_ring->reuse_seq_list) {
				flow_ring->ring_ageing_info.sup_cnt = 0;
				pciedev_free_reuse_seq_list(pciedev, flow_ring);
			}
			pciedev_flush_cached_amsdu_frag(pciedev, flow_ring);
			break;
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */
}
#ifdef WLSQS
/* Convert tid bintmap to ac bitmap */
static uint8
pciedev_ac2tid_bitmap(uint8 ac_bitmap)
{
	uint8 tid_bitmap = 0;

	if (AC_BITMAP_TST(ac_bitmap, AC_BE))
		tid_bitmap |= TID_BMP_AC_BE;
	if (AC_BITMAP_TST(ac_bitmap, AC_BK))
		tid_bitmap |= TID_BMP_AC_BK;
	if (AC_BITMAP_TST(ac_bitmap, AC_VI))
		tid_bitmap |= TID_BMP_AC_VI;
	if (AC_BITMAP_TST(ac_bitmap, AC_VO))
		tid_bitmap |= TID_BMP_AC_VO;

	return tid_bitmap;
}
#endif /* WLSQS */

/** Related to power save mode for softAP in PropTx, processing WLFC_CTL_TYPE_MAC_REQUEST_PACKET */
void pciedev_process_reqst_packet(struct dngl_bus * pciedev,
	uint8 handle, uint8 ac_bitmap, uint8 count)
{
	int i;
	uint16 prev_maxpktcnt;
	msgbuf_ring_t	*flow_ring;
	msgbuf_ring_t	*fptr[PCIE_MAX_TID_COUNT];
	struct dll * cur, * prio;
	uint8 prio_bitmap;

#ifdef WLSQS
	/* SQS maintains per tid flow. So 4 bit AC bitmap has
	 * to be converted to 8 bit TID bitmap
	 */
	prio_bitmap = pciedev_ac2tid_bitmap(ac_bitmap);
#else
	prio_bitmap = ac_bitmap;
#endif // endif

	PCI_TRACE(("pciedev_process_reqst_packet: handle=%d; ac_bitmap=%d, count=%d\n",
			handle, prio_bitmap, count));

	if (pciedev_ds_in_host_sleep(pciedev) || !count) {
		DBG_BUS_INC(pciedev, pciedev_process_reqst_packet);
		return;
	}
	bzero(fptr, sizeof(fptr));

	/* loop through active queues */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			uint8 tid_ac;
			flow_ring = ((flowring_pool_t *)cur)->ring;
			tid_ac = PCIEDEV_TID_REMAP(pciedev, flow_ring);
			if (flow_ring->handle != handle) {
				cur = dll_next_p(cur);
				continue;
			}

			if (prio_bitmap & (0x1 << tid_ac))
				fptr[flow_ring->flow_info.tid_ac] = flow_ring;

			cur = dll_next_p(cur);
		} /* while() */

		/* get next priority ring node */
		prio = dll_next_p(prio);
	} /* while() */

	i = PCIE_MAX_TID_COUNT - 1;
	while (i >= 0) {
		if (fptr[i] == NULL) {
			i--;
			continue;
		}

		/* Check write pointer of the flow ring and mark it for pending pkt pull */
		if (!MSGBUF_READ_AVAIL_SPACE(fptr[i])) {
			fptr[i]->status &= ~FLOW_RING_PKT_PENDING;
			if (fptr[i]->flow_info.flags & FLOW_RING_FLAG_LAST_TIM) {
				/* informs WL subsystem about the TIM_RESET operation */
				uint8 tid_ac = PCIEDEV_TID_REMAP(pciedev, fptr[i]);
				fptr[i]->flow_info.flags &= ~FLOW_RING_FLAG_LAST_TIM;
				dngl_flowring_update(pciedev->dngl, fptr[i]->flow_info.ifindex,
					(uint8) fptr[i]->handle, FLOW_RING_TIM_RESET,
					(uint8 *)&fptr[i]->flow_info.sa,
					(uint8 *)&fptr[i]->flow_info.da, tid_ac, NULL);
			}
			i--;
			continue;
		}
		if (count > 0) {
			prev_maxpktcnt = fptr[i]->flow_info.maxpktcnt;
			count = pciedev_reuse_seq_amsdu_cnt(pciedev, fptr[i]);
			fptr[i]->flow_info.maxpktcnt = count;
			fptr[i]->flow_info.reqpktcnt += count;
			fptr[i]->flow_info.flags |= FLOW_RING_FLAG_PKT_REQ;
			count -= pciedev_h2d_start_fetching_host_buffer(pciedev, fptr[i]);
			fptr[i]->flow_info.maxpktcnt = prev_maxpktcnt;
			i--;
		} else
			break;
	} /* while() */
} /* pciedev_process_reqst_packet */

static void pciedev_push_pkttag_tlv_info(struct dngl_bus *pciedev, void *p,
       msgbuf_ring_t *flow_ring, uint16 index)
{
	uint32 pkt_flags = 0;
	uint8 flags = 0;
	uint8 len = ROUNDUP((TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_PKTTAG + WLFC_CTL_VALUE_LEN_SEQ),
		sizeof(uint32));
	uint16 seq = 0;
#ifdef BCM_DHDHDR
	uint8 status;
#ifdef WL_REUSE_KEY_SEQ
	uint8 *key_seq;
	int j;
#endif // endif
#endif /* BCM_DHDHDR */
	uint8 *buf;
	uint16 bit_idx = MSGBUF_MODULO_IDX(flow_ring, index);
	BCM_REFERENCE(flags);
#ifdef BCM_DHDHDR
	ASSERT(len <= sizeof(struct lbuf_finfo));
	buf = PKTFRAGFCTLV(pciedev->osh, p);
#else
	if (PKTHEADROOM(pciedev->osh, p) < len) {
		PCI_ERROR(("pciedev_push_pkttag_tlv_info: No room for pkttag TLV\n"));
		DBG_BUS_INC(pciedev, pciedev_push_pkttag_tlv_info);
		return;
	}

	PKTPUSH(pciedev->osh, p, len);
	buf = PKTDATA(pciedev->osh, p);
#endif /* BCM_DHDHDR */

	PKTSETDATAOFFSET(p, len >> 2);
#ifdef BCM_DHDHDR
	/* for reusing d11 seq number for suppressed packets */
	if (isset(flow_ring->reuse_sup_seq, bit_idx)) {
		seq = flow_ring->reuse_seq_list[bit_idx];
		PCI_TRACE(("PCIE Reused Seq %x %x %x\n", index,
			flow_ring->reuse_seq_list[bit_idx],
			WL_SEQ_GET_NUM(flow_ring->reuse_seq_list[bit_idx])));

#ifdef WL_REUSE_KEY_SEQ
		/* reuse key seq number for suppressed packets */
		if (isset(flow_ring->reuse_sup_key_seq, bit_idx)) {
			PCI_TRACE(("PCIE Reused d11 seq %x key seq => ",
				WL_SEQ_GET_NUM(flow_ring->reuse_seq_list[bit_idx])));

			key_seq = &flow_ring->reuse_key_seq_list[bit_idx * WL_KEY_SEQ_INFO_SIZE];
			for (j = WL_KEY_SEQ_INFO_SIZE; j > 0; --j) {
				PCI_TRACE(("%02x", key_seq[j - 1]));
			}
			PCI_TRACE(("\n"));
			memcpy(buf, key_seq, WL_KEY_SEQ_INFO_SIZE);
		}
#endif // endif
	}
	/* for packets sent as a result of Ps-poll from peer STA in PS */
	if (flow_ring->flow_info.flags & FLOW_RING_FLAG_PKT_REQ) {
		pkt_flags = WLFC_PKTFLAG_PKT_REQUESTED;
		PKTFRAGSETPKTFLAGS(pciedev->osh, p, pkt_flags);
		flow_ring->flow_info.reqpktcnt--;
		if (!flow_ring->flow_info.reqpktcnt)
			flow_ring->flow_info.flags &= ~FLOW_RING_FLAG_PKT_REQ;
	}

	PKTSETWLFCSEQ(pciedev->osh, p, seq);
	status = WLFC_CTL_TYPE_PKTTAG;
	PKTFRAGSETTXSTATUS(pciedev->osh, p, status);
#else
	buf[TLV_TAG_OFF] = WLFC_CTL_TYPE_PKTTAG;
	buf[TLV_LEN_OFF] = WLFC_CTL_VALUE_LEN_PKTTAG + WLFC_CTL_VALUE_LEN_SEQ;

	/* for packets sent as a result of Ps-poll from peer STA in PS */
	if (flow_ring->flow_info.flags & FLOW_RING_FLAG_PKT_REQ) {
		flags |= WLFC_PKTFLAG_PKT_REQUESTED;
		WL_TXSTATUS_SET_FLAGS(pkt_flags, flags);
		memcpy(&buf[TLV_HDR_LEN], &pkt_flags, sizeof(uint32));
		flow_ring->flow_info.reqpktcnt--;
		if (!flow_ring->flow_info.reqpktcnt)
			flow_ring->flow_info.flags &= ~FLOW_RING_FLAG_PKT_REQ;
	}

	/* for reusing d11 seq number for suppressed packets */
	if (isset(flow_ring->reuse_sup_seq, bit_idx)) {
		seq = flow_ring->reuse_seq_list[bit_idx];
		PCI_TRACE(("PCIE Reused Seq %d %x %x\n", index,
			flow_ring->reuse_seq_list[bit_idx],
			WL_SEQ_GET_NUM(flow_ring->reuse_seq_list[bit_idx])));
	}
	memcpy(&buf[TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_PKTTAG],
		&seq, sizeof(uint16));
#endif /* BCM_DHDHDR */
}

/** Called on wl dpc. Send out chained pkt completions */
void
pciedev_flush_chained_pkts(struct dngl_bus *pciedev)
{
	/* dequeue and send out rx payloads and rx completes */
	if (!IS_IOCTLREQ_PENDING(pciedev)) {
		pciedev_queue_d2h_req_send(pciedev);
	}

	/* Send out tx completes */
	if (PCIEDEV_TXCPL_PEND_ITEM_CNT(pciedev)) {
		PCI_TRACE(("TX: pend count is %d\n", PCIEDEV_TXCPL_PEND_ITEM_CNT(pciedev)));
		PCIEDEV_XMIT_TXSTATUS(pciedev);
	}
}

/** Timer callback function */
void
pciedev_flush_glommed_rxstatus(dngl_timer_t *t)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);

	if (PCIEDEV_RXCPL_PEND_ITEM_CNT(pciedev)) {
		PCI_TRACE(("RX: pend count is %d\n", PCIEDEV_RXCPL_PEND_ITEM_CNT(pciedev)));
		PCIEDEV_XMIT_RXCOMPLETE(pciedev);
	}
	pciedev->glom_timer_active = FALSE;
}

#if defined(BCMPCIE_IDMA)
/** reset implicit dma */
static void
pciedev_idma_reset(struct dngl_bus *pciedev, hnddma_t *di)
{
	/* toggle implicit dma mode */
	AND_REG(pciedev->osh, PCIE_pciecontrol(pciedev->autoregs), ~PCIE_IDMA_MODE_EN);
	OR_REG(pciedev->osh, PCIE_pciecontrol(pciedev->autoregs), PCIE_IDMA_MODE_EN);

	/* enable ctrl/rx/tx interrupt index for implicit dma */
	OR_REG(pciedev->osh, &pciedev->regs->u.pcie2.h2d_intmask_2,
		PD_DMA_INT_MASK_H2D);
	OR_REG(pciedev->osh, &pciedev->regs->u.pcie2.h2d_frg_intmask_2,
		PD_IDMA_COMP);

	/* toggle rx dma chan enable */
	dma_rxchan_reset(di);
	/* toggle tx dma chan enable */
	dma_txchan_reset(di);
}
#endif /* BCMPCIE_IDMA */

#ifdef BCMPCIE_IDMA
/* program and prefetch implicit dma 16rx/16tx descriptors */
int
pciedev_idma_desc_load(struct dngl_bus *pciedev)
{
	int i;
	dmaindex_t *dmaindex;
	hnddma_t *di = pciedev->h2d_di[DMA_CH2];
	m2m_desc_t *desc = pciedev->m2m_desc;

	/* set implicit dma mode */
	pciedev_idma_reset(pciedev, pciedev->h2d_di[DMA_CH2]);

	if (!pciedev->idma_info->inited) {
		PCI_ERROR(("idma desc prg fail because host is not inited\n"));
		ASSERT(0);
		return BCME_ERROR;
	}

	INIT_M2M_DESC(desc);

	/* NUM_VEC_PCIE is total desc number for both RX and TX */
	ASSERT(MAX_IDMA_DESC * 2 <= NUM_VEC_PCIE);

	/* Program Rx Desc for frm */
	for (i = 0; i < MAX_IDMA_DESC; i++) {
		dmaindex = &pciedev->idma_info->dmaindex[i];

		if (!dmaindex->inited)
			break;

		SETUP_RX_DESC(desc, dmaindex->daddr64, dmaindex->size);
	}
	ASSERT(i > 0);

	/* Program Tx Desc for frm */
	for (i = 0; i < MAX_IDMA_DESC; i++) {
		dmaindex = &pciedev->idma_info->dmaindex[i];

		if (!dmaindex->inited)
			break;

		SETUP_TX_DESC(desc, dmaindex->haddr64, dmaindex->size);
	}

	if (DMA_M2M_SUBMIT_IMPLICIT(di, desc)) {
		PCI_ERROR(("idma desc fill failed\n"));
		ASSERT(0);
		goto exit;
	}

	return BCME_OK;

exit:
	return BCME_ERROR;
}
#endif /* BCMPCIE_IDMA */

void /* Memcpy using sbtopcie0 btwn host and dongle, in 4Byte units length */
pciedev_sbcopy(void *dev,
               uint64 haddr64, daddr32_t daddr32, int len_4B, bool h2d_dir)
{
	int i;
	uint32 host_mem, base_addr, base_lo;
#ifdef HOST_DMA_ADDR64
	uint32 base_hi;
#endif // endif
	struct dngl_bus *pciedev = (struct dngl_bus *)dev;

	ASSERT(pciedev->regs);
	BCM_REFERENCE(base_addr);

	/* Remap the host address as per transalation 0 */
	SBTOPCIE_ADDR_REMAP(pciedev, haddr64, 0, host_mem);

	/* Backup previous base configuration */
#ifdef HOST_DMA_ADDR64
	/* for 64 bit Host address, take sbtopcie0upper backup */
	base_hi = R_REG(pciedev->osh, &pciedev->regs->sbtopcie0upper);
#endif // endif
	base_lo = R_REG(pciedev->osh, &pciedev->regs->sbtopcie0);

	SBTOPCIE_BASE_CONFIG(pciedev, haddr64, len_4B, 0, base_addr, FALSE);

	/* Perform the memcpy in 4Byte units in requested direction */
	for (i = 0; i < len_4B; i++) {
		if (h2d_dir) {
			*((uint32*)daddr32)  = *((uint32*)host_mem);
		} else {
			*((uint32*)host_mem) = *((uint32*)daddr32);
		}
		daddr32  += 4;
		host_mem += 4;
	}

	INSTR_BARRIER();

	/* Restore backed up configuration */
	W_REG(pciedev->osh, &pciedev->regs->sbtopcie0, base_lo);
#ifdef HOST_DMA_ADDR64
	W_REG(pciedev->osh, &pciedev->regs->sbtopcie0upper, base_hi);
#endif // endif

}   /* pciedev_sbcopy() */

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
/*
 * sb2pcie access to host memmory using sbtopcie0  and sbtopcie0upper address translation
 * Used for temporary accees only; Put back base reg values after access
 *
 * Input : <4 byte Low address> for 32 bit Host address< 2/4 byte pointer to data>
 * Input : <8 byte Low address> for 64 bit Host address< 2/4 byte pointer to data>
 */
void
pciedev_sbtopcie_access(struct dngl_bus * pciedev, uint64 haddr64,
	pcie_rw_index_t *data, bool read)
{
	uint32 host_mem;
	uint32 base_lo;
#ifdef HOST_DMA_ADDR64
	uint32 base_hi;
#endif // endif
	uint32 base_addr;

	ASSERT(pciedev->regs);
	BCM_REFERENCE(base_addr);

	/* Use sbtopcie translation 0 */
	/* Remap the address as per transalation 0 */
	SBTOPCIE_ADDR_REMAP(pciedev, haddr64, 0, host_mem);

	/* for 64 bit Host address, take sbtopcie0upper backup */
#ifdef HOST_DMA_ADDR64
	base_hi = R_REG(pciedev->osh, &pciedev->regs->sbtopcie0upper);
#endif // endif

	/* Take a backup of base address */
	base_lo = R_REG(pciedev->osh, &pciedev->regs->sbtopcie0);

	SBTOPCIE_BASE_CONFIG(pciedev, haddr64, sizeof(uint32),
		0, base_addr, FALSE);

	/* Access host memory via sbtopcie translation 0 */
	if (read)
		*data = *((pcie_rw_index_t *)host_mem);
	else
		*((pcie_rw_index_t *)host_mem) = *data;

	/* Insert an instruction barrier to prevent base reg from over-writing */
	INSTR_BARRIER();

	/* restore back old base address */
	W_REG(pciedev->osh, &pciedev->regs->sbtopcie0, base_lo);
#ifdef HOST_DMA_ADDR64
	W_REG(pciedev->osh, &pciedev->regs->sbtopcie0upper, base_hi);
#endif // endif
}

/* Sync up read pointers of h2d rings[common rings + flow rings */
void
pciedev_sync_h2d_read_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	const uint32 rw_index_sz = PCIE_RW_INDEX_SZ;
	uint32 haddr64_lo;
#ifdef HOST_DMA_ADDR64
	uint32 haddr64_hi;
	uint64 haddr64;
#endif // endif
	pcie_rw_index_t host_data;

	ASSERT(ring);

	BUZZZ_KPI_QUE1(KPI_QUE_BUS_RD_UPD, 2, MSGBUF_RD(ring), ring->ringid);

	/* Host address to access */
#ifdef HOST_DMA_ADDR64
	haddr64_hi = HADDR64_HI(pciedev->dmaindex_h2d_rd.haddr64);
#endif // endif
	haddr64_lo = HADDR64_LO(pciedev->dmaindex_h2d_rd.haddr64);
	haddr64_lo = (uint32)BCMPCIE_RW_INDEX_ADDR(haddr64_lo, rw_index_sz, ring->ringid);

	/* Data to be written */
	host_data = MSGBUF_RD(ring);

#ifdef HOST_DMA_ADDR64
	haddr64 = haddr64_hi;
	haddr64 = (haddr64 << (NBITS(uint32)) | haddr64_lo);
#endif // endif

	/* Write to host Mem */
#ifdef HOST_DMA_ADDR64
	pciedev_sbtopcie_access(pciedev, haddr64, &host_data, FALSE);
#else
	pciedev_sbtopcie_access(pciedev, haddr64_lo, &host_data, FALSE);
#endif // endif
}
/* Sync up D2h write pointers between host and dongle [completion rings] */
static void
pciedev_sync_d2h_write_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	uint32 index;
	const uint32 rw_index_sz = PCIE_RW_INDEX_SZ;
	uint32 haddr64_lo;
#ifdef HOST_DMA_ADDR64
	uint32 haddr64_hi;
	uint64 haddr64;
#endif // endif
	pcie_rw_index_t host_data;

	ASSERT(ring);

	index = BCMPCIE_D2H_RING_IDX(ring->ringid);

	/* Host address to access */
#ifdef HOST_DMA_ADDR64
	haddr64_hi = HADDR64_HI(pciedev->dmaindex_d2h_wr.haddr64);
#endif // endif
	haddr64_lo = HADDR64_LO(pciedev->dmaindex_d2h_wr.haddr64);

	haddr64_lo = (uint32)BCMPCIE_RW_INDEX_ADDR(haddr64_lo, rw_index_sz, index);

	/* Data to be written */
	host_data = MSGBUF_WR(ring);

#ifdef HOST_DMA_ADDR64
		haddr64 = haddr64_hi;
		haddr64 = (haddr64 << (NBITS(uint32)) | haddr64_lo);
#endif // endif

	/* Write to host mem */
#ifdef HOST_DMA_ADDR64
	pciedev_sbtopcie_access(pciedev,  haddr64, &host_data, FALSE);
#else
	pciedev_sbtopcie_access(pciedev,  haddr64_lo, &host_data, FALSE);
#endif // endif
}
/* Sync up read pointers for d2h completion rings from host */
void
pciedev_sync_d2h_read_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	uint32 index;
	const uint32 rw_index_sz = PCIE_RW_INDEX_SZ;
	uint32 haddr64_lo;
#ifdef HOST_DMA_ADDR64
		uint32 haddr64_hi;
		uint64 haddr64;
#endif // endif
	pcie_rw_index_t host_data;

	ASSERT(ring);
	index = BCMPCIE_D2H_RING_IDX(ring->ringid);

	/* Host address to access */
#ifdef HOST_DMA_ADDR64
	haddr64_hi = HADDR64_HI(pciedev->dmaindex_h2d_rd.haddr64);
#endif // endif
	haddr64_lo = HADDR64_LO(pciedev->dmaindex_h2d_rd.haddr64);
	haddr64_lo = (uint32)BCMPCIE_RW_INDEX_ADDR(haddr64_lo, rw_index_sz, index);

#ifdef HOST_DMA_ADDR64
	haddr64 = haddr64_hi;
	haddr64 = (haddr64 << (NBITS(uint32)) | haddr64_lo);
#endif // endif
	/* Read from host */
#ifdef HOST_DMA_ADDR64
	pciedev_sbtopcie_access(pciedev,  haddr64, &host_data, TRUE);
#else
	pciedev_sbtopcie_access(pciedev,  haddr64_lo, &host_data, TRUE);
#endif // endif
	MSGBUF_RD(ring) = host_data;
}

/* If multiple flow rings gets queued up, sync up read pointers
 * on every flow ring change.
 * Maintain a head of line ring info;
 * if ring changes sync up read pointer for the saved HOL ring
 */
static INLINE void
pciedev_upd_last_queued_flowring(struct dngl_bus * pciedev, uint16 ringid)
{
	/* Get flow ring info from ringid */
	msgbuf_ring_t *flow_ring = pciedev->h2d_submitring_ptr[ringid];

	ASSERT(flow_ring);

	if (flow_ring != pciedev->txcpl_last_queued_ring) {
		/* Change of Flow rings. flush read pointer for prev ring */
		if (pciedev->txcpl_last_queued_ring) {
			pciedev_sync_h2d_read_ptrs(pciedev,
				pciedev->txcpl_last_queued_ring);
		}

		/* Update last queued ring
		 * Read index for this flow ring would be synced
		 * along with txstatus xmit.
		 */
		pciedev->txcpl_last_queued_ring = flow_ring;
	}
}
/* To optimize the sbtopcie access, flow ring read pointer
 * updates are batched together and sent as lazy updates.
 * A good place to do that would be along with txstatus xmit.
 */
static INLINE void
pciedev_sync_flowring_read_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	if (ring->dmaindex_d2h_supported && pciedev->dmaindex_h2d_rd.inited) {

		/* Update flow ring read pointers before sending txstatus */
		if (pciedev->txcpl_last_queued_ring) {
			/* Update flowring RD pointer if bulk_txs_pending is not zero.
			 * This could avoid sync old read pointer to host side.
			 */
			pciedev_bulk_update_txstatus(pciedev, pciedev->txcpl_last_queued_ring);

			pciedev_sync_h2d_read_ptrs(pciedev, pciedev->txcpl_last_queued_ring);
		}
		/* reset the last queue info */
		pciedev->txcpl_last_queued_ring = NULL;

	}
}
#endif /* SBTOPCIE_INDICES && DMA_INDICES */

#ifdef BCM_DHDHDR
/* With DHDHDR enabled, status bytes are seq numbers are embedded
 * inside lfrag rather than the headroom section
 */
static void
pciedev_tx_processed(struct dngl_bus *pciedev, void *p,
		uint32 *status, uint16 *seq, uint8 *key_seq)
{
#ifdef WL_REUSE_KEY_SEQ
	uint8 *buf;
	int j;
	buf = PKTFRAGFCTLV(pciedev->osh, p);
#endif // endif
	*status = PKTFRAGTXSTATUS(pciedev->osh, p);
	*seq = PKTWLFCSEQ(pciedev->osh, p);
	if (*seq && ((*status == WLFC_CTL_PKTFLAG_D11SUPPRESS) ||
		(*status == WLFC_CTL_PKTFLAG_WLSUPPRESS))) {
		/*
		 * The WLC layer assigns d11 sequence numbers to packets. If
		 * a packet was suppressed, it might or might not have been
		 * assigned a valid sequence number. Therefore, need to
		 * check if seq number is valid from FW by checking the
		 * FROMFW flag before reusing it. Note that for PCIe FD, the
		 * usage of the 'FROMDRV' flag does not correspond with its
		 * name, since the driver was not compiled with
		 * PROP_TXSTATUS support.
		 * Symptom was sniff capture showed pkts with seq number
		 * of zero being re-used leading to OOO.
		 */
		if (GET_WL_HAS_ASSIGNED_SEQ(*seq)) {
			SET_DRV_HAS_ASSIGNED_SEQ(*seq);
			RESET_WL_HAS_ASSIGNED_SEQ(*seq);
#ifdef WL_REUSE_KEY_SEQ
			if (GET_WL_HAS_ASSIGNED_KEY_SEQ(buf)) {
				SET_DRV_HAS_ASSIGNED_KEY_SEQ(buf);
				RESET_WL_HAS_ASSIGNED_KEY_SEQ(buf);
				PCI_TRACE(("%s+ d11 seq %x key seq =>",
					__FUNCTION__, WL_SEQ_GET_NUM(*seq)));
				for (j = WL_KEY_SEQ_INFO_SIZE; j > 0; --j) {
					PCI_TRACE(("%02x", buf[j - 1]));
				}
				PCI_TRACE(("\n"));
				memcpy(key_seq, buf, WL_KEY_SEQ_INFO_SIZE);
			}
#endif // endif
		} else {
			/* Explicitly clear seq no to indicate 'no reuse' */
			*seq = 0;
		}
#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
		/* see if the packet needs to be dropped at this point */
		*status = pciedev_flow_ring_age_out_check(pciedev, p, *status);
#endif /* PCIEDEV_SUPPR_RETRY_TIMEOUT */
	}
}
#else
static void
pciedev_tx_processed(struct dngl_bus *pciedev, void *p,
		uint32 *status, uint16 *seq, uint8 *key_seq)
{
	uint32 metadata_len;

	metadata_len = PKTLEN(pciedev->osh, p);
	if (metadata_len == TXSTATUS_LEN) {
		*status = *((uint8*)PKTDATA(pciedev->osh, p) +
			BCMPCIE_D2H_METADATA_HDRLEN);
	} else if (metadata_len >= (BCMPCIE_D2H_METADATA_HDRLEN + TLV_HDR_LEN +
		WLFC_CTL_VALUE_LEN_TXSTATUS)) {
		/* Copy txstatus which is the first 4 bytes after metdata header */
		memcpy(status, (char *)PKTDATA(pciedev->osh, p) +
			BCMPCIE_D2H_METADATA_HDRLEN + TLV_HDR_LEN,
			WLFC_CTL_VALUE_LEN_TXSTATUS);
		*status = WL_TXSTATUS_GET_FLAGS(*status);

		if ((*status == WLFC_CTL_PKTFLAG_D11SUPPRESS) ||
			(*status == WLFC_CTL_PKTFLAG_WLSUPPRESS)) {
			uint16 *buf = (uint16 *)((char *)PKTDATA(pciedev->osh,
				p) + BCMPCIE_D2H_METADATA_HDRLEN +
				TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_TXSTATUS);
			memcpy((uint *)seq, buf, WLFC_CTL_VALUE_LEN_SEQ);
			/*
			 * The WLC layer assigns d11 sequence numbers to packets. If
			 * a packet was suppressed, it might or might not have been
			 * assigned a valid sequence number. Therefore, need to
			 * check if seq number is valid from FW by checking the
			 * FROMFW flag before reusing it. Symptom was sniff capture
			 * showed pkts with seq number of zero being re-used leading
			 * to OOO.
			 * Note that for PCIe FD, the host driver is not compiled
			 * with PROP_TXSTATUS support.
			 */
			if (GET_WL_HAS_ASSIGNED_SEQ(*seq)) {
				PCI_TRACE(("PCIE Reuse Seq %x %x\n", *seq, WL_SEQ_GET_NUM(*seq)));
				SET_DRV_HAS_ASSIGNED_SEQ(*seq);
				RESET_WL_HAS_ASSIGNED_SEQ(*seq);
			} else {
				/* Explicitly clear seq no to indicate 'no reuse' */
				*seq = 0;
			}
		}
	}
#ifdef PCIEDEV_SUPPR_RETRY_TIMEOUT
	/* see if the packet needs to be dropped at this point */
	*status = pciedev_flow_ring_age_out_check(pciedev, p, *status);
#endif /* PCIEDEV_SUPPR_RETRY_TIMEOUT */
}
#endif /* BCM_DHDHDR */

#ifdef PCIEDEV_FAST_DELETE_RING
static void
pciedev_fastdelete_update_status(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring, uint16 rdptr)
{
	if ((pciedev->fastdeletering) && ((FL_STATUS(flow_ring) &
		(FLOW_RING_DELETE_RESP_PENDING | FLOW_RING_FAST_DELETE_ACTIVE)) ==
			FLOW_RING_DELETE_RESP_PENDING))	 {
		/* Check if rdptr is at or past delete_idx. If so set the
		 * FAST_DELETE_ACTIVE flag.
		 */
		if (rdptr >= MSGBUF_WR(flow_ring)) {
			if ((flow_ring->delete_idx >= MSGBUF_WR(flow_ring)) &&
			    (flow_ring->delete_idx <= rdptr)) {
				FL_STATUS(flow_ring) |= FLOW_RING_FAST_DELETE_ACTIVE;
			}
		} else {
			if ((flow_ring->delete_idx <= rdptr) ||
			    (flow_ring->delete_idx > MSGBUF_WR(flow_ring))) {
				FL_STATUS(flow_ring) |= FLOW_RING_FAST_DELETE_ACTIVE;
			}
		}
	}
}
#endif /* PCIEDEV_FAST_DELETE_RING */

static void
pciedev_clr_bitmaps_tracking(msgbuf_ring_t *flow_ring, uint16 rindex, uint16 nframes, bool all)
{
	/*
	 * This routine does the following
	 * for (i = 0; i < nframes; i++) {
	 *   clrbit(flow_ring->inprocess, rindex);
	 *   clrbit(flow_ring->status_cmpl, rindex);
	 *   if (all) clrbit(flow_ring->reuse_sup_seq, rindex);
	 *   rindex = (rindex + 1) % MSGBUF_MAX(flow_ring);
	 * }
	 */
	uint i;
	uint nbytes;
	uint byte_idx;
	uint bitmap_size = flow_ring->bitmap_size;
	uint remain = nframes;
	uint nbits;

	rindex = MSGBUF_MODULO_IDX(flow_ring, rindex);
	nbits = NBBY - (rindex % NBBY);
	if (nbits > remain) {
		nbits = remain;
	}

	/* Clear the bits in the 1st byte */
	for (i = 0; i < nbits; i++) {
		clrbit(flow_ring->inprocess, rindex);
		clrbit(flow_ring->status_cmpl, rindex);
		if (all) {
			clrbit(flow_ring->reuse_sup_seq, rindex);
#ifdef WL_REUSE_KEY_SEQ
			clrbit(flow_ring->reuse_sup_key_seq, rindex);
#endif // endif
		}
		rindex = (rindex + 1) % bitmap_size;
		remain--;
	}
	nbytes = (nframes - nbits) / NBBY;
	byte_idx = rindex / NBBY;
	/* Clear the bytes */
	for (i = 0; i < nbytes; i++) {
		flow_ring->inprocess[byte_idx] = 0;
		flow_ring->status_cmpl[byte_idx] = 0;
		if (all) {
			flow_ring->reuse_sup_seq[byte_idx] = 0;
#ifdef WL_REUSE_KEY_SEQ
			flow_ring->reuse_sup_key_seq[byte_idx] = 0;
#endif // endif
		}
		byte_idx = (byte_idx + 1) % (bitmap_size / NBBY);
		rindex = (rindex + NBBY) % bitmap_size;
		remain -= NBBY;
	}
	/* Clear the bits in the last byte */
	while (remain) {
		clrbit(flow_ring->inprocess, rindex);
		clrbit(flow_ring->status_cmpl, rindex);
		if (all) {
			clrbit(flow_ring->reuse_sup_seq, rindex);
#ifdef WL_REUSE_KEY_SEQ
			clrbit(flow_ring->reuse_sup_key_seq, rindex);
#endif // endif
		}
		rindex = (rindex + 1) % bitmap_size;
		remain--;
	}
}

static void
pciedev_bulk_update_txstatus(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring)
{
	uint16 rdptr, free_rp = 0;
	uint16 nframes;
	uint count;

	if (pciedev->bulk_txs_pending == 0)
		return;

	rdptr = MSGBUF_RD(flow_ring);
	nframes = pciedev->bulk_txs_pending;

	/* Reset for next process */
	pciedev->bulk_txs_pending = 0;

	/* Clear inprocess, status_cmpl, reuse_sup_seq bits */
	pciedev_clr_bitmaps_tracking(flow_ring, rdptr, nframes, TRUE);
	rdptr = (rdptr + nframes) % MSGBUF_MAX(flow_ring);
	MSGBUF_RD(flow_ring) = rdptr;

	/*
	 * If the bitmaps status_cmpl starting from rdptr are set,
	 *   loop until a non pending packet is found (!status_cmpl)
	 *   - clear inprocess, status_cmpl bits
	 *   Update the flowring's rdptr
	 */
	for (count = 0; count < flow_ring->bitmap_size; count++) {
		free_rp = (rdptr + count) % MSGBUF_MAX(flow_ring);
		if (!isset(flow_ring->status_cmpl, MSGBUF_MODULO_IDX(flow_ring, free_rp))) {
			break;
		}
	}
	if (count) {
		/* Clear inprocess, status_cmpl bits */
		pciedev_clr_bitmaps_tracking(flow_ring, rdptr, count, FALSE);
		MSGBUF_RD(flow_ring) = free_rp;
	}
}

/**
 * Invoke when a bundle of packets in spktq is freed.
 * Similar to pciedev_lbuf_callback (invoked when pkt is freed).
 * The spq holds a list of packets chained through PKTLINK() pointers.
 *   Each the packet may have one or more lbufs chained through the PKTNEXT()
 *   pointers.
 * This routine flattens the spq into the new list of lbufs chained through
 *   PKTLINK() pointers (newspq) to append to d2h_req_q before invoking
 *   pciedev_xmit_txstatus.
 * @param spq : single priority packet queue
 */
void
pciedev_spktq_callback(void *arg, struct spktq *spq)
{
	uint16 nframes;
	void *p;
	uint16 rindex, expect_rindex, first_rindex;
	uint16 flowid, expect_flowid;
	msgbuf_ring_t *flow_ring;
	uint8 ifidx;
	uint16 rdptr;
	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	uint16 seq = 0;
	uint8 key_seq[WL_KEY_SEQ_INFO_SIZE] = {0};

	p = spktq_peek(spq);
	if (p == NULL) {
		return;
	}
	first_rindex = expect_rindex = PKTFRAGRINGINDEX(pciedev->osh, p);
	expect_flowid = PKTFRAGFLOWRINGID(pciedev->osh, p);
	flow_ring = pciedev->h2d_submitring_ptr[expect_flowid];
	if (flow_ring == NULL) {
		PCI_ERROR(("pciedev_spktq_callback with invalid flowid %d\n", expect_flowid));
		goto slow_mode;
	}

	rdptr = MSGBUF_RD(flow_ring);
	if ((rdptr != first_rindex) || (flow_ring->status & FLOW_RING_DELETE_RESP_PENDING)) {
		goto slow_mode;
	}

	nframes = 0;
	pciedev->bulk_txs_pending = 0;

	/* The outer loop walks the list of packets chained through the PKTLINK() */
	while ((p = spktq_deq(spq))) {
		/* The inner loop walks the lbufs in a packet chained through PKTNEXT() */
		while (p) { /* while (p) */
			void *nextlb, *_lb;
			uint32 status = 0;
#ifndef BCM_DHDHDR
			cmn_msg_hdr_t *cmn_msg;
#endif // endif
			if (!PKTHASMETADATA(pciedev->osh, (struct lbuf *)p)) {
				PCI_PRINT(("%s: %p pktid 0x%08x no metadata!\n", __FUNCTION__,
					p, PKTFRAGPKTID(pciedev->osh, p)));
				spktq_enq_head(spq, p);
				goto bailout;
			}

			rindex = PKTFRAGRINGINDEX(pciedev->osh, p);
			flowid = PKTFRAGFLOWRINGID(pciedev->osh, p);

			/* Check if PKT has been TXstatus processed */
			if (PKTISTXSPROCESSED(pciedev->osh, p)) {
				pciedev_tx_processed(pciedev, p, &status, &seq, &key_seq[0]);
			}

			/* If there are any unexpect status, flowid, or rindex happened.
			 * Before goto slow mode process. We should handle the bitmaps
			 * (inprocess, status_cmpl, reuse_sup_seq) for nframes first.
			 * Also enqueue packet back to spktq head instead of free packet.
			 * This could ensure the order of the bitmaps processsing.
			 */
			if (status || (flowid != expect_flowid) || (rindex != expect_rindex)) {
				spktq_enq_head(spq, p);
				goto bailout;
			}

			/* Reset TXstatus processed state */
			PKTRESETTXSPROCESSED(pciedev->osh, p);
			PKTRESETTXSHOLD(pciedev->osh, p);
			PCIEDEV_FLOW_COMPL_TS(flow_ring, rindex, 0);

			nframes++;
			pciedev->bulk_txs_pending++;
			expect_rindex = (expect_rindex + 1) % MSGBUF_MAX(flow_ring);
			nextlb = PKTNEXT(OSH_NULL, p);
			PKTSETNEXT(OSH_NULL, p, NULL);

			/* Skip non-txfrag, as we shall not send Tx completion for it */
			while (nextlb && !PKTISTXFRAG(pciedev->osh, nextlb)) {
				_lb = nextlb;
				PKTRESETTXSHOLD(pciedev->osh, _lb);
				nextlb = PKTNEXT(OSH_NULL, _lb);
				PKTSETNEXT(OSH_NULL, _lb, NULL);
				PKTFREE(pciedev->osh, _lb, TRUE);
			}
			/* Enqueue tx status quickly if its a success packet */
			if (status == 0 && !pciedev_ds_in_host_sleep(pciedev) &&
				!IS_IOCTLREQ_PENDING(pciedev) &&
				PCIEDEV_TXCPL_RESOURCE_AVAILABLE(pciedev)) {
				ipc_timestamp_t ts;
				int ret;
				if (pciedev->timesync && PKTISTXTSINSERTED(pciedev->osh, p)) {
					ts.ts_low = PKTFRAGDATA_LO(pciedev->osh, p, 1);
					ts.ts_high = PKTFRAGDATA_HI(pciedev->osh, p, 1);
					PKTRESETTXTSINSERTED(pciedev->osh, p);
				}

				ret = PCIEDEV_QUEUE_TXSTATUS(pciedev, PKTFRAGPKTID(pciedev->osh, p),
						PKTIFINDEX(pciedev->osh, p),
						PKTFRAGFLOWRINGID(pciedev->osh, p),
						status, &ts);

				if (ret ==  BCME_OK) {
					PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *) p);
					PKTFREE(pciedev->osh, p, FALSE);
					p = nextlb;
					continue;
				}
			}

#ifndef BCM_DHDHDR
			/* check for tx frag */
			if (PKTHEADROOM(pciedev->osh, p) < 8) {
				DBG_BUS_INC(pciedev, pciedev_lbuf_callback);
				PCI_ERROR(("PKTHEADROOM is less than needed 8, %d\n",
					PKTHEADROOM(pciedev->osh, p)));
				ASSERT(0);
			}
			PKTPUSH(pciedev->osh, p, sizeof(cmn_msg_hdr_t));
			cmn_msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);
			cmn_msg->msg_type = MSG_TYPE_TXMETADATA_PYLD;
			/* Passing tx status on request_id as it is unused internally */
			cmn_msg->request_id = status;
#endif /* !BCM_DHDHDR */

			/* Now no matter DHDHDR is enable or not we can
			 * use generic txstats field  to save it.
			 */
			PKTFRAGSETTXSTATUS(pciedev->osh, p, status);

			/* Tx status couldnt be queued. Add packet to d2h req Q */
			if (pciedev_queue_d2h_req(pciedev, p, D2H_REQ_PRIO_0) == BCME_OK)
				flow_ring->d2h_q_txs_pending++;

			p = nextlb;

		} /* while (lb) */
	} /* while (p) */

bailout:
	if (nframes == 0) {
		goto slow_mode;
	}

	/* pciedev_update_txstatus FASTPATH */
	pciedev_bulk_update_txstatus(pciedev, flow_ring);

	flow_ring->flow_info.pktinflight -= nframes;
	ifidx = flow_ring->flow_info.ifindex;
	pciedev->ifidx_account[ifidx].cur_cnt -= nframes;
	pciedev->pend_user_tx_pkts -= nframes;

	if (!flow_ring->flow_info.pktinflight && !flow_ring->fetch_pending) {
		flow_ring->status &= ~FLOW_RING_SUP_PENDING;
	}
	if (flow_ring->status & FLOW_RING_FLUSH_PENDING) {
		pciedev_process_pending_flring_resp(pciedev, flow_ring);
	}
#if defined(WLSQS)
	pciedev_schedule_sqs_timer(pciedev);
#else
	pciedev_schedule_flow_ring_read_buffer(pciedev);
#endif /* !WLSQS */

	/* pciedev_update_txstatus FASTPATH */

	/* Keeping count of how many host tx lfrags are freed while in D3 */
	if (pciedev_ds_in_host_sleep(pciedev)) {
		pciedev->num_tx_status_in_d3 += nframes;
	}

	if (dll_empty(&pciedev->active_prioring_list) &&
		PCIEDEV_TXCPL_PEND_ITEM_CNT(pciedev)) {
		PCIEDEV_XMIT_TXSTATUS(pciedev);
	}

slow_mode:
	while ((p = spktq_deq(spq))) {
		PKTFREE(pciedev->osh, p, TRUE);
	}
} /* pciedev_spktq_callback */

/**
 * rindex: read index of the current frame
 * rdptr:  flowring's read pointer
 * This routine does the following
 * - Clear the bitmap reuse_sup_seq at rindex
 * - If rdptr!=rindex Set the bitmap status_cmpl at rindex
 * - If rdptr==rindex;
 *   Do the following in a loop until a non pending packet !status_cmpl is found
 *     Starting from rdptr+1, if status_cmpl is clear, we are done
 *     Otherwise, clear the bitmaps status_cmpl and inprocess
 *   Update the rdptr (MSGBUF_RD)
 * For example: 4 frames with the following rindex: 261 262 347 348
 * If all the bitmaps status_cmpl between 263 and 346 are set, we clear all the
 * bitmaps status_cmpl between 263 and 346, and update the flowring's
 * rdptr to 347.
 */
static void
pciedev_update_bitmaps_tracking(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring, uint16 rindex)
{
	uint i;
	uint16 rdptr;
	uint16 free_rp;
	uint16 bit_idx = MSGBUF_MODULO_IDX(flow_ring, rindex);

	clrbit(flow_ring->reuse_sup_seq, bit_idx); /* d11 seq numbers */
#ifdef WL_REUSE_KEY_SEQ
	clrbit(flow_ring->reuse_sup_key_seq, bit_idx); /* d11 IV seq numbers */
#endif // endif
	PCIEDEV_FLOW_COMPL_TS(flow_ring, rindex, 0);

	rdptr = MSGBUF_RD(flow_ring); /* advanced by dongle (consumer) */
	if (rdptr == rindex) {
		/* txstatus was returned for next packet host is waiting for */
		clrbit(flow_ring->inprocess, bit_idx);
		i = 1;
		free_rp = 0;
		/* Find index until non pending packet and update the read index */
		while (i < flow_ring->bitmap_size) {
			free_rp = (rdptr + i) % MSGBUF_MAX(flow_ring);
			if (!isset(flow_ring->status_cmpl, MSGBUF_MODULO_IDX(flow_ring, free_rp))) {
				break;
			}
			/* Clear inprocess bit, we are done with the packet now */
			clrbit(flow_ring->status_cmpl, MSGBUF_MODULO_IDX(flow_ring, free_rp));
			clrbit(flow_ring->inprocess, MSGBUF_MODULO_IDX(flow_ring, free_rp));
			i++;
		}
		MSGBUF_RD(flow_ring) =  free_rp;

#ifdef PCIEDEV_FAST_DELETE_RING
		pciedev_fastdelete_update_status(pciedev, flow_ring, free_rp);
#endif /* PCIEDEV_FAST_DELETE_RING */
	} else {
		/* txstatus was returned for a later packet than the one the host is waiting
		 * for.
		 */
		setbit(flow_ring->status_cmpl, bit_idx);
	}
}

#ifdef BCMHWA
#ifdef HWA_RXPATH_BUILD

#if !defined(HWA_RXCPLE_BUILD)
#error "HWA_RXPATH_BUILD need HWA_RXCPLE_BUILD support"
#endif // endif

#define PCIEDEV_HWA_QUEUE_RXCPL_FAST_LOOP	1000

/* Report a dropped Rx packet, do rxcpl directly. */
int
pciedev_hwa_queue_rxcomplete_fast(struct dngl_bus *pciedev, uint32 pktid)
{
	uint32 loop_count;

	loop_count = PCIEDEV_HWA_QUEUE_RXCPL_FAST_LOOP;
	while (hwa_rxcple_wi_add(hwa_dev, 0, BCMPCIE_PKT_FLAGS_FRAME_802_11,
		0, 0, pktid)) {
		OSL_DELAY(10);
		if (--loop_count == 0) {
			PCI_ERROR(("Queue_rxcomplete: alloc wi space failed "
				"for HWA2b RxCPLE fast\n"));
			OSL_SYS_HALT();
			break;
		}
		hwa_rxcple_commit(hwa_dev);
	}

	BCMPCIE_IPC_HPA_TEST(pciedev, pktid,
		BCMPCIE_IPC_PATH_RECEIVE, BCMPCIE_IPC_TRANS_RESPONSE);
	BUZZZ_KPI_PKT1(KPI_PKT_BUS_RXCMPL, 1, pktid);

	return 0;
}

void
pciedev_hwa_flush_rxcomplete(struct dngl_bus *pciedev)
{
	/* I need HWA 2b support to flush RxCpl */
	while (hwa_rxcple_pend_item_cnt(hwa_dev)) {
		PCI_ERROR(("%s(): RxCpl pend count <%u>\n", __FUNCTION__,
			hwa_rxcple_pend_item_cnt(hwa_dev)));
		hwa_rxcple_commit(hwa_dev);
	}
}
#endif /* HWA_RXPATH_BUILD */

#ifdef HWA_TXPOST_BUILD
#ifdef WLCFP
/* Check for CFP capability given a HWA packet list.
 * Fill up hwa_cfp_tx_info for HWA to enquue packet to CFP layer.
 */
void
pciedev_hwa_cfp_tx_enabled(struct dngl_bus *pciedev, hwa_txpost_pkt_t * head,
	hwa_cfp_tx_info_t *hwa_cfp_tx_info)
{
	msgbuf_ring_t *flow_ring;
	union {
		uint32 not_capable;
		struct {
			uint8 disabled;
			uint8 reuse_seq;
			uint8 eapol_pkt;
			uint8 tcb_state;
		} flags;
	} cfp;		/* Aggregate all conditional checks to one uint32 */

	/* Local Initialization */
	cfp.not_capable = 0U;

	/* Retrieve Flow ring ptr */
	flow_ring = pciedev->h2d_submitring_ptr[head->flowid_override];
	ASSERT(flow_ring);

	/* Check if CFP enabled for the flow ring */
	cfp.flags.disabled = !wlc_cfp_tx_enabled(FD_CFP_UNIT, flow_ring->cfp_flowid,
	        &(hwa_cfp_tx_info->expiry_time));

	/* Check for re-use sequence number in suppression cases */
	cfp.flags.reuse_seq = FLOWRING_HAS_REUSED_SEQ_PKT(flow_ring);

	/* Check for 802.1X packet
	 *
	 * 802.1X packets are never chained, count would be always 1.
	 * So check for eth type only for pkt_cnt = 1
	 */
	cfp.flags.eapol_pkt = ((hwa_cfp_tx_info->pktlist_count == 1) &&
		(ntoh16(head->eth_type) ==  ETHER_TYPE_802_1X));

	/* Assign the priority base on flow ring tid or prio in first packet */
#ifdef WLSQS
	hwa_cfp_tx_info->pktlist_prio = flow_ring->flow_info.tid_ac;
#else
	hwa_cfp_tx_info->pktlist_prio = head->prio;
#endif // endif

	/* Check for CFP TCB State */
	cfp.flags.tcb_state = ((flow_ring->tcb_state == NULL) ||
		CFP_NOT_CAPABLE(flow_ring->tcb_state,
		hwa_cfp_tx_info->pktlist_prio) ||
		(flow_ring->flow_info.flags & FLOW_RING_FLAG_PKT_REQ));

	/* Return CFP Flowid */
	hwa_cfp_tx_info->cfp_flowid = flow_ring->cfp_flowid;

	/* Finally , Check for CFP capable */
	hwa_cfp_tx_info->cfp_capable = (cfp.not_capable == 0);
}
#endif /* WLCFP */

/* Return the current flow ring state.
 * 	FLOW_RING_STATE_ACTIVE
 * 	FLOW_RING_STATE_SUPPRESS_PEND
 * 	FLOW_RING_STATE_FLUSH_PEND
 */
uint16
pciedev_flowring_state_get(struct dngl_bus *pciedev, uint16 flowid)
{
	msgbuf_ring_t *flow_ring;

	flow_ring = pciedev->h2d_submitring_ptr[flowid];

	if ((flow_ring == NULL) ||
#ifdef WLCFP
		!CFP_FLOWID_VALID(flow_ring->cfp_flowid) ||
#endif // endif
		(flow_ring->status & FLOW_RING_FLUSH_PENDING)) {
		return FLOW_RING_STATE_FLUSH_PEND;
	} else if (flow_ring->status & FLOW_RING_SUP_PENDING) {
		return FLOW_RING_STATE_SUPPRESS_PEND;
	} else {
		return FLOW_RING_STATE_ACTIVE;
	}
}

/* Converts a list of HWA packets to OS packets.
 * NOTE: HWA txpost packet chain may mix unicast, bcmc/null packets,
 * only unicast packets can go through CFP fast path.
 */
void
pciedev_hwa_txpost_pkt2native(struct dngl_bus *pciedev, hwa_txpost_pkt_t *head,
	uint32 pkt_count, uint32 total_octets, hwa_cfp_tx_info_t *hwa_cfp_tx_info,
	void **pktcs, uint32 *pktc_cnts)
{
	msgbuf_ring_t *flow_ring;
	uint32 pkt_index;
	osl_t *osh;
	void *lfrag, **lfrag_prev, *lfrag_uprev, *lfrag_ulprev, *lfrag_mprev;
	void *lfrag_fmprev;
	hwa_txpost_pkt_t *curr, *next;
	bool last_packet, unicast, tid_ac_matched;
	hwa_pktc_type_t pktc_type;
	amsdu_sup_pktlist_t *amsdu_sup_pktc, *amsdu_sup_part1;
	HWA_DEBUG_EXPR(char eabuf_da[ETHER_ADDR_STR_LEN]);
	HWA_DEBUG_EXPR(char eabuf_sa[ETHER_ADDR_STR_LEN]);

	/* Clear out args */
	pktcs[HWA_PKTC_TYPE_UNICAST] = NULL;
	pktcs[HWA_PKTC_TYPE_UNICAST_LEGACY] = NULL;
	pktcs[HWA_PKTC_TYPE_BCMC] = NULL;
	pktcs[HWA_PKTC_TYPE_FLOW_MISMATCH] = NULL;
	pktc_cnts[HWA_PKTC_TYPE_UNICAST] = 0;
	pktc_cnts[HWA_PKTC_TYPE_UNICAST_LEGACY] = 0;
	pktc_cnts[HWA_PKTC_TYPE_BCMC] = 0;
	pktc_cnts[HWA_PKTC_TYPE_FLOW_MISMATCH] = 0;

	/* Setup local */
	pkt_index = 0;
	last_packet = FALSE;
	osh = pciedev->osh;
	curr = head;
	lfrag_uprev = NULL;
	lfrag_ulprev = NULL;
	lfrag_mprev = NULL;
	lfrag_fmprev = NULL;

	HWA_DEBUG_EXPR({
		bcm_ether_ntoa((struct ether_addr *)curr->eth_sada.u8, eabuf_da);
		bcm_ether_ntoa((struct ether_addr *)&curr->eth_sada.u8[ETHER_ADDR_STR_LEN],
			eabuf_sa);
		HWA_TRACE(("%s got %d packets %d octects flowid_ovd %d [da=%s, sa=%s]\n",
			HWA3a, pkt_count, total_octets, curr->flowid_override, eabuf_da, eabuf_sa));
	});

	ASSERT(pkt_count >= 1);

	flow_ring = pciedev->h2d_submitring_ptr[curr->flowid_override];
	ASSERT(flow_ring);
	if (flow_ring == NULL) {
		OSL_SYS_HALT();
	}

	amsdu_sup_pktc = &flow_ring->amsdu_sup_pktc;
	amsdu_sup_part1 = &flow_ring->amsdu_sup_part1;

	/* Save chain info */
	if (amsdu_sup_part1->pkts) {
		uchar *da;

		ASSERT((flow_ring->status & FLOW_RING_SUP_PENDING) == 0);

		/* Use previous stored suppressed AMSDU as head of packet chain. */
		lfrag = amsdu_sup_part1->head;
		da = (uchar *)PKTDATA(osh, lfrag);
		if (ETHER_ISNULLDEST(da) || ETHER_ISMULTI(da)) {
			pktcs[HWA_PKTC_TYPE_BCMC] = lfrag;
			lfrag_mprev = amsdu_sup_part1->tail;
			pktc_cnts[HWA_PKTC_TYPE_BCMC] += amsdu_sup_part1->pkts;
		} else if (pciedev_flow_mismatch(pciedev, flow_ring, da)) {
			pktcs[HWA_PKTC_TYPE_FLOW_MISMATCH] = lfrag;
			lfrag_fmprev = amsdu_sup_part1->tail;
			pktc_cnts[HWA_PKTC_TYPE_FLOW_MISMATCH] += amsdu_sup_part1->pkts;
		} else if (PKTPRIO(lfrag) == hwa_cfp_tx_info->pktlist_prio) {
			pktcs[HWA_PKTC_TYPE_UNICAST] = lfrag;
			lfrag_uprev = amsdu_sup_part1->tail;
			pktc_cnts[HWA_PKTC_TYPE_UNICAST] += amsdu_sup_part1->pkts;
		} else {
			pktcs[HWA_PKTC_TYPE_UNICAST_LEGACY] = lfrag;
			lfrag_ulprev = amsdu_sup_part1->tail;
			pktc_cnts[HWA_PKTC_TYPE_UNICAST_LEGACY] += amsdu_sup_part1->pkts;
		}
		amsdu_sup_part1->pkts = 0;

		/* reuse_sup_seq may be clear from pciedev_flow_ageing_timerfn().
		 * If flow_ring->amsdu_sup_part1 exists,
		 * always set hwa_cfp_tx_info->cfp_capable to FALSE.
		 */
		hwa_cfp_tx_info->cfp_capable = FALSE;
	}

	/* Force chain lfrags */
	while (curr) {
		next = curr->next;

		/* lfrag start after HWA_TXPOST_PKT_BYTES */
		lfrag = HWAPKT2LFRAG((char *)curr);

		/* Terminate next */
		PKTSETCLINK(lfrag, NULL);
		HWA_PKT_TERM_NEXT(curr);

		pkt_index++;

		if (pkt_index == pkt_count) {
			last_packet = TRUE;
		}

		HWA_DEBUG_EXPR(hwa_txpost_dump_pkt(curr, NULL, "pkt2native", pkt_index, TRUE));

		/* Mark packet as TXFRAG and HWAPKT */
		PKTSETTXFRAG(osh, lfrag);
		PKTSETHWAPKT(osh, lfrag);
		PKTRESETHWA3BPKT(osh, lfrag);

		/* Allow only IP/Unicast packets to go through chained TX path
		 * HWA 3a ether type audit can check IP packets, so here SW needs
		 * to check non-Unicast packets
		 */
		if (ETHER_ISNULLDEST(curr->eth_sada.u8) ||
			ETHER_ISMULTI(curr->eth_sada.u8)) {
			unicast = FALSE;
			pktc_type = HWA_PKTC_TYPE_BCMC;
			lfrag_prev = &lfrag_mprev;
		} else {
			unicast = TRUE;
			if (pciedev_flow_mismatch(pciedev, flow_ring, curr->eth_sada.u8)) {
				/* Stale flowring packets */
				tid_ac_matched = FALSE;
				pktc_type = HWA_PKTC_TYPE_FLOW_MISMATCH;
				lfrag_prev = &lfrag_fmprev;
			} else if (curr->prio == hwa_cfp_tx_info->pktlist_prio) {
				/* Is prio same as flow ring? */
				tid_ac_matched = TRUE;
				pktc_type = HWA_PKTC_TYPE_UNICAST;
				lfrag_prev = &lfrag_uprev;
			} else {
				tid_ac_matched = FALSE;
				pktc_type = HWA_PKTC_TYPE_UNICAST_LEGACY;
				lfrag_prev = &lfrag_ulprev;
			}
		}

#ifdef WLCFP
		if (hwa_cfp_tx_info->cfp_capable && unicast && tid_ac_matched) {
			/* With DHDHDR, LLC SNAP inserted by host;
			 * So adjust host address & length here
			 * Legacy path adjust this later in WL layer.
			 */
			curr->data_buf_haddr.loaddr -= DOT11_LLC_SNAP_HDR_LEN;
			curr->data_buf_hlen += DOT11_LLC_SNAP_HDR_LEN;

			/* Save CFP Flowid into the packet */
			PKTSETCFPFLOWID(lfrag, hwa_cfp_tx_info->cfp_flowid);

			/* Prepare the packet for CFP Transmit */
			wlc_cfp_pkt_prepare(FD_CFP_UNIT, hwa_cfp_tx_info->cfp_flowid,
				curr->prio, lfrag, hwa_cfp_tx_info->expiry_time);
		} else {
			PKTCLRCFPFLOWID(lfrag, CFP_FLOWID_INVALID);
		}
#endif /* WLCFP */

		/* Compatbility with PCIE FD */
		PKTSETFRAGTOTNUM(osh, lfrag, 1);
		PKTSETFRAGLEN(osh, lfrag, 1, curr->data_buf_hlen);
		PKTSETFRAGTOTLEN(osh, lfrag, curr->data_buf_hlen);
		PKTFRAGSETRINGINDEX(osh, lfrag, curr->rd_index);

		/* Metadata is handled, doesn't matter if the host has given buffer or not */
		PKTSETHASMETADATA(osh, lfrag);

		/* Set priority */
		PKTSETPRIO(lfrag, curr->prio);

		/* Mark from PCIe */
		HWA_ASSERT(curr->data_buf_haddr.hiaddr & 0x80000000);
		PKTSETFRAGDATA_HI(osh, lfrag, 1, curr->data_buf_haddr.hiaddr);
		PKTSETFRAGDATA_LO(osh, lfrag, 1, curr->data_buf_haddr.loaddr);

		BCMPCIE_IPC_HPA_TEST(pciedev, curr->host_pktid,
			BCMPCIE_IPC_PATH_TRANSMIT, BCMPCIE_IPC_TRANS_REQUEST);
		BUZZZ_KPI_PKT1(KPI_PKT_BUS_TXPOST, 2, curr->host_pktid, flow_ring->ringid);

		// Set host_pktid
		ASSERT(curr->host_pktid != HWA_HOST_PKTID_NULL);
		PKTSETFRAGPKTID(osh, lfrag, curr->host_pktid);

		// Set flowid
		PKTSETFRAGFLOWRINGID(osh, lfrag, curr->flowid_override);
		PKTSETIFINDEX(osh, lfrag, curr->ifid);

		/* BCA: We use DHDHDR feature, each lfrag only has D3_BUFFER and we can reuse
		 * "eth_sada" and "eth_type" inside 3a TXPKT, no need to allocate extra
		 * D3_BUFFER and copy ether header to lfrag, for each txbuffer,
		 * it has pktid associated already.
		 */
		PKTSETBUF(osh, lfrag, (void *)curr->eth_sada.u8, ETHER_HDR_LEN);

		if ((pktc_type != HWA_PKTC_TYPE_FLOW_MISMATCH) &&
			(!(flow_ring->status & FLOW_RING_SUP_PENDING)) &&
			(!(flow_ring->status & FLOW_RING_FLUSH_PENDING)) &&
			pciedev_process_tx_post_reuse_seq(pciedev, flow_ring, &lfrag,
			curr->rd_index, last_packet)) {
			/* When we are here, it means this last lfrag has reuse seq.
			 * We need to defer the pakcet chain of suppressed amsdu
			 * and process it in next round.
			 */
#ifdef WLCFP
			/* cfp_capable should be FALSE if there is any reuse_seq packet. */
			ASSERT(!hwa_cfp_tx_info->cfp_capable);
#endif /* WLCFP */
		} else {
			if (amsdu_sup_pktc->pkts) {
				if (*lfrag_prev == NULL)
					pktcs[pktc_type] = amsdu_sup_pktc->head;
				else
					PKTSETCLINK(*lfrag_prev, amsdu_sup_pktc->head);
				*lfrag_prev = amsdu_sup_pktc->tail;
				pktc_cnts[pktc_type] += amsdu_sup_pktc->pkts;
				bzero(amsdu_sup_pktc, sizeof(amsdu_sup_pktlist_t));
			}
			if (lfrag != NULL) {
				if (*lfrag_prev == NULL)
					pktcs[pktc_type] = lfrag;
				else
					PKTSETCLINK(*lfrag_prev, lfrag);
				*lfrag_prev = lfrag;
				pktc_cnts[pktc_type]++;
			}
		}

		// 3a audit failure chain may has invalid next point
		// Ingore it and don't care.
		if (last_packet) {
			HWA_DEBUG_EXPR({
				if (next != NULL)
					hwa_txpost_dump_pkt(next, NULL, "dummy", pkt_index, TRUE);
			});

			/* Break while */
			break;
		}

		curr = next;
	}

#ifdef WLCFP
	/* Fillup head/tail lfrag pointers for CFP TX submission */
	hwa_cfp_tx_info->pktlist_tail = lfrag_uprev;
	hwa_cfp_tx_info->pktlist_head = pktcs[HWA_PKTC_TYPE_UNICAST];
#endif /* WLCFP */
}

void
pciedev_hwa_process_pending_flring_resp(struct dngl_bus *pciedev, uint16 ringid)
{
	msgbuf_ring_t *ring = pciedev->h2d_submitring_ptr[ringid];

	if (ring && (ring->status & FLOW_RING_FLUSH_PENDING)) {
		pciedev_process_pending_flring_resp(pciedev, ring);
	}
}

#ifdef WLSQS
void
pciedev_sqs_v2r_dequeue(struct dngl_bus *pciedev, uint16 ringid, uint8 prio,
	uint16 pkt_count, bool sqs_force)
{
	msgbuf_ring_t *msgbuf = pciedev->h2d_submitring_ptr[ringid];

	ASSERT(msgbuf);

	if (CFP_FLOWID_VALID(msgbuf->cfp_flowid)) {
		if (wlc_sqs_v2r_pkts(msgbuf->cfp_flowid, prio) > 0) {
			wlc_sqs_v2r_dequeue(msgbuf->cfp_flowid, prio, pkt_count, sqs_force);
		}
	}
}
/* Callback from TAF module to indicate end of a pull request set */
int
sqs_eops_rqst(void* arg)
{
	/* Update the schedcmd flags for the last request */
	return hwa_txpost_schedcmd_flags_update(hwa_dev, TXPOST_SCHED_FLAGS_RESP_PEND);
}
#endif /* WLSQS */
#endif /* HWA_TXPOST_BUILD */

#ifdef HWA_TXCPLE_BUILD
void
hwa_upd_last_queued_flowring(struct dngl_bus * pciedev, uint16 ringid)
{
#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	pciedev_upd_last_queued_flowring(pciedev, ringid);
#endif /* PCIE_DMA_INDEX && SBTOPCIE_INDICES */
}

void
hwa_sync_flowring_read_ptrs(struct dngl_bus *pciedev)
{
#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	pciedev_sync_flowring_read_ptrs(pciedev, pciedev->dtoh_txcpl);
#endif /* PCIE_DMA_INDEX && SBTOPCIE_INDICES */
}
#endif /* HWA_TXCPLE_BUILD */
#endif /* BCMHWA */
